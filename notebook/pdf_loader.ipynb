{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3aaa7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8771672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 PDF files to process\n",
      "Processing: Abhishek_Pant_AI_ML_Resume.docx.pdf\n",
      "  ‚úì Loaded 1 pages\n",
      "Processing: AI.pdf\n",
      "  ‚úì Loaded 42 pages\n",
      "Processing: attachment.pdf\n",
      "  ‚úì Loaded 2 pages\n",
      "Processing: Python Interview Questions.pdf\n",
      "  ‚úì Loaded 56 pages\n",
      "Processing: Python Questions -3.pdf\n",
      "  ‚úì Loaded 40 pages\n",
      "Processing: RAG Cheat Sheet.pdf\n",
      "  ‚úì Loaded 14 pages\n",
      "\n",
      "Total pages loaded: 155\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ‚úì Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal pages loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96ed1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m146 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Abhishek_Pant_AI_ML_Fresher_Resume.docx', 'source': '..\\\\data\\\\pdf\\\\Abhishek_Pant_AI_ML_Resume.docx.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Abhishek_Pant_AI_ML_Resume.docx.pdf', 'file_type': 'pdf'}, page_content='ABHISHEK  PANT     Phone :   8126961106                                                                                                                 |  Email :-  abhishekpant.as@gmail.com       LinkedIn:-www.linkedin.com/in/abhishek-pant-2b2129236                                                                                GitHub:-https://github.com/Rusty-user365                                                                               \\nTechnical  Skills   \\n     Programming:  Python  (Pandas,  NumPy,  Seaborn,  Matplotlib,  Scikit-Learn   Regression,  Classification,  Model  Evaluation),  SQL       Machine  Learning  &  Deep  Learning:  TensorFlow,  Transformers  (Hugging  Face),  Experiment  Tracking,Monitoring        Natural  Language  Processing:  LLM  Fine ‚Äë Tuning,  RAG  Systems,  Embeddings,  LangChain,  ChromaDB       Data  &  Visualization:  Data  Cleaning,  Feature  Engineering,  Exploratory  Data  Analysis  (EDA),  Matplotlib,  Seaborn       MLOps  &  Deployment:  MLflow,  DVC,  Docker,  Kubernetes,  CI/CD  (GitHub  Actions),  Model  Registry       Tools  &  Platforms:  Git,  Jupyter  Notebook,  VS  Code,  Aws  Projects    AI  Interview  Assistant (Project  Link)                                                                                                               Jan   2026   ‚Äì   Feb  2026  \\n‚Ä¢\\n \\nDeveloped  an  AI  Interview  Assistant  using  Streamlit  to  simulate  real  interview  scenarios  with  voice-based  interaction  \\n‚Ä¢\\n \\nImplemented  live  transcription  pipelines  with  Whisper  and  other  ASR  models  to  capture  candidate  responses  in  real  \\ntime.Includes\\n \\n3\\n \\nrounds:\\n \\nIntroduction,Technical,Coding\\n.\\n \\nWith\\n \\npost\\n \\nanalysis\\n \\nwith\\n \\nAi\\n.\\n \\n \\n‚Ä¢\\n \\nIntegrated  LLMs  for  dynamic  question  generation  and  feedback,  enabling  adaptive  interview  practice.  \\n‚Ä¢\\n \\nDesigned   dashboard  visualizations  for  tracking  strengths,  weaknesses,  and  progress  trends  across  multiple  \\ninterviews.\\n \\n     Tech  Used:   Python,Gemma3:250M    MLOps  Pipeline  ‚Äì  Experiment  Tracking  &  Deployment  (PROJECT  LINK)                                                    Jan   2026   ‚Äì   Feb  2026  \\n‚Ä¢\\n \\nImplemented  an  end-to-end  machine  learning  workflow  for  a  classification  task  using  MLflow  for  experiment  \\ntracking\\n \\nand\\n \\nmodel\\n \\nregistry.\\n \\nDemonstrated\\n \\nmodel\\n \\nlifecycle\\n \\nmanagement.\\n \\n \\n‚Ä¢\\n \\nContainerized  the  application  with  Docker  and  deployed  it  locally  for  reproducible  environments.  \\n‚Ä¢\\n \\nIntegrated  GitHub  Actions  CI/CD  to  automate  testing  and  deployment  on  every  commit.  Tech  Used:   Python,  Scikit-Learn,  MLflow,  Docker,  GitHub  Actions,  Git    Reddit  Persona  Generation  Tool  (LLM-based)  (PROJECT  LINK)                                                                June  2025   ‚Äì   July  2025  \\n‚Ä¢\\n \\nDesigned  a  system  to  analyze  a  user‚Äôs  Reddit  posts  and  comments  to  generate  a  detailed  persona  profile.  \\n‚Ä¢\\n \\nImplemented  persona  extraction  using  a  local  LLM  via  Ollama .  \\n‚Ä¢\\n \\nExtracted  attributes  such  as  interests,  profession,  personality  traits,  and  age  group  with  source  citations.  Tech  Used:  Python,  LLM  (Ollama),  NLP   Financial  LLM  ‚Äì  Data  Extraction  from  PDFs   (PROJECT  LINK)                                                                      Feb  2025   ‚Äì   Aug  2025  \\n‚Ä¢\\n \\nImplemented  a  financial  document  processing  pipeline   using  a  fine-tuned  Qwen  family  LLM  to  extract  key  financial  \\nentities\\n \\nfrom\\n \\nPDFs\\n \\ninto\\n \\nstructured\\n \\nJSON\\n \\nformat\\n.\\n \\n‚Ä¢\\n \\nIntegrated  the  financial  LLM  into  a  Retrieval-Augmented  Generation  (RAG)  chatbot  for  domain-specific  Q&A.  \\n‚Ä¢\\n \\nEmbedded  curated  financial  content  into  ChromaDB  and  implemented  semantic  search  with  LangChain  for  context  \\nretrieval.\\n \\n‚Ä¢\\n \\nEnabled  grounded,  explainable  responses  by  combining  extracted  structured  data  with  retrieved  knowledge.  Tech  Used:  Python,  LLM  (Qwen,  Ollama),  LangChain,  ChromaDB,  NLP,  Jupyter  Notebook     Education   \\nUttaranchal  University                                                                                                                                              Aug  2023  ‚Äì  June  2025  Master  of  Computer  Applications (AI/ML)                                                                                                                   Dehradun,  Uttarakhand     Certifications   \\n‚Ä¢  \\nOracle  Cloud  Infrastructure  2025  Generative  AI  Professional  (1Z0-1127-25)  ‚Äì Oracle \\n‚Ä¢  \\nOracle  Cloud  Infrastructure  2025   Data  Science  Professional  (1Z0-1110-25  ) ‚Äì  Oracle \\n‚Ä¢  \\nAdvanced  AI  and  Machine  Learning  Techniques  and  Capstone  ‚Äì Microsoft \\n‚Ä¢  \\nPython  ‚Äì HackerRank  and   AWS  ‚Äì  Scaler \\n‚Ä¢  \\nSoftware  Engineer  Intern  (Skill  Certification)  ‚Äì HackerRank'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 0, 'page_label': '1', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 1, 'page_label': '2', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q1.  Explain  the  requirement  of  RAG  when  LLMs  are  already  powerful.   LLMs  are  powerful,  as  they  are  trained  on  large  volumes  of  data  using  sophisticated  \\ntechniques.\\n \\nHowever,\\n \\nLLMs\\n \\nbecause\\n \\nof\\n \\nknowledge\\n \\ncutoff\\n \\n(static\\n \\nknowledge),\\n \\nstruggle\\n \\nto\\n \\nanswer\\n \\nqueries\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nlatest\\n \\nevents\\n \\nor\\n \\nthe\\n \\ndata\\n \\nnot\\n \\npresent\\n \\nin\\n \\ntheir\\n \\ntraining\\n \\ncorpus.\\n \\n \\n RAG  addresses  this  challenge  by  retrieving  relevant  context  from  external  knowledge  \\nsources,\\n \\nwhich\\n \\nallows\\n \\nLLMs\\n \\nto\\n \\nprovide\\n \\naccurate\\n \\nresponses.\\n \\nThis\\n \\nis\\n \\nwhy\\n \\nRAG\\n \\nis\\n \\nessential\\n \\nfor\\n \\nLLM-based\\n \\napplications\\n \\nthat\\n \\nneed\\n \\nto\\n \\nbe\\n \\naccurate.\\n \\nOtherwise,\\n \\nLLMs\\n \\nalone\\n \\nmight\\n \\nprovide\\n \\nyou\\n \\nanswers\\n \\nthat\\n \\nare\\n \\nincomplete\\n \\nor\\n \\noutdated.\\n  \\nQ2.\\n \\nIs\\n \\nRAG\\n \\nstill\\n \\nrelevant\\n \\nin\\n \\nthe\\n \\nera\\n \\nof\\n \\nlong\\n \\ncontext\\n \\nLLMs?\\n  RAG  is  still  important  even  with  long  context  LLMs.  This  is  because  long-context  LLMs  \\nwithout\\n \\nRAG\\n \\nhave\\n \\nthree\\n \\nbig\\n \\nproblems:\\n \\n\"lost\\n \\nin\\n \\nthe\\n \\nmiddle,\",\\n \\nhigh\\n \\nAPI\\n \\ncosts,\\n \\nand\\n \\nincreased\\n \\nlatency.\\n  Long-context  LLMs  often  struggle  to  find  the  most  relevant   information  in  large  \\ncontexts,\\n \\nwhich\\n \\nhurts\\n \\nthe\\n \\nquality\\n \\nof\\n \\ngenerated\\n \\nresponses.\\n \\nFurthermore,\\n \\nprocessing\\n \\nlengthy\\n \\nsequences\\n \\nin\\n \\neach\\n \\nAPI\\n \\ncall\\n \\nresults\\n \\nin\\n \\nhigh\\n \\nlatency\\n \\nand\\n \\nhigh\\n \\nAPI\\n \\ncosts.\\n  RAG  addresses  these  issues  by  providing  the  most  relevant  information  from  external  \\nknowledge\\n \\nsources.\\n \\nSo,\\n \\nyou\\n \\nstill\\n \\nneed\\n \\nRAG\\n \\nto\\n \\nget\\n \\naccurate\\n \\nand\\n \\ncost-efficient\\n \\nresponses,\\n \\neven\\n \\nwith\\n \\nlong\\n \\ncontext\\n \\nLLMs.\\n  Q3.  What  are  the  fundamental  challenges  of  RAG  systems?  RAG  is  powerful,  but  it  has  to  deal  with  the  following  challenges:   Scalability:  Searching  and  retrieving  from  large,  dynamic  knowledge  sources  quickly  \\nand\\n \\nefficiently\\n \\nrequires\\n \\na\\n \\nlot\\n \\nof\\n \\ncomputing\\n \\npower\\n \\nand\\n \\nwell-optimized\\n \\nindexing,\\n \\nwhich\\n \\ncan\\n \\nbe\\n \\nexpensive\\n \\nor\\n \\ntake\\n \\na\\n \\nlong\\n \\ntime.\\n  Latency  -  The  two-step  process  (retrieval  then  generation)  can  cause  delays,  making  it  \\nless\\n \\nsuitable\\n \\nfor\\n \\nreal-time\\n \\napplications\\n \\nwithout\\n \\ncareful\\n \\noptimization.\\n  Hallucination  Risk  -  Even  with  retrieval,  the  model  might  generate  plausible  but  \\nunsupported\\n \\ndetails\\n \\nif\\n \\nthe\\n \\nretrieved\\n \\ndata\\n \\nis\\n \\nambiguous\\n \\nor\\n \\ninsufficient.\\n  1                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 2, 'page_label': '3', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Bias  and  Noise  -  Retrieved  content  might  carry  biases,  errors,  or  irrelevant  noise  from  \\nthe\\n \\nweb\\n \\nor\\n \\nother\\n \\nsources,\\n \\nwhich\\n \\ncan\\n \\npropagate\\n \\ninto\\n \\nthe\\n \\noutput.\\n  Q4.  What  are  eÔ¨Äective  strategies  to  reduce  latency  in  RAG  systems?  Caching,  embedding  quantization,  selective  query  rewriting,  and  selective  re-ranking  are  \\nsome\\n \\nof\\n \\nthe\\n \\nways\\n \\nto\\n \\nreduce\\n \\nRAG\\n \\nlatency.\\n \\nCaching\\n \\nstores\\n \\nretrieved\\n \\nresults\\n \\nor\\n \\ngenerated\\n \\nresponses\\n \\nto\\n \\navoid\\n \\nredundant\\n \\ncomputation.\\n \\nEmbedding\\n \\nquantization\\n \\nto\\n \\nlower\\n \\nbit\\n \\nprecision\\n \\nreduces\\n \\nmemory\\n \\nand\\n \\ncomputational\\n \\nload,\\n \\nspeeding\\n \\nup\\n \\nretrieval.\\n \\n  Selective  query  rewriting  enhances  recall  and  relevance  by  refining  queries  prior  to  \\nretrieval,\\n \\nprimarily\\n \\nutilized\\n \\nfor\\n \\ncomplex\\n \\nor\\n \\nambiguous\\n \\nqueries.\\n \\nSelective\\n \\nre-ranking\\n \\nis\\n \\nonly\\n \\nused\\n \\nfor\\n \\ncomplicated\\n \\nqueries,\\n \\nwhich\\n \\ncuts\\n \\ndown\\n \\non\\n \\nunnecessary\\n \\ncomputation\\n \\nfor\\n \\nsimpler\\n \\nones.\\n \\n \\n Q5.  Explain  R,  A,  and  G  in  RAG.   RAG  stands  for  Retrieval-Augmented  Generation.  The  \"R\"  or  Retrieval,  refers  to  the  \\nprocess\\n \\nof\\n \\nsearching\\n \\nand\\n \\nfetching\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\ninformation\\n \\nfrom\\n \\nexternal\\n \\nknowledge\\n \\nsources\\n \\nfor\\n \\nthe\\n \\ngiven\\n \\nuser\\n \\nquery.\\n \\n  The  \"A\"  or  Augmented,  involves  including  the  retrieved  relevant  context  in  the  LLM  \\nprompt\\n \\nhaving\\n \\nthe\\n \\nuser\\n \\nquery\\n \\nand\\n \\ninstructions\\n \\nso\\n \\nthat\\n \\nthe\\n \\nLLM\\n \\ncan\\n \\ngenerate\\n \\na\\n \\nresponse\\n \\nbased\\n \\non\\n \\nthe\\n \\nprovided\\n \\ncontext.\\n  \\n  Finally,  the  \"G\"  or  Generation  is  the  phase  during  which  the  generator  LLM  processes  \\nthe\\n \\nprompt\\n \\nhaving\\n \\ninstructions,\\n \\na\\n \\nquery,\\n \\nand\\n \\ncontext\\n \\nto\\n \\ngenerate\\n \\na\\n \\nresponse\\n \\nthat\\n \\nis\\n \\ncoherent,\\n \\naccurate,\\n \\nand\\n \\ncontextually\\n \\nrelevant.\\n \\n  Q6.  How  does  RAG  help  reduce  hallucinations  in  LLM  generated  responses?  Without  RAG,  LLM  answers  user  questions  based  on  what  it  learned  from  the  training  \\ncorpus,\\n \\nwhich\\n \\nmay\\n \\nnot\\n \\nbe\\n \\nup-to-date\\n \\nor\\n \\ncomplete.\\n \\nThis\\n \\ncould\\n \\nlead\\n \\nto\\n \\nhallucinated\\n \\nresponses,\\n \\nwhich\\n \\nare\\n \\nanswers\\n \\nthat\\n \\nsound\\n \\nright\\n \\nbut\\n \\nare\\n \\nwrong.\\n  Retrieval-Augmented  Generation  (RAG)  helps  cut  down  on  hallucinations  in  \\nLLM-generated\\n \\nresponses\\n \\nby\\n \\nadding\\n \\nan\\n \\nexternal\\n \\nretrieval\\n \\nsystem\\n \\nthat\\n \\npulls\\n \\nrelevant,\\n \\nfactual\\n \\ninformation\\n \\nfrom\\n \\ntrusted,\\n \\nup-to-date\\n \\nexternal\\n \\nknowledge\\n \\nsources.\\n  \\n2                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 3, 'page_label': '4', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           By  combining  retrieval  with  generation,  RAG  ensures  that  answers  are  more  accurate,  \\ncontextually\\n \\nrelevant,\\n \\nand\\n \\nless\\n \\nprone\\n \\nto\\n \\nfabrications\\n \\nor\\n \\nfalse\\n \\ninformation,\\n \\nsignificantly\\n \\nenhancing\\n \\nthe\\n \\nreliability\\n \\nof\\n \\nthe\\n \\noutput.\\n \\n  Q7.  Why  is  re-ranking  important  in  the  RAG  pipeline  after  initial  document  \\nretrieval?  The  top  K  chunks  fetched  by  the  RAG  retriever  may  have  irrelevant  chunks  ahead  of  \\nrelevant\\n \\nones.\\n \\nPassing\\n \\nthese\\n \\nresults\\n \\ndirectly\\n \\nto\\n \\nthe\\n \\nLLM\\n \\nhurts\\n \\nthe\\n \\nquality\\n \\nof\\n \\nthe\\n \\nanswers\\n \\nbecause\\n \\nLLMs\\n \\nmostly\\n \\nlook\\n \\nat\\n \\nthe\\n \\ntop-ranked\\n \\nchunks\\n \\nthat\\n \\nare\\n \\ngiven\\n \\nas\\n \\ncontext.\\n \\n  Re-ranking  uses  cross-encoder  models  to  deeply  measure  the  semantic  relevance  of   \\nquery-chunk\\n \\npairs\\n \\nand\\n \\nthen\\n \\nbrings\\n \\nrelevant\\n \\nchunks\\n \\nahead\\n \\nof\\n \\nirrelevant\\n \\nchunks.\\n \\nThis\\n \\nreduces\\n \\nthe\\n \\nnoise\\n \\nand\\n \\nhelps\\n \\nthe\\n \\ngenerator\\n \\nLLM\\n \\nto\\n \\ngenerate\\n \\nmore\\n \\naccurate\\n \\nand\\n \\ncoherent\\n \\nanswers.\\n \\n  Q8.  What  is  the  purpose  of  character  overlap  during  chunking  in  a  RAG  \\npipeline?  In  a  RAG  pipeline,  chunk  overlap  during  chunking  ensures  contextual  continuity  and  \\nprevents\\n \\nloss\\n \\nof\\n \\ninformation\\n \\nat\\n \\nthe\\n \\nboundaries\\n \\nof\\n \\nchunks.\\n \\nThis\\n \\nimproves\\n \\nthe\\n \\nretrieval\\n \\naccuracy\\n \\nand\\n \\nmaintains\\n \\ncoherence\\n \\nin\\n \\nthe\\n \\ntext\\n \\nfed\\n \\nto\\n \\nthe\\n \\nLLM.\\n \\n  Typically,  an  overlap  of  about  10-20%  of  the  chunk  size  is  used  to  strike  a  balance  \\nbetween\\n \\npreserving\\n \\ncontext\\n \\nand\\n \\ncomputational\\n \\nefficiency\\n \\nin\\n \\nRAG\\n \\napplications.\\n  Q9.  What  role  does  cosine  similarity  play  in  relevant  chunk  retrieval  within  \\na\\n \\nRAG\\n \\npipeline?  Cosine  similarity  measures  how  similar  the  query  embedding  is  to  the  embeddings  of  \\nchunks\\n \\nin\\n \\nthe\\n \\nvector\\n \\ndatabase.\\n \\nIt\\n \\nfinds\\n \\nthe\\n \\ncosine\\n \\nof\\n \\nthe\\n \\nangle\\n \\nbetween\\n \\ntwo\\n \\nvectors\\n \\nand\\n \\nprovides\\n \\na\\n \\nscore\\n \\nthat\\n \\nshows\\n \\nhow\\n \\nclosely\\n \\nrelated\\n \\nthe\\n \\nquery\\n \\nis\\n \\nto\\n \\neach\\n \\nchunk.\\n \\nHigher\\n \\nscores\\n \\nmean\\n \\nthat\\n \\nthe\\n \\nchunk\\n \\nis\\n \\nmore\\n \\nrelevant.\\n \\n  This  enables  the  RAG  system  to  retrieve  the  most  relevant  chunks  for  the  query,  which  is  \\nthen\\n \\nused\\n \\nby\\n \\nthe\\n \\ngenerator\\n \\nLLM\\n \\nto\\n \\ngenerate\\n \\naccurate\\n \\nanswers.\\n  Q10.  Can  you  give  examples  of  real-world  applications  where  RAG  systems  \\nhave\\n \\ndemonstrated\\n \\nvalue? \\n3                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 4, 'page_label': '5', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            AI  search  engines  are  a  great  example  of  how  RAG  systems  have  changed  the  way  people  \\nfind\\n \\ninformation\\n \\nonline.\\n \\nAI\\n \\nsearch\\n \\nengines\\n \\ngive\\n \\nyou\\n \\naccurate,\\n \\nrelevant\\n \\nanswers\\n \\nby\\n \\ncombining\\n \\ninformation\\n \\nretrieval\\n \\nwith\\n \\ngenerative\\n \\nAI.\\n \\n  For  instance,  RAG-based  AI  search  platforms  like  Perplexity  AI  improve  the  user  \\nexperience\\n \\nby\\n \\nfetching\\n \\nthe\\n \\nmost\\n \\nrecent\\n \\nand\\n \\nrelevant\\n \\ninformation\\n \\nfrom\\n \\nlarge\\n \\nknowledge\\n \\nbases\\n \\nand\\n \\nthen\\n \\ngiving\\n \\nit\\n \\nback\\n \\nin\\n \\nthe\\n \\nformat\\n \\nthat\\n \\nthe\\n \\nuser\\n \\nwants.\\n  Q11.  Explain  the  steps  in  the  indexing  process  in  a  RAG  pipeline.   There  are  four  steps  in  the  indexing  process  of  a  RAG  pipeline:  parsing,  chunking,  \\nencoding,\\n \\nand\\n \\nstoring.\\n \\nThe\\n \\nparsing\\n \\nstep\\n \\ndeals\\n \\nwith\\n \\nextracting\\n \\nthe\\n \\ndocument\\n \\ncontent.\\n \\nThen,\\n \\nthe\\n \\nchunking\\n \\nstep\\n \\nsplits\\n \\nthe\\n \\nextracted\\n \\ncontent\\n \\ninto\\n \\nsmaller\\n \\npieces\\n \\ncalled\\n \\nchunks.\\n \\n  The  encoding  step  uses  an  embedding  model  to  convert  chunks  into  dense  numerical  \\nvectors\\n \\ncalled\\n \\nembeddings.\\n \\nFinally,\\n \\nthese\\n \\nembeddings\\n \\nare\\n \\nsaved\\n \\nin\\n \\na\\n \\nvector\\n \\ndatabase\\n \\nfor\\n \\nefficient\\n \\nsearch\\n \\nand\\n \\nretrieval.\\n  All  these  steps  in  the  indexing  process  are  performed  offline.   Q12.  Explain  the  importance  of  chunking  in  RAG. \\n Chunking  in  Retrieval-Augmented  Generation  (RAG)  is  crucial  because  it  breaks  down  \\nlarge\\n \\ntexts\\n \\ninto\\n \\nsmaller\\n \\nand\\n \\nsemantically\\n \\ncoherent\\n \\nsegments\\n \\ncalled\\n \\nchunks.\\n \\nProper\\n \\nchunking\\n \\nhelps\\n \\nto\\n \\nfind\\n \\nrelevant\\n \\ninformation\\n \\nefficiently\\n \\nby\\n \\ncreating\\n \\nfocused\\n \\nchunks\\n \\nthat\\n \\nmaintain\\n \\ncontext\\n \\nand\\n \\navoid\\n \\nirrelevant\\n \\nnoise.\\n \\n  Choosing  the  right  chunk  size  balances  detail  and  context,  optimizing  both  retrieval  \\naccuracy\\n \\nand\\n \\ncomputational\\n \\nefficiency.\\n \\nIneffective\\n \\nchunking\\n \\ncan\\n \\nlead\\n \\nto\\n \\npoor\\n \\nretrieval\\n \\nresults\\n \\nand\\n \\nincoherent\\n \\nresponses,\\n \\nwhich\\n \\nmakes\\n \\nit\\n \\na\\n \\nfoundational\\n \\nstep\\n \\nfor\\n \\nsuccessful\\n \\nRAG\\n \\nperformance\\n \\nin\\n \\nreal-world\\n \\napplications.\\n  Q13.  How  do  you  choose  the  chunk  size  for  a  RAG  system?  Choosing  the  chunk  size  for  a  RAG  system  involves  balancing  granularity,  context  \\ncompleteness,\\n \\nand\\n \\ncomputational\\n \\nefficiency.\\n \\nSmaller\\n \\nchunks\\n \\n(e.g.,\\n \\n100-200\\n \\ntokens)\\n \\nallow\\n \\nprecise\\n \\nretrieval\\n \\nbut\\n \\nmay\\n \\nlack\\n \\nsufficient\\n \\ncontext.\\n \\nLarger\\n \\nchunks\\n \\n(e.g.,\\n \\n500-1000\\n \\ntokens)\\n \\nprovide\\n \\nmore\\n \\ncontext\\n \\nat\\n \\nthe\\n \\ncost\\n \\nof\\n \\nincreased\\n \\ncomputational\\n \\nload\\n \\nand\\n \\npotential\\n \\nnoise.\\n \\n 4                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 5, 'page_label': '6', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            The  optimal  size  depends  on  the  use  case,  document  structure,  embedding  model,  and  \\nthe\\n \\ngenerator\\n \\n(LLM)\\n \\nmodel.\\n \\nFor\\n \\nexample,\\n \\nsmaller\\n \\nchunks\\n \\nare\\n \\nsuitable\\n \\nfor\\n \\nfact-based\\n \\nqueries,\\n \\nand\\n \\nmore\\n \\ncomplex\\n \\nqueries\\n \\nbenefit\\n \\nfrom\\n \\nlarger\\n \\nones.\\n  Q14.  What  are  the  potential  consequences  of  having  chunks  that  are  too  \\nlarge\\n \\nversus\\n \\nchunks\\n \\nthat\\n \\nare\\n \\ntoo\\n \\nsmall? \\n Large  chunks  often  mix  different  topics  into  one  chunk  and  reduce  the  chunk's  \\nrelevance.\\n \\nThis\\n \\ncan\\n \\nlead\\n \\nto\\n \\ncoarse\\n \\nvector\\n \\nrepresentations\\n \\nand\\n \\nless\\n \\naccurate\\n \\nretrieval.\\n \\nLarge\\n \\nchunks\\n \\ncan\\n \\nalso\\n \\nadd\\n \\nnoise\\n \\nand\\n \\nconfuse\\n \\nthe\\n \\nmodel\\n \\nwith\\n \\nirrelevant\\n \\ninformation\\n \\nthat\\n \\nisn't\\n \\nimportant,\\n \\nresulting\\n \\nin\\n \\na\\n \\nless\\n \\naccurate\\n \\nanswer.\\n  Small  chunk  sizes  in  RAG  systems  can  lead  to  fragmented  context.  This  fragmentation  \\noften\\n \\nleads\\n \\nto\\n \\npoor\\n \\nretrieval\\n \\nquality\\n \\nbecause\\n \\ninformation\\n \\nthat\\n \\nis\\n \\nsemantically\\n \\nrelated\\n \\nmay\\n \\nbe\\n \\nsplit\\n \\nup\\n \\ninto\\n \\nchunks\\n \\nthat\\n \\nare\\n \\nnot\\n \\nretrieved\\n \\ntogether.\\n \\nFurthermore,\\n \\nsmaller\\n \\nchunks\\n \\nmean\\n \\nthat\\n \\nthere\\n \\nare\\n \\nmore\\n \\nchunks\\n \\nin\\n \\nthe\\n \\nvector\\n \\ndatabase,\\n \\nwhich\\n \\nincreases\\n \\nstorage\\n \\ncosts\\n \\nand\\n \\nslows\\n \\ndown\\n \\nthe\\n \\nsimilarity\\n \\nsearch.\\n \\n Q15.  Explain  the  retrieval  process  step-by-step  in  a  RAG  pipeline.  The  retrieval  process  in  RAG  systems  starts  by  encoding  the  user  query,  i.e.,  converting  \\nit\\n \\ninto\\n \\na\\n \\ndense\\n \\nvector\\n \\nrepresentation\\n \\nusing\\n \\nan\\n \\nembedding\\n \\nmodel.\\n  \\nThis\\n \\nvector\\n \\nrepresentation\\n \\nis\\n \\nthen\\n \\nused\\n \\nto\\n \\nsearch\\n \\nthe\\n \\nvector\\n \\ndatabase,\\n \\nwhich\\n \\nhas\\n \\nthe\\n \\nembeddings\\n \\nof\\n \\nchunks.\\n  \\nBased\\n \\non\\n \\nthe\\n \\nsimilarity\\n \\nscores,\\n \\nthe\\n \\nvector\\n \\ndatabase\\n \\nsystem\\n \\nreturns\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\ndocument\\n \\nchunks.\\n \\n  Q16.  What  are  the  key  considerations  when  choosing  an  LLM  for  a  RAG  \\nsystem?  The  key  considerations  when  choosing  an  LLM  for  a  RAG  system  are  \\nreading-comprehension\\n \\nability,\\n \\ncontext\\n \\nwindow\\n \\nsize,\\n \\nand\\n \\ninference\\n \\nspeed.\\n \\nReading-comprehension\\n \\nability\\n \\nreflects\\n \\nhow\\n \\neffectively\\n \\nthe\\n \\nmodel\\n \\nprocesses\\n \\nthe\\n \\nretrieved\\n \\ncontext\\n \\nto\\n \\ngenerate\\n \\naccurate\\n \\nresponses.\\n \\n  Context  window  size  is  crucial,  as  longer  context  models  enable  RAG  systems  to  \\neffectively\\n \\ninclude\\n \\nmore\\n \\nrelevant\\n \\nchunks.\\n \\nHowever,\\n \\nthis\\n \\nmust\\n \\nbe\\n \\nbalanced\\n \\nagainst\\n \\ncost\\n \\nand\\n \\nlatency\\n \\nrequirements.\\n \\n  \\n5                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Additionally,  inference  speed,  infrastructure  compatibility,  and  licensing  terms  also  play  \\na\\n \\nkey\\n \\nrole\\n \\nin\\n \\ndeployment\\n \\ndecisions\\n \\nfor\\n \\nreal-world\\n \\nRAG\\n \\nsolutions.\\n  Q17.  How  is  the  prompt  provided  to  the  LLM  in  a  RAG  system  diÔ¨Äerent  from  \\na\\n \\nstandard,\\n \\nnon-RAG\\n \\nprompt?\\n  The  prompt  provided  to  the  LLM  without  a  RAG  setup  includes  only  the  user  query  and  \\nthe\\n \\noptional\\n \\ninstructions.\\n \\nHere,\\n \\nthe\\n \\nLLM\\n \\ngenerates\\n \\nthe\\n \\nresponse\\n \\nbased\\n \\non\\n \\nits\\n \\nknowledge\\n \\ngained\\n \\nduring\\n \\ntraining.\\n  The  prompt  provided  to  the  LLM  with  the  RAG  setup  includes  the  user  query,  \\ninstructions,\\n \\nand\\n \\nrelevant\\n \\ncontext.\\n \\nHere,\\n \\nthe\\n \\nLLM\\n \\ngenerates\\n \\nthe\\n \\nresponse\\n \\nas\\n \\nper\\n \\nthe\\n \\ninstructions\\n \\nsolely\\n \\nbased\\n \\non\\n \\nthe\\n \\nprovided\\n \\nrelevant\\n \\ncontext.\\n \\n  Q18.  What  are  the  key  hyperparameters  in  a  RAG  pipeline?  Chunk  size,  chunk  overlap,  embedding  dimensionality,  retrieval  top-k,  and  retrieval  \\nthreshold\\n \\nare\\n \\nsome\\n \\nof\\n \\nthe\\n \\nmost\\n \\nimportant\\n \\nhyperparameters\\n \\nfor\\n \\nretrieval\\n \\nin\\n \\nRAG.\\n \\nTemperature\\n \\nand\\n \\nmax\\n \\noutput\\n \\nlength\\n \\nare\\n \\ntwo\\n \\nimportant\\n \\nhyperparameters\\n \\nfor\\n \\nRAG\\n \\ngeneration.\\n  The  chunk  size  determines  how  much  text  is  put  into  a  segment  before  embedding,  \\ninfluencing\\n \\nthe\\n \\ncontext\\n \\ngranularity\\n \\nretrieved.\\n \\nChunk\\n \\noverlap\\n \\nrepeats\\n \\na\\n \\nset\\n \\nof\\n \\ntokens\\n \\nat\\n \\nchunk\\n \\nboundaries,\\n \\nhelping\\n \\npreserve\\n \\nimportant\\n \\ncontext\\n \\nacross\\n \\nsegments.\\n \\nEmbedding\\n \\ndimensionality\\n \\nis\\n \\nthe\\n \\nvector\\n \\nsize\\n \\nused\\n \\nto\\n \\nrepresent\\n \\ntext,\\n \\nwhich\\n \\naffects\\n \\nretrieval\\n \\nprecision\\n \\nand\\n \\ndatabase\\n \\nefficiency.\\n  Retrieval  top-k  sets  the  number  of  most  similar  chunks  returned,  directly  impacting  \\nrecall\\n \\nand\\n \\ncontext\\n \\ndiversity\\n \\nin\\n \\nthe\\n \\nresponse.\\n \\nThe\\n \\nretrieval\\n \\nthreshold\\n \\nis\\n \\na\\n \\nsimilarity\\n \\ncutoff\\n \\nthat\\n \\nfilters\\n \\nretrieved\\n \\nresults,\\n \\nensuring\\n \\nonly\\n \\nrelevant\\n \\nchunks\\n \\nare\\n \\nselected.\\n  Temperature  controls  the  randomness  of  generated  text,  balancing  creativity  and  \\ndeterminism\\n \\nin\\n \\nmodel\\n \\noutputs.\\n \\nMax\\n \\noutput\\n \\nlength\\n \\nlimits\\n \\nthe\\n \\nnumber\\n \\nof\\n \\ntokens\\n \\ngenerated,\\n \\nmanaging\\n \\nthe\\n \\nverbosity\\n \\nand\\n \\ncomputational\\n \\ncost\\n \\nof\\n \\nresponses.\\n  Q19.  What  are  the  popular  frameworks  to  implement  a  RAG  system?  Justify  \\nyour\\n \\nchoice\\n \\nof\\n \\nframework.  LangChain,  LlamaIndex,  and  Haystack  are  the  most  popular  frameworks  for  RAG  \\nimplementation.\\n \\nLangChain\\n \\nis\\n \\ngreat\\n \\nfor\\n \\ncustom\\n \\npipelines,\\n \\nand\\n \\nLlamaIndex\\n \\nis\\n \\ngreat\\n \\nfor\\n 6                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 7, 'page_label': '8', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           efficient  document  indexing  and  retrieval.  The  Haystack  framework  provides  excellent  \\nmodularity\\n \\nfor\\n \\nbuilding\\n \\nRAG\\n \\nsystems.\\n  I  would  recommend  LangChain  because  of  its  comprehensive  ecosystem,  extensive  \\ndocumentation,\\n \\nactive\\n \\ncommunity\\n \\nsupport,\\n \\nand\\n \\nflexibility\\n \\nin\\n \\nhandling\\n \\nvarious\\n \\ndata\\n \\nsources\\n \\nand\\n \\nLLM\\n \\nintegrations.\\n  Q20.  Explain  the  inÔ¨Çuence  of  LLM  context  window  size  on  RAG  \\nhyperparameters.  The  size  of  the  LLM  context  window  has  a  big  impact  on  RAG  hyperparameters,  like  \\nchunk\\n \\nsize\\n \\nand\\n \\nthe\\n \\nnumber\\n \\nof\\n \\nchunks\\n \\nthat\\n \\nare\\n \\nretrieved.\\n \\nLarger\\n \\ncontext\\n \\nwindows\\n \\nlet\\n \\nyou\\n \\nfeed\\n \\nmore\\n \\nretrieved\\n \\nchunks\\n \\nto\\n \\nthe\\n \\nLLM,\\n \\nwhich\\n \\nincreases\\n \\nthe\\n \\nchance\\n \\nof\\n \\nincluding\\n \\nmore\\n \\nrelevant\\n \\ninformation.\\n \\nThis\\n \\ncould\\n \\nmake\\n \\nthe\\n \\nquality\\n \\nof\\n \\nthe\\n \\ngenerated\\n \\nanswers\\n \\nbetter.\\n \\n  But  after  a  certain  point,  performance  gains  start  to  go  down  because  of  problems  like  \\n\"lost\\n \\nin\\n \\nthe\\n \\nmiddle\"\\n \\nand\\n \\nhigher\\n \\nlatency.\\n  Q21.  How  do  you  choose  values  for  various  LLM  inference  hyperparameters  \\nin\\n \\na\\n \\nRAG\\n \\nsystem?  Temperature  controls  randomness‚Äîlower  values  give  more  focused  and  deterministic  \\nresponses\\n \\nsuitable\\n \\nfor\\n \\ntechnical\\n \\nor\\n \\nprecise\\n \\ntasks,\\n \\nwhile\\n \\nhigher\\n \\nvalues\\n \\nmake\\n \\noutput\\n \\nmore\\n \\ncreative\\n \\nand\\n \\ndiverse.\\n \\n  The  max  tokens  limit  the  length  of  the  output,  making  sure  that  the  answers  are  short  or  \\nlong\\n \\nenough\\n \\ndepending\\n \\non\\n \\nthe\\n \\nuse\\n \\ncase,\\n \\nwith\\n \\na\\n \\ntrade-off\\n \\nbetween\\n \\ncompleteness\\n \\nand\\n \\nlatency.\\n \\nOptimal\\n \\nsettings\\n \\ndepend\\n \\non\\n \\nthe\\n \\nspecific\\n \\napplication\\n \\ncontext\\n \\nand\\n \\nare\\n \\nfound\\n \\nthrough\\n \\niterative\\n \\nexperimentation.\\n \\n \\n Q22.  Compare  reasoning  vs.  non-reasoning  LLMs  for  RAG  systems.  Reasoning  LLMs  such  as  GPT-4o1  and  DeepSeek  R1  are  better  generators  in  RAG  \\nsystems\\n \\nbecause\\n \\nthey\\n \\nhave\\n \\nadvanced\\n \\n\"test-time\\n \\ncompute\"\\n \\nand\\n \\nchain-of-thought\\n \\nfeatures.\\n \\nThese\\n \\nunique\\n \\nabilities\\n \\nallow\\n \\nthem\\n \\nto\\n \\nanalyze\\n \\nthe\\n \\nretrieved\\n \\ninformation\\n \\nmore\\n \\neffectively\\n \\nand\\n \\ndo\\n \\nmulti-step\\n \\nreasoning\\n \\nto\\n \\ncome\\n \\nup\\n \\nwith\\n \\nbetter\\n \\nanswers.\\n  \\n  \\n7                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 8, 'page_label': '9', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           But  non-reasoning  LLMs  are  still  cheaper  and  faster,  which  makes  them  a  good  choice  \\nfor\\n \\nmany\\n \\napplications.\\n \\nIn\\n \\nthe\\n \\nend,\\n \\nthe\\n \\nchoice\\n \\nbetween\\n \\nreasoning\\n \\nand\\n \\nnon-reasoning\\n \\nmodels\\n \\ndepends\\n \\non\\n \\nthe\\n \\nquery\\n \\ncomplexity.\\n  \\n \\n Q23.  What  happens  with  a  weak  generator  LLM  in  a  RAG  system?  \\n A  weak  generator  LLM  may  find  it  difficult  to  understand  the  retrieved  context,  which  \\ncould\\n \\nlead\\n \\nto\\n \\nanswers\\n \\nthat\\n \\nare\\n \\nincomplete\\n \\nor\\n \\nhallucinated.\\n \\nThis\\n \\nmakes\\n \\nthe\\n \\nwhole\\n \\nRAG\\n \\nsystem\\n \\nless\\n \\nuseful\\n \\nbecause\\n \\nthe\\n \\nfinal\\n \\nanswers\\n \\nlack\\n \\ncoherence\\n \\nand\\n \\nfactual\\n \\ncorrectness,\\n \\neven\\n \\nthough\\n \\nthe\\n \\ncontext\\n \\nthat\\n \\nwas\\n \\nretrieved\\n \\nis\\n \\ngood.\\n  So,  in  a  RAG  setup,  a  strong  generator  LLM  is  necessary  to  convert  retrieved  knowledge  \\ninto\\n \\nreliable,\\n \\ncontextually\\n \\nrelevant\\n \\noutputs.\\n     \\n                  ‚òï  Support  the  Author   I  hope  you  found  this  ‚ÄúRAG  Interview  Questions  and  Answers‚Äù  book  highly  \\nuseful.\\n  \\n  I‚Äôve  made  this  book  freely  available  to  help  the  AI  and  NLP  community  grow  \\nand\\n \\nto\\n \\nsupport\\n \\nlearners\\n \\nlike\\n \\nyou.\\n \\nIf\\n \\nyou\\n \\nfound\\n \\nit\\n \\nhelpful\\n \\nand\\n \\nwould\\n \\nlike\\n \\nto\\n \\nshow\\n \\nyour\\n \\nappreciation,\\n \\nyou\\n \\ncan\\n \\nbuy\\n \\nme\\n \\na\\n \\ncoÔ¨Äee\\n \\nto\\n \\nkeep\\n \\nme\\n \\nmotivated\\n \\nin\\n \\ncreating\\n \\nmore\\n \\nfree\\n \\nresources\\n \\nlike\\n \\nthis.\\n  üëâ  Buy  Me  a  CoÔ¨Äee  Your  small  gesture  goes  a  long  way  in  supporting  my  work‚Äîthank  you  for  \\nbeing\\n \\npart\\n \\nof\\n \\nthis\\n \\njourney!\\n \\nüôè\\n  ‚Äî  Kalyan  KS      \\n8                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 9, 'page_label': '10', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q24.  How  do  you  handle  ambiguous  or  vague  user  queries  in  RAG  systems?   Issues  with  ambiguous  or  vague  user  queries  in  RAG  systems  include  retrieval  of  \\nirrelevant\\n \\ninformation,\\n \\nincomplete\\n \\nanswers,\\n \\nand\\n \\nincreased\\n \\nrisk\\n \\nof\\n \\nhallucination\\n \\ndue\\n \\nto\\n \\nthe\\n \\nlack\\n \\nof\\n \\nspecificity.\\n \\n  The  most  common  strategy  to  handle  ambiguous  or  vague  user  queries  is  query  \\nrewriting.\\n  \\nQuery\\n \\nrewriting\\n \\ntransforms\\n \\nunclear\\n \\nqueries\\n \\ninto\\n \\nprecise\\n \\nand\\n \\nfocused\\n \\nqueries,\\n \\nthereby\\n \\nenhancing\\n \\nretrieval\\n \\nquality\\n \\nand\\n \\nleading\\n \\nto\\n \\nmore\\n \\naccurate,\\n \\ngrounded\\n \\nresponses.\\n  Q25.  What  are  the  diÔ¨Äerent  query  transformation  techniques  that  enhance  \\nuser\\n \\nqueries\\n \\nin\\n \\nRAG?  Different  query  transformation  techniques  in  RAG  include  query  rewriting,  query  \\nexpansion,\\n \\nquery\\n \\ndecomposition,\\n \\nand\\n \\nHyDE\\n \\nto\\n \\nenhance\\n \\nretrieval\\n \\nrelevance\\n \\nand\\n \\ncontext\\n \\nprecision.\\n \\n  Query  Rewriting:  Rewrites  the  initial  user  query  to  make  it  more  specific  and  detailed,  \\nboosting\\n \\nretrieval\\n \\naccuracy.\\n  Query  Expansion  using  Step-Back  Prompting:  Generates  a  broader,  generalized  version  \\nof\\n \\nthe\\n \\nquery.\\n  Query  Decomposition:  Divides  complex  queries  into  simpler  sub-queries  to  ensure  \\ncomprehensive\\n \\ncoverage\\n \\nand\\n \\nmore\\n \\nprecise\\n \\nretrieval\\n \\nfor\\n \\neach\\n \\ncomponent\\n \\nquestion.\\n  HyDE  (Hypothetical  Document  Embedding):  Synthesizes  a  hypothetical  answer  to  the  \\nquery\\n \\nand\\n \\nuses\\n \\nit\\n \\nas\\n \\na\\n \\nretrieval\\n \\nquery\\n \\nto\\n \\nget\\n \\nmore\\n \\nrelevant\\n \\ndocument\\n \\nchunks.\\n  Q26.  What  are  the  pros  and  cons  of  query  transformation  techniques?  Query  transformation  techniques  in  RAG  systems  offer  significant  advantages,  such  as  \\nimproved\\n \\nretrieval\\n \\naccuracy\\n \\nleading\\n \\nto\\n \\nmore\\n \\nrelevant\\n \\nand\\n \\ncontextually\\n \\naccurate\\n \\nresponses.\\n \\n  However,  their  downsides  include  increased  computational  cost,  added  latency,  and  \\npotential\\n \\nnoise\\n \\nfrom\\n \\noverexpansion.\\n \\nOver\\n \\nexpansion\\n \\nrisks\\n \\nretrieving\\n \\nnoisy\\n \\nor\\n \\noff-topic\\n \\ndocuments,\\n \\nwhile\\n \\ncomplex\\n \\nmethods\\n \\nlike\\n \\nquery\\n \\ndecomposition\\n \\nrequire\\n \\ncareful\\n \\nhandling\\n \\nto\\n \\nensure\\n \\nsubqueries\\n \\nalign\\n \\nwith\\n \\nthe\\n \\noriginal\\n \\nintent.\\n \\n  \\n9                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Some  strategies  may  also  require  substantial  prompt  engineering  and  continuous  \\noptimization\\n \\nto\\n \\nmatch\\n \\ndiverse\\n \\nquery\\n \\nscenarios.\\n \\nBalancing\\n \\neffectiveness\\n \\nand\\n \\nefficiency\\n \\nis\\n \\ncritical\\n \\nto\\n \\navoid\\n \\ndiminishing\\n \\nreturns.\\n  Q27.  Explain  how  the  HyDE  query  transformation  technique  works.  The  HyDE  (Hypothetical  Document  Embedding)  technique  improves  RAG  retrieval  by  \\ntransforming\\n \\nthe\\n \\nuser\\n \\nquery\\n \\ninto\\n \\na\\n \\nhypothetical\\n \\nanswer\\n \\nbefore\\n \\nembedding\\n \\nit.\\n \\nRather\\n \\nthan\\n \\ndirectly\\n \\nsearching\\n \\nwith\\n \\nthe\\n \\nquery\\n \\nembedding,\\n \\nthe\\n \\nHyDE\\n \\ntechnique\\n \\nutilizes\\n \\na\\n \\nlarge\\n \\nlanguage\\n \\nmodel\\n \\n(LLM)\\n \\nto\\n \\ncreate\\n \\na\\n \\nbrief,\\n \\nplausible\\n \\ndocument\\n \\nthat\\n \\ncould\\n \\npotentially\\n \\nanswer\\n \\nthe\\n \\nquery.\\n \\n  This  synthetic  document  is  then  encoded  into  an  embedding  and  used  for  retrieval,  \\nleading\\n \\nto\\n \\nbetter\\n \\nsemantic\\n \\nalignment\\n \\nwith\\n \\nactual\\n \\ndocument\\n \\nchunks\\n \\nin\\n \\nthe\\n \\ndatabase.\\n \\nAs\\n \\na\\n \\nresult,\\n \\nHyDE\\n \\nenhances\\n \\nretrieval\\n \\nquality,\\n \\nespecially\\n \\nfor\\n \\nvague\\n \\nor\\n \\nunderspecified\\n \\nqueries.\\n  Q28.  Explain  how  the  HyPE  technique  works  in  RAG.  The  HyPE  (Hypothetical  Prompt  Embedding)  technique  improves  retrieval  accuracy  by  \\naddressing\\n \\nthe\\n \\nsemantic\\n \\nmismatch\\n \\nbetween\\n \\nuser\\n \\nqueries\\n \\nand\\n \\ndocument\\n \\nchunks.\\n \\n  Unlike  HyDE,  which  generates  hypothetical  answer  documents  at  query  time,  HyPE  \\nprecomputes\\n \\nhypothetical\\n \\nquestions\\n \\nfor\\n \\neach\\n \\ndocument\\n \\nchunk\\n \\nduring\\n \\nthe\\n \\nindexing\\n \\nphase.\\n \\nThese\\n \\nquestions\\n \\nare\\n \\ndesigned\\n \\nto\\n \\ncapture\\n \\nthe\\n \\nkey\\n \\nconcepts\\n \\nin\\n \\nthe\\n \\nchunk,\\n \\ntransforming\\n \\nretrieval\\n \\ninto\\n \\na\\n \\n\"question-to-question\"\\n \\nmatching\\n \\nprocess,\\n \\nwhich\\n \\nreduces\\n \\nlatency\\n \\nand\\n \\nimproves\\n \\nretrieval.\\n  Q29.  Compare  HyPE  and  HyDE  techniques  in  RAG.  HyDE  (Hypothetical  Document  Embedding)  and  HyPE  (Hypothetical  Prompt  \\nEmbedding)\\n \\nenhance\\n \\nRAG\\n \\nby\\n \\naddressing\\n \\nthe\\n \\nsemantic\\n \\ngap\\n \\nbetween\\n \\nuser\\n \\nqueries\\n \\nand\\n \\ndocument\\n \\nchunks,\\n \\nbut\\n \\nthey\\n \\ndiffer\\n \\nin\\n \\napproach.\\n  Timing:  HyPE  generates  hypothetical  questions  during  indexing,  while  HyDE  generates  \\nhypothetical\\n \\nanswer\\n \\ndocuments\\n \\nat\\n \\nquery\\n \\ntime.\\n  Efficiency:  HyPE  reduces  runtime  latency  by  avoiding  LLM  calls  during  retrieval,  unlike  \\nHyDE,\\n \\nwhich\\n \\nrequires\\n \\nan\\n \\nLLM\\n \\ncall\\n \\nper\\n \\nquery.\\n  \\n10                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Focus:  HyPE  focuses  on  question-question  matching,  while  HyDE  focuses  on  \\nanswer-answer\\n \\nmatching.\\n  While  HyDE  is  flexible  for  diverse  queries,  HyPE‚Äôs  pre-indexed  approach  is  more  \\nefficient\\n \\nfor\\n \\nreal-time\\n \\napplications.\\n  Q30.  To  minimize  RAG  system  latency,  which  pre-retrieval  enhancement  \\ntechnique\\n \\nwill\\n \\nyou\\n \\nchoose?  To  minimize  RAG  system  latency,  I  would  choose  the  HyPE  (Hypothetical  Prompt  \\nEmbedding)\\n \\ntechnique.\\n \\nUnlike\\n \\nquery\\n \\ntransformation\\n \\ntechniques\\n \\nsuch\\n \\nas\\n \\nquery\\n \\nrewriting,\\n \\nquery\\n \\nexpansion,\\n \\nquery\\n \\ndecomposition,\\n \\nor\\n \\nHyDE,\\n \\nwhich\\n \\nrequire\\n \\nLLM\\n \\ncalls\\n \\nat\\n \\nquery\\n \\ntime\\n \\nand\\n \\nincrease\\n \\nlatency,\\n \\nHyPE\\n \\nprecomputes\\n \\nhypothetical\\n \\nquestions\\n \\nfor\\n \\ndocument\\n \\nchunks\\n \\nduring\\n \\nthe\\n \\nindexing\\n \\nphase.\\n \\n  This  question-to-question  matching  approach  reduces  runtime  latency  by  avoiding  \\nreal-time\\n \\nLLM\\n \\ncalls,\\n \\nmaking\\n \\nit\\n \\nmore\\n \\nefficient\\n \\nfor\\n \\nreal-time\\n \\napplications\\n \\nwhile\\n \\nmaintaining\\n \\nhigh\\n \\nretrieval\\n \\naccuracy.\\n \\nBy\\n \\nshifting\\n \\nthe\\n \\ncomputational\\n \\neffort\\n \\nto\\n \\nindexing,\\n \\nHyPE\\n \\nensures\\n \\nfaster\\n \\nand\\n \\nmore\\n \\nprecise\\n \\ndocument\\n \\nretrieval.\\n  Q31.  What  are  the  diÔ¨Äerent  chunk  enhancement  techniques  in  RAG? \\n The  different  chunk  enhancement  techniques  in  RAG  are  HyPE,  Contextual  Chunk  \\nHeader,\\n \\nand\\n \\nDocument\\n \\nAugmentation.\\n  HyPE  (Hypothetical  Prompt  Embedding)  precomputes  hypothetical  questions  for  each  \\ndocument\\n \\nchunk\\n \\nat\\n \\nindexing\\n \\ntime,\\n \\nenabling\\n \\nretrieval\\n \\nby\\n \\nquestion-to-question\\n \\nmatching,\\n \\nwhich\\n \\nimproves\\n \\nsemantic\\n \\nalignment\\n \\nand\\n \\nretrieval\\n \\naccuracy\\n \\nwithout\\n \\nadding\\n \\nquery-time\\n \\nlatency.\\n  Contextual  Chunk  Header  adds  relevant  contextual  information  such  as  document  titles  \\nor\\n \\nsection\\n \\nheadings\\n \\nto\\n \\neach\\n \\nchunk\\n \\nbefore\\n \\nembedding,\\n \\nhelping\\n \\nretrieval\\n \\nmodels\\n \\nunderstand\\n \\nand\\n \\nrank\\n \\nchunks\\n \\nbetter\\n \\nwhen\\n \\nchunk\\n \\ntext\\n \\nalone\\n \\nis\\n \\nambiguous.\\n  Document  Augmentation  enhances  chunks  by  including  additional  metadata  and  \\nenhances\\n \\nretrieval\\n \\nquality.\\n   \\n11                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 12, 'page_label': '13', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q32.  What  are  the  pros  and  cons  of  chunk  enhancement  techniques  in  \\nRAG?  Chunk  enhancement  techniques  in  RAG,  such  as  HyPE,  Contextual  Chunk  Header,  and  \\nDocument\\n \\nAugmentation,\\n \\nimprove\\n \\nretrieval\\n \\naccuracy\\n \\nby\\n \\nenhancing\\n \\nsemantic\\n \\nalignment,\\n \\npreserving\\n \\ncontext,\\n \\nand\\n \\nbridging\\n \\nquery-document\\n \\nchunk\\n \\ngaps,\\n \\nleading\\n \\nto\\n \\nbetter\\n \\ngeneration\\n \\nperformance.\\n \\n  HyPE  boosts  relevance  through  precomputed  question  embeddings  without  query-time  \\nlatency,\\n \\nContextual\\n \\nChunk\\n \\nHeader\\n \\nclarifies\\n \\nambiguous\\n \\nchunks\\n \\nwith\\n \\ndocument\\n \\nor\\n \\nsection\\n \\ntitles,\\n \\nand\\n \\nDocument\\n \\nAugmentation\\n \\nenriches\\n \\nchunks\\n \\nwith\\n \\nadditional\\n \\nmetadata.\\n \\n  However,  these  methods  increase  indexing  complexity  and  storage  requirements,  \\npotentially\\n \\nraising\\n \\ncomputational\\n \\ncosts.\\n \\nBalancing\\n \\nenhanced\\n \\nretrieval\\n \\nquality\\n \\nwith\\n \\nresource\\n \\ndemands\\n \\nis\\n \\na\\n \\nkey\\n \\nconsideration.\\n  Q33.  Explain  how  the  contextual  chunk  header  technique  enhances  RAG  \\nretrieval.\\n  The  Contextual  Chunk  Header  technique  in  RAG  enhances  retrieval  by  adding   \\ndocument\\n \\ntitles,\\n \\nsection\\n \\nheadings,\\n \\nor\\n \\nsummaries\\n \\nto\\n \\neach\\n \\nchunk\\n \\nbefore\\n \\nembedding,\\n \\nproviding\\n \\ncritical\\n \\ncontext\\n \\nthat\\n \\nclarifies\\n \\nambiguous\\n \\nor\\n \\nisolated\\n \\nchunk\\n \\ncontent.\\n  \\nThis\\n \\nadditional\\n \\ninformation\\n \\nhelps\\n \\nthe\\n \\nretrieval\\n \\nmodel\\n \\nbetter\\n \\nunderstand\\n \\nthe\\n \\nchunk‚Äôs\\n \\nrelevance\\n \\nto\\n \\na\\n \\nquery,\\n \\nimproving\\n \\nsemantic\\n \\nalignment\\n \\nand\\n \\nranking\\n \\naccuracy.\\n \\n  Q34.  What  are  some  common  chunking  methods  used  in  RAG?  Common  chunking  methods  used  in  RAG  are  fixed-size  chunking,  recursive  chunking,  \\nsemantic\\n \\nchunking,\\n \\nand\\n \\nagentic\\n \\nchunking.\\n \\n  Fixed-size  chunking  divides  text  into  uniform  segments  based  on  a  predefined  token  or  \\ncharacter\\n \\nlength,\\n \\noften\\n \\nincorporating\\n \\noverlap\\n \\nto\\n \\nmaintain\\n \\ncontext.\\n \\n  Recursive  chunking  iteratively  splits  text  using  natural  separators  like  paragraphs  or  \\nsentences\\n \\nto\\n \\npreserve\\n \\nlogical\\n \\nboundaries.\\n \\nSemantic\\n \\nchunking\\n \\ngroups\\n \\ntext\\n \\nbased\\n \\non\\n \\nsemantic\\n \\nsimilarity\\n \\nusing\\n \\nembeddings,\\n \\ncreating\\n \\ncoherent,\\n \\nmeaning-based\\n \\nchunks.\\n \\n  Agentic  chunking  leverages  AI  agents  to  dynamically  segment  text  into  task-oriented,  \\nsemantically\\n \\ncoherent\\n \\nchunks,\\n \\noften\\n \\nwith\\n \\nmetadata\\n \\nto\\n \\nenhance\\n \\nretrieval\\n \\nrelevance.\\n  12                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 13, 'page_label': '14', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q35.  What  are  the  criteria  to  choose  a  speciÔ¨Åc  chunking  method  in  RAG? \\n The  criteria  for  choosing  a  specific  chunking  method  in  RAG  include  the  nature  and  \\nstructure\\n \\nof\\n \\nthe\\n \\nsource\\n \\ndocuments,\\n \\ncapabilities\\n \\nof\\n \\nthe\\n \\nembedding\\n \\nmodel,\\n \\nand\\n \\nthe\\n \\nspecific\\n \\ntask\\n \\nor\\n \\napplication\\n \\nneeds.\\n  \\n  For  structured  or  well-formatted  data,  semantic  or  agentic  chunking  ensures  logical  \\nboundaries\\n \\nand\\n \\ncontext\\n \\npreservation.\\n \\nThe\\n \\nchunk\\n \\nsize\\n \\nmust\\n \\nbalance\\n \\nbetween\\n \\nbeing\\n \\nlarge\\n \\nenough\\n \\nto\\n \\ncapture\\n \\nmeaningful\\n \\ncontext\\n \\nand\\n \\nsmall\\n \\nenough\\n \\nto\\n \\nfit\\n \\nwithin\\n \\nmodel\\n \\nconstraints\\n \\nfor\\n \\nefficient\\n \\nprocessing.\\n \\n  Task  specificity  matters  since  complex  tasks  may  require  semantic  or  agentic  chunking  \\nfor\\n \\nbetter\\n \\ncontext\\n \\nand\\n \\nrelevance,\\n \\nwhile\\n \\nsimpler\\n \\ncases\\n \\ncan\\n \\nuse\\n \\nfixed-size\\n \\nchunking.\\n \\n  Ultimately,  the  chunking  method  should  balance  retrieval  relevance,  context  \\ncompleteness,\\n \\nand\\n \\ncomputational\\n \\nefficiency.\\n  Q36.  Explain  the  pros  and  cons  of  semantic  chunking.  Semantic  chunking  groups  text  based  on  meaning,  creating  coherent  chunks  that  \\nenhance\\n \\nretrieval\\n \\nrelevance\\n \\nand\\n \\ncontext\\n \\npreservation.\\n \\n  Pros:  It  aligns  chunks  with  natural  topic  shifts,  improving  the  quality  of  retrieved  \\ncontent\\n \\nfor\\n \\ncomplex\\n \\nqueries,\\n \\nand\\n \\nreduces\\n \\ninformation\\n \\nloss\\n \\nacross\\n \\nboundaries.\\n \\n  Cons:  It  is  computationally  intensive,  requiring  embedding  models.  Additionally,  it  may  \\nstruggle\\n \\nwith\\n \\nhighly\\n \\ncomplex\\n \\nor\\n \\npoorly\\n \\nstructured\\n \\ndocuments\\n \\nwhere\\n \\nsemantic\\n \\nboundaries\\n \\nare\\n \\nunclear.\\n   Q37.  How  does  the  chunking  strategy  diÔ¨Äer  when  dealing  with  structured  \\ndocuments\\n \\n(like\\n \\nPDFs\\n \\nwith\\n \\ntables\\n \\nand\\n \\nÔ¨Ågures)\\n \\nversus\\n \\nplain\\n \\ntext\\n \\ndocuments?  Chunking  strategies  for  structured  documents  like  PDFs  with  tables  and  figures  differ  \\nsignificantly\\n \\nfrom\\n \\nplain\\n \\ntext\\n \\nchunking\\n \\ndue\\n \\nto\\n \\nthe\\n \\nneed\\n \\nto\\n \\npreserve\\n \\ncomplex\\n \\nlayouts\\n \\nand\\n \\nrelationships.\\n  \\nFor\\n \\nstructured\\n \\ndocuments,\\n \\nthe\\n \\nchunking\\n \\nstrategy\\n \\nmust\\n \\nrespect\\n \\ndocument\\n \\nelements\\n \\nsuch\\n \\nas\\n \\ntables,\\n \\nfigures,\\n \\nheaders,\\n \\nand\\n \\npages\\n \\nto\\n \\nmaintain\\n \\ncontext\\n \\nand\\n \\nsemantic\\n \\nmeaning.\\n \\n  \\n13                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 14, 'page_label': '15', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Agentic  and  recursive  chunking  are  more  suitable  for  structured  documents  due  to  their  \\nflexibility\\n \\nin\\n \\nrespecting\\n \\nstructure\\n \\nand\\n \\ncontext.\\n \\nFixed-size\\n \\nand\\n \\nsemantic\\n \\nchunking\\n \\nare\\n \\noften\\n \\nbetter\\n \\nsuited\\n \\nfor\\n \\nplain\\n \\ntext\\n \\ndocuments\\n \\nwhere\\n \\nsemantic\\n \\ncoherence\\n \\nand\\n \\nsimplicity\\n \\nare\\n \\nprioritized.\\n                                                            LLM  Engineer  Toolkit  ü§ñ  This  repository  contains  a  curated  list  of  120+  LLM  libraries  category  \\nwise.\\n \\n                This  repository  is  highly  useful  for  AI/ML  Engineers.     \\n          \\n14                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 15, 'page_label': '16', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q38.  What  are  the  possible  reasons  for  the  poor  performance  of  a  RAG  \\nretriever?  The  possible  reasons  for  the  poor  performance  of  a  RAG  retriever  are  an  outdated  or  \\nincomplete\\n \\nknowledge\\n \\nbase,\\n \\na\\n \\nweak\\n \\nretrieval\\n \\nmodel,\\n \\nlow-quality\\n \\nembeddings,\\n \\nand\\n \\nlack\\n \\nof\\n \\ndomain-specific\\n \\nfine-tuning.\\n  An  outdated  or  incomplete  knowledge  base  prevents  the  retriever  from  accessing  recent  \\nor\\n \\nrelevant\\n \\ninformation,\\n \\nlimiting\\n \\nanswer\\n \\naccuracy.\\n \\nA\\n \\nweak\\n \\nretrieval\\n \\nmodel,\\n \\nsuch\\n \\nas\\n \\nusing\\n \\nTF-IDF\\n \\nor\\n \\nBM25\\n \\ninstead\\n \\nof\\n \\ndense\\n \\nvector\\n \\nmodels,\\n \\nleads\\n \\nto\\n \\nless\\n \\neffective\\n \\nretrieval\\n \\nof\\n \\nrelevant\\n \\ncontext.\\n  Low-quality  embeddings  reduce  the  semantic  understanding  between  queries  and  \\ndocument\\n \\nchunks,\\n \\ncausing\\n \\nmismatches.\\n \\nLack\\n \\nof\\n \\ndomain-specific\\n \\nfine-tuning\\n \\nresults\\n \\nin\\n \\nretrieval\\n \\nerrors\\n \\nbecause\\n \\nthe\\n \\nembedding\\n \\nmodel\\n \\ndoesn‚Äôt\\n \\nfully\\n \\ncapture\\n \\nthe\\n \\nnuances\\n \\nor\\n \\nterminology\\n \\nof\\n \\nthe\\n \\ntarget\\n \\ndomain.\\n  Q39.  What  happens  with  a  weak  retriever  in  Retrieval-Augmented  \\nGeneration\\n \\n(RAG)\\n \\nsystems?  A  weak  retriever  in  RAG  systems  leads  to  the  retrieval  of  irrelevant  or  noisy  document  \\nchunks.\\n \\nThis\\n \\ncan\\n \\nsignificantly\\n \\ndegrade\\n \\nthe\\n \\nquality\\n \\nof\\n \\ngenerated\\n \\nanswers,\\n \\nas\\n \\nthe\\n \\nRAG\\n \\ngenerator\\n \\nrelies\\n \\nheavily\\n \\non\\n \\nthe\\n \\nretrieved\\n \\ncontext.\\n \\nThe\\n \\npresence\\n \\nof\\n \\nirrelevant\\n \\nor\\n \\nnoisy\\n \\ndocument\\n \\nchunks\\n \\nin\\n \\nthe\\n \\ncontext\\n \\nbecause\\n \\nof\\n \\npoor\\n \\nretrieval\\n \\ncauses\\n \\nthe\\n \\ngenerator\\n \\nmodel\\n \\nto\\n \\nproduce\\n \\nanswers\\n \\nthat\\n \\nare\\n \\ninaccurate\\n \\nor\\n \\nhallucinated\\n \\nwhile\\n \\nstill\\n \\nappearing\\n \\nfluent.\\n \\n  Therefore,  strong  retrievers  are  necessary  to  provide  the  most  relevant  context  and  \\nensure\\n \\nfactual\\n \\nand\\n \\nrelevant\\n \\noutputs\\n \\nin\\n \\nRAG\\n \\nsystems.\\n  Q40.  What  are  the  common  retrieval  approaches  used  in  RAG  systems?  Common  retrieval  approaches  in  RAG  systems  include  dense  retrieval,  sparse  retrieval,  \\nand\\n \\nhybrid\\n \\nretrieval.\\n \\nDense\\n \\nretrieval\\n \\nuses\\n \\nembeddings\\n \\nto\\n \\ncapture\\n \\nsemantic\\n \\nsimilarity,\\n \\nenabling\\n \\neffective\\n \\nquery\\n \\nmatching\\n \\nto\\n \\nrelevant\\n \\ndocument\\n \\nchunks.\\n \\nSparse\\n \\nretrieval\\n \\nrelies\\n \\non\\n \\ntraditional\\n \\nmethods\\n \\nlike\\n \\nTF-IDF\\n \\nor\\n \\nBM25,\\n \\nfocusing\\n \\non\\n \\nkeyword-based\\n \\nmatching\\n \\nfor\\n \\nefficiency.\\n \\n  Hybrid  retrieval  combines  dense  and  sparse  methods  to  balance  semantic  \\nunderstanding\\n \\nand\\n \\ncomputational\\n \\nspeed.\\n \\nThese\\n \\napproaches\\n \\nensure\\n \\nrelevant\\n \\ncontext\\n \\nis\\n \\nretrieved\\n \\nfor\\n \\ngenerating\\n \\naccurate\\n \\nresponses\\n \\nin\\n \\nRAG\\n \\nsystems.\\n 15                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            Q41.  What  are  some  common  challenges  in  RAG  retrieval?  Common  challenges  in  RAG  retrieval  include  ineffective  query  understanding,  \\nscalability\\n \\nissues,\\n \\ncontext\\n \\nfragmentation,\\n \\nand\\n \\nhandling\\n \\nmultimodal\\n \\ndata.\\n  \\nIneffective\\n \\nquery\\n \\nunderstanding\\n \\nleads\\n \\nto\\n \\nmisinterpreting\\n \\nuser\\n \\nintent,\\n \\nresulting\\n \\nin\\n \\nirrelevant\\n \\nretrieved\\n \\ndocument\\n \\nchunks.\\n  \\n  Scalability  issues  arise  when  large-scale  data  retrieval  slows  performance  or  overwhelms  \\nthe\\n \\nsystem.\\n   \\nContext\\n \\nfragmentation\\n \\nhappens\\n \\nwhen\\n \\nretrieved\\n \\nchunks\\n \\nlack\\n \\nsufficient\\n \\ncontext,\\n \\nlowering\\n \\nresponse\\n \\nquality.\\n   \\nHandling\\n \\nmultimodal\\n \\ndata\\n \\nis\\n \\nchallenging\\n \\ndue\\n \\nto\\n \\ncomplexities\\n \\nin\\n \\nintegrating\\n \\ntext,\\n \\nimages,\\n \\nor\\n \\nother\\n \\nformats\\n \\neffectively.\\n  Q42.  What  are  the  key  metrics  for  evaluating  retrieval  quality  in  RAG?  Key  metrics  for  evaluating  retrieval  quality  in  RAG  systems  are  precision,  recall,  Mean  \\nReciprocal\\n \\nRank\\n \\n(MRR),\\n \\nand\\n \\nNormalized\\n \\nDiscounted\\n \\nCumulative\\n \\nGain\\n \\n(NDCG).\\n \\nPrecision\\n \\nmeasures\\n \\nthe\\n \\nproportion\\n \\nof\\n \\nretrieved\\n \\ndocument\\n \\nchunks\\n \\nthat\\n \\nare\\n \\nrelevant,\\n \\nwhile\\n \\nrecall\\n \\nassesses\\n \\nthe\\n \\nproportion\\n \\nof\\n \\nrelevant\\n \\ndocument\\n \\nchunks\\n \\nretrieved\\n \\nfrom\\n \\nthe\\n \\ntotal\\n \\navailable.\\n \\n  MRR  evaluates  the  ranking  quality  by  considering  the  position  of  the  first  relevant  \\ndocument\\n \\nchunks,\\n \\nand\\n \\nNDCG\\n \\naccounts\\n \\nfor\\n \\nthe\\n \\nrelevance\\n \\nand\\n \\nranking\\n \\nof\\n \\nretrieved\\n \\ndocuments.\\n \\nThese\\n \\nmetrics\\n \\ncollectively\\n \\nensure\\n \\nthe\\n \\nretriever\\n \\neffectively\\n \\nidentifies\\n \\nand\\n \\nranks\\n \\nrelevant\\n \\ninformation.\\n  Q43.  What  are  embeddings,  and  how  are  they  utilized  in  RAG  retrieval?  Embeddings  are  numerical  vector  representations  of  text  that  capture  the  semantic  \\nmeaning\\n \\nand\\n \\nrelationships\\n \\nof\\n \\nthe\\n \\ndata\\n \\nin\\n \\na\\n \\nhigh-dimensional\\n \\nspace.\\n \\nIn\\n \\nRetrieval-Augmented\\n \\nGeneration\\n \\n(RAG),\\n \\nembeddings\\n \\nare\\n \\nused\\n \\nto\\n \\nconvert\\n \\nboth\\n \\nthe\\n \\nuser\\n \\nquery\\n \\nand\\n \\ndocument\\n \\nchunks\\n \\ninto\\n \\nvectors,\\n \\nenabling\\n \\nsemantic\\n \\nsearch\\n \\nby\\n \\ncomparing\\n \\nthese\\n \\nvectors\\n \\nfor\\n \\nsimilarity.\\n \\n  This  process  allows  RAG  systems  to  retrieve  the  most  relevant  and  contextually  \\nappropriate\\n \\ndocument\\n \\nchunks\\n \\nfrom\\n \\na\\n \\nknowledge\\n \\nbase,\\n \\nwhich\\n \\nare\\n \\nthen\\n \\nused\\n \\nas\\n \\ncontext\\n \\nto\\n \\ngenerate\\n \\naccurate\\n \\nand\\n \\ngrounded\\n \\nresponses.\\n \\nThus,\\n \\nembeddings\\n \\nform\\n \\nthe\\n \\nbackbone\\n \\nof\\n \\nRAG\\n \\nretrieval\\n \\nby\\n \\nenabling\\n \\nefficient,\\n \\nmeaning-driven\\n \\nretrieval\\n \\nbeyond\\n \\nsimple\\n \\nkeyword\\n \\nmatching.\\n  \\n16                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 17, 'page_label': '18', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q44.  What  are  the  key  considerations  when  choosing  an  embedding  model  \\nfor\\n \\na\\n \\nRAG\\n \\nsystem?  When  choosing  an  embedding  model  for  a  RAG  system,  key  considerations  include    (i)  the  model's  domain  relevance  to  ensure  it  accurately  captures  domain-specific  \\nsemantics,\\n \\n (ii)  embedding  dimensionality,  which  balances  retrieval  precision  against  computational  \\nand\\n \\nstorage\\n \\ncosts,\\n \\nand\\n \\n (ii)  embedding  model  performance  on  the  specific  dataset  to  ensure  good  retrieval  \\nquality.\\n \\nThis\\n \\nis\\n \\nnecessary,\\n \\nas\\n \\nthe\\n \\nreal-world\\n \\ndata\\n \\noften\\n \\ndiffer\\n \\nfrom\\n \\nacademic\\n \\ndatasets.\\n \\n (iv)  Additionally,  factors  such  as  embedding  model  size,  API  availability,  latency,  cost  \\nimplications,\\n \\nand\\n \\nlicensing\\n \\nshould\\n \\nbe\\n \\nconsidered\\n \\nto\\n \\nalign\\n \\nwith\\n \\ninfrastructure\\n \\nconstraints\\n \\nand\\n \\nuse\\n \\ncase\\n \\nrequirements.\\n \\n  Choosing  the  right  embedding  model  directly  impacts  the  effectiveness  and  scalability  of  \\nthe\\n \\nRAG\\n \\nsystem.\\n  Q45.  What  is  a  VectorDB,  and  how  is  it  utilized  in  RAG  retrieval?  A  vector  database,  or  VectorDB  for  short,  is  a  specialized  database  designed  to  store  and  \\nretrieve\\n \\nhigh-dimensional\\n \\nvector\\n \\nembeddings.\\n \\nIn\\n \\nRAG\\n \\nretrieval,\\n \\nVectorDB\\n \\nis\\n \\nutilized\\n \\nto\\n \\nefficiently\\n \\nperform\\n \\nsemantic\\n \\nsearches\\n \\nby\\n \\nmatching\\n \\nthe\\n \\nvector\\n \\nrepresentation\\n \\nof\\n \\na\\n \\nuser\\n \\nquery\\n \\nwith\\n \\nthe\\n \\nclosest\\n \\nvectors\\n \\nstored\\n \\nin\\n \\nthe\\n \\ndatabase,\\n \\nthereby\\n \\nretrieving\\n \\nthe\\n \\nmost\\n \\ncontextually\\n \\nrelevant\\n \\ndocument\\n \\nchunks.\\n \\n  VectorDBs  enable  scalable  and  fast  similarity  search,  which  is  crucial  for  the  RAG  \\nsystems.\\n \\n Q46.  Explain  the  role  of  ANN  (Approximate  Nearest  Neighbor)  search  \\nalgorithms\\n \\nin\\n \\nRAG\\n \\nretrieval. \\n Approximate  Nearest  Neighbor  (ANN)  search  algorithms  play  a  crucial  role  in  RAG  \\nretrieval\\n \\nby\\n \\nenabling\\n \\nfast\\n \\nand\\n \\nscalable\\n \\nsearch\\n \\nof\\n \\nrelevant\\n \\ndocument\\n \\nchunks\\n \\nwithin\\n \\nlarge\\n \\nvector\\n \\ndatabases.\\n  \\nApproximate\\n \\nNearest\\n \\nNeighbor\\n \\n(ANN)\\n \\nalgorithms\\n \\nenable\\n \\nfast\\n \\nsearch\\n \\nin\\n \\nRAG\\n \\nretrieval\\n \\nby\\n \\nquickly\\n \\nnarrowing\\n \\ndown\\n \\nthe\\n \\nsearch\\n \\nspace\\n \\nto\\n \\na\\n \\nsmall\\n \\nsubset\\n \\nof\\n \\ncandidate\\n \\nvectors\\n \\ninstead\\n \\nof\\n \\nscanning\\n \\nall\\n \\nvectors.\\n \\n  \\n17                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 18, 'page_label': '19', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           This  reduces  the  number  of  comparisons  needed,  significantly  speeding  up  retrieval  \\nwhile\\n \\nmaintaining\\n \\ngood\\n \\nenough\\n \\naccuracy\\n \\nfor\\n \\nrelevant\\n \\ndocument\\n \\nchunks\\n \\nmatching.\\n \\nThis\\n \\nbalance\\n \\nof\\n \\nspeed\\n \\nand\\n \\nprecision\\n \\nis\\n \\ncrucial\\n \\nfor\\n \\nreal-time\\n \\nand\\n \\nlarge-scale\\n \\nRAG\\n \\nsystems.\\n  Q47.  Explain  the  step-by-step  working  of  ANN  algorithms  for  fast  search  in  \\nRAG\\n \\nretrieval.  ANN  algorithms  for  fast  search  in  RAG  retrieval  involve  four  steps  namely  -  Encoding,  \\nIndexing,\\n \\nNavigating,\\n \\nRetrieving.\\n \\n  (i)  Encoding:  Convert  document  chunks  and  queries  into  vector  representations.   (ii)  Indexing:  Organize  these  vectors  into  a  specialized  data  structure  (like  graphs  or  \\nhash\\n \\ntables)\\n \\nfor\\n \\nquick\\n \\nlookup.\\n (iii)  Navigating:  Efficiently  explore  the  index  to  find  vectors  close  to  the  query  without  \\nchecking\\n \\nall\\n \\ndata\\n \\npoints.\\n \\n (iv)  Retrieving:  Return  the  closest  approximate  neighbors  that  provide  relevant  \\ninformation\\n \\nfor\\n \\nRAG\\n \\nretrieval.\\n \\n  This  approach  balances  search  speed  and  accuracy,  enabling  fast  retrieval  in  large-scale  \\nRAG\\n \\nsystems.\\n \\n  Q48.  What  are  the  typical  distance  metrics  used  for  similarity  search  in  \\nvector\\n \\ndatabases,\\n \\nand\\n \\nwhy\\n \\nare\\n \\nthey\\n \\nchosen?  Typical  distance  metrics  used  in  vector  databases  for  similarity  search  are  Euclidean  \\ndistance,\\n \\ncosine\\n \\nsimilarity,\\n \\nand\\n \\ndot\\n \\nproduct\\n \\nsimilarity.\\n \\nEuclidean\\n \\ndistance\\n \\nmeasures\\n \\nthe\\n \\nstraight-line\\n \\ndistance\\n \\nbetween\\n \\nvectors,\\n \\nmaking\\n \\nit\\n \\nintuitive\\n \\nfor\\n \\ngeometric\\n \\ncloseness.\\n \\nCosine\\n \\nsimilarity\\n \\nevaluates\\n \\nthe\\n \\nangle\\n \\nbetween\\n \\nvectors,\\n \\nfocusing\\n \\non\\n \\ntheir\\n \\ndirection\\n \\n(meaning)\\n \\nrather\\n \\nthan\\n \\nmagnitude.\\n \\n  Dot  product  similarity  considers  both  magnitude  and  direction.  These  metrics  are  \\nselected\\n \\nbased\\n \\non\\n \\nthe\\n \\ndata\\n \\ntype\\n \\nand\\n \\nthe\\n \\nunderlying\\n \\nembedding\\n \\nmodel\\n \\nto\\n \\nensure\\n \\neffective\\n \\nand\\n \\naccurate\\n \\nretrieval.\\n \\n Q49.  Explain  why  cosine  similarity  is  preferred  over  other  distance  metrics  \\nin\\n \\nRAG\\n \\nretrieval. \\n Cosine  similarity  is  preferred  in  RAG  retrieval  because  it  measures  the  angle  between  \\nvectors,\\n \\nfocusing\\n \\non\\n \\ntheir\\n \\ndirection\\n \\n(meaning)\\n \\nrather\\n \\nthan\\n \\nmagnitude.\\n \\nThis\\n \\nmakes\\n \\nit\\n \\n18                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 19, 'page_label': '20', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           effective  for  textual  data  where  the  meaning  lies  more  in  the  direction  of  the  embedding  \\nthan\\n \\nits\\n \\nlength.\\n \\n  Unlike  Euclidean  distance  or  dot  product,  cosine  similarity  is  invariant  to  vector  length,  \\nproviding\\n \\nstable\\n \\nand\\n \\ninterpretable\\n \\nsimilarity\\n \\nscores.\\n \\nThis\\n \\nhelps\\n \\nRAG\\n \\nsystems\\n \\nretrieve\\n \\nrelevant\\n \\ndocument\\n \\nchunks\\n \\neven\\n \\nwhen\\n \\ntext\\n \\nlengths\\n \\nvary,\\n \\nimproving\\n \\naccuracy\\n \\nand\\n \\nconsistency\\n \\nin\\n \\nsemantic\\n \\nsearch.\\n  Q50.  Compare  keyword-based  retrieval  and  semantic  retrieval  in  RAG  \\nsystems.  Keyword-based  retrieval  in  RAG  systems  relies  on  the  exact  or  partial  matching  of  \\nkeywords\\n \\nin\\n \\na\\n \\nquery\\n \\nto\\n \\nfetch\\n \\nrelevant\\n \\ndocument\\n \\nchunks.\\n \\nThis\\n  \\noffers\\n \\nhigh\\n \\nprecision\\n \\nfor\\n \\nqueries\\n \\nwith\\n \\nspecific\\n \\nterms\\n \\nbut\\n \\nmay\\n \\nmiss\\n \\nsemantically\\n \\nrelated\\n \\ninformation.\\n \\nIn\\n \\ncontrast,\\n \\nsemantic\\n \\nretrieval\\n \\nuses\\n \\nembeddings\\n \\nto\\n \\nunderstand\\n \\nthe\\n \\nmeaning\\n \\nbehind\\n \\nthe\\n \\nquery\\n \\nand\\n \\nretrieves\\n \\nconceptually\\n \\nrelevant\\n \\ncontent\\n \\neven\\n \\nwhen\\n \\nkeywords\\n \\ndiffer.\\n  Combining  both  methods  can  balance  precision  and  semantic  understanding  for  \\neffective\\n \\nretrieval\\n \\nin\\n \\nRAG\\n \\nsystems.\\n  Q51.  How  does  hybrid  search  work  in  the  context  of  RAG  retrieval?  Hybrid  search  in  RAG  systems  combines  keyword-based  retrieval  and  semantic  vector  \\nsearch\\n \\nto\\n \\nleverage\\n \\nthe\\n \\nstrengths\\n \\nof\\n \\nboth\\n \\nmethods.\\n \\nIt\\n \\nuses\\n \\na\\n \\nweighted\\n \\nformula\\n \\nto\\n \\nbalance\\n \\nkeyword\\n \\nrelevance\\n \\nand\\n \\nsemantic\\n \\nsimilarity\\n \\nscores.\\n \\nThis\\n \\nallows\\n \\nprecise\\n \\nmatching\\n \\non\\n \\nexact\\n \\nterms\\n \\nwhile\\n \\nalso\\n \\ncapturing\\n \\nconceptually\\n \\nrelated\\n \\ncontent.\\n \\n  Q52.  When  do  you  opt  for  hybrid  search  instead  of  semantic  search?  Hybrid  search  is  preferred  over  pure  semantic  search  when  there  is  a  need  to  balance  \\nexact\\n \\nkeyword\\n \\nmatches\\n \\nwith\\n \\nsemantic\\n \\nunderstanding,\\n \\nespecially\\n \\nin\\n \\nscenarios\\n \\nwhere\\n \\nusers\\n \\nrequire\\n \\nboth\\n \\nprecision\\n \\nand\\n \\ncontextual\\n \\nrelevance.\\n \\n  It  is  ideal  for  domains  where  queries  may  include  specific  terms,  codes,  or  entities  that  \\nmust\\n \\nbe\\n \\nmatched\\n \\nexactly,\\n \\nwhile\\n \\nalso\\n \\nbenefiting\\n \\nfrom\\n \\ncapturing\\n \\nsynonyms\\n \\nor\\n \\nrelated\\n \\nconcepts.\\n \\n  Q53.  How  do  you  balance  relevance  and  diversity  when  retrieving  \\ndocument\\n \\nchunks\\n \\nfor\\n \\nRAG? \\n19                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 20, 'page_label': '21', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            The  retrieval  step  in  RAG  relies  on  cosine  similarity  to  identify  top-k  relevant  document  \\nchunks.\\n \\nHowever,\\n \\none\\n \\ndownside\\n \\nof\\n \\nthis\\n \\napproach\\n \\nis\\n \\nthat\\n \\nit\\n \\ncan\\n \\nreturn\\n \\nhighly\\n \\nsimilar\\n \\ndocument\\n \\nchunks,\\n \\nleading\\n \\nto\\n \\nredundancy.\\n \\n  Balancing  relevance  and  diversity  is  crucial  in  RAG  retrieval  to  include  contextually  \\nimportant\\n \\nyet\\n \\ndiverse\\n \\ndocument\\n \\nchunks,\\n \\npreventing\\n \\nredundancy\\n \\nand\\n \\ncapturing\\n \\na\\n \\nbroader\\n \\nrange\\n \\nof\\n \\nperspectives.\\n \\nThis\\n \\nbalance\\n \\nhelps\\n \\nwhen\\n \\ndealing\\n \\nwith\\n \\ncomplex\\n \\nquestions,\\n \\nas\\n \\ndifferent\\n \\nviewpoints\\n \\nor\\n \\nunique\\n \\ninsights\\n \\ncan\\n \\nimprove\\n \\nthe\\n \\nanswer's\\n \\nquality\\n \\nwhile\\n \\nstill\\n \\nbeing\\n \\naccurate.\\n  Techniques  like  Maximal  Marginal  Relevance  (MMR)  help  to  select  document  chunks  \\nthat\\n \\nare\\n \\nboth\\n \\nhighly\\n \\nrelevant\\n \\nto\\n \\nthe\\n \\nquery\\n \\nand\\n \\ndiverse\\n \\nfrom\\n \\neach\\n \\nother,\\n \\nreducing\\n \\nredundancy.\\n  Q54.   How  do  sparse  embeddings  diÔ¨Äer  from  dense  embeddings  in  terms  of  \\nkeyword\\n \\nmatching\\n \\nand\\n \\nretrieval\\n \\ninterpretability?  Sparse  embeddings  provide  interpretability  and  excel  at  exact  keyword  matching.  These  \\nembeddings\\n \\nrepresent\\n \\ntext\\n \\nas\\n \\nhigh-dimensional\\n \\nvectors\\n \\nwith\\n \\nmany\\n \\nzeros.\\n \\nIn\\n \\nthis,\\n \\neach\\n \\ndimension\\n \\ncorresponds\\n \\nto\\n \\na\\n \\nspecific\\n \\nterm\\n \\nor\\n \\nfeature,\\n \\nmaking\\n \\nretrieval\\n \\nresults\\n \\nmore\\n \\nunderstandable.\\n \\n  In  contrast,  dense  embeddings  are  low-dimensional,  continuous  vectors  with  mostly  \\nnon-zero\\n \\nvalues\\n \\nlearned\\n \\nfrom\\n \\nneural\\n \\nnetworks,\\n \\ncapturing\\n \\nsemantic\\n \\nrelationships\\n \\nand\\n \\ncontext\\n \\nbeyond\\n \\nexact\\n \\nmatches.\\n \\nThis\\n \\nmakes\\n \\ndense\\n \\nembeddings\\n \\nless\\n \\ninterpretable\\n \\nbut\\n \\nmore\\n \\neffective\\n \\nfor\\n \\nretrieving\\n \\nsemantically\\n \\nrelated\\n \\ncontent\\n \\nwhere\\n \\nkeywords\\n \\ndo\\n \\nnot\\n \\nexactly\\n \\noverlap.\\n \\n  Thus,  sparse  embeddings  are  favored  for  precise  keyword-based  retrieval  and  \\ninterpretability,\\n \\nwhile\\n \\ndense\\n \\nembeddings\\n \\nsupport\\n \\nricher,\\n \\ncontext-aware\\n \\nretrieval.\\n \\nHybrid\\n \\napproaches\\n \\nleverage\\n \\nthe\\n \\nstrengths\\n \\nof\\n \\nboth\\n \\nsparse\\n \\nand\\n \\ndense\\n \\nembeddings\\n \\nto\\n \\nenhance\\n \\nretrieval\\n \\nperformance.\\n  Q55.  How  can  Ô¨Åne-tuning  embedding  models  improve  the  retriever‚Äôs  \\nperformance\\n \\nin\\n \\nRAG?\\n \\n \\n20                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 21, 'page_label': '22', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           General  embedding  models  in  RAG  systems  are  trained  on  broad  and  diverse  datasets  \\nthat\\n \\ncapture\\n \\nwide-ranging\\n \\nlanguage\\n \\npatterns.\\n \\nHowever,\\n \\nthey\\n \\noften\\n \\nlack\\n \\ndepth\\n \\nin\\n \\nvocabulary\\n \\nand\\n \\ncontext\\n \\nspecific\\n \\nto\\n \\ndomains.\\n  Fine-tuning  embedding  models  aligns  the  embedding  space  more  closely  with  \\ndomain-specific\\n \\nlanguage\\n \\nand\\n \\ncontext.\\n \\nThis\\n \\nallows\\n \\nthe\\n \\nembedding\\n \\nmodel\\n \\nto\\n \\nbetter\\n \\nrepresent\\n \\ndomain-specific\\n \\nterminology\\n \\nand\\n \\njargon,\\n \\nwhich\\n \\nresults\\n \\nin\\n \\nmore\\n \\nprecise\\n \\nand\\n \\nrelevant\\n \\nretrieval.\\n   Q56.  Design  a  retrieval  strategy  for  a  RAG  system  that  needs  to  handle  \\nboth\\n \\nstructured\\n \\ndata\\n \\n(knowledge\\n \\ngraphs)\\n \\nand\\n \\nunstructured\\n \\ndata\\n \\n(text\\n \\ndocuments)\\n \\nsimultaneously.  A  retrieval  strategy  for  a  RAG  system  handling  both  structured  (knowledge  graphs)  and  \\nunstructured\\n \\ndata\\n \\n(text\\n \\ndocuments)\\n \\ninvolves\\n \\na\\n \\nhybrid\\n \\napproach\\n \\ncombining\\n \\nvector-based\\n \\nsemantic\\n \\nsearch\\n \\nwith\\n \\ngraph-based\\n \\nretrieval\\n \\ntechniques.\\n \\n  The  system  first  indexes  unstructured  text  document  chunks  using  vector  embeddings  \\nfor\\n \\nsemantic\\n \\nsimilarity\\n \\nsearch,\\n \\nwhile\\n \\nstructured\\n \\ndata\\n \\nfrom\\n \\nknowledge\\n \\ngraphs\\n \\nis\\n \\nqueried\\n \\nusing\\n \\ngraph\\n \\ntraversal\\n \\nmethods\\n \\nthat\\n \\nleverage\\n \\nexplicit\\n \\nentity\\n \\nrelationships\\n \\nand\\n \\nschema\\n \\nmetadata.\\n \\n  The  results  from  both  sources  are  then  fused  to  ensure  factual  precision  from  structured  \\ndata\\n \\nand\\n \\ncontextual\\n \\nrichness\\n \\nfrom\\n \\nunstructured\\n \\ntext.\\n \\nThis\\n \\ncombined\\n \\napproach\\n \\nenhances\\n \\ncompleteness\\n \\nand\\n \\nreduces\\n \\nhallucinations\\n \\nin\\n \\ngenerated\\n \\nresponses.\\n \\n Q57.  Discuss  the  strategies  to  scale  embeddings  in  RAG  retrieval.  To  scale  embeddings  in  RAG  retrieval,  strategies  like  Matryoshka  Representation  \\nLearning\\n \\n(MRL)\\n \\nand\\n \\nquantization\\n \\nare\\n \\nhighly\\n \\neffective.\\n  MRL  enables  flexible  embeddings  by  training  a  single  model  to  produce  nested  \\nrepresentations\\n \\nof\\n \\nvarying\\n \\nsizes,\\n \\nallowing\\n \\ntruncation\\n \\nto\\n \\nsmaller\\n \\ndimensions\\n \\n(e.g.,\\n \\n64\\n \\nor\\n \\n128)\\n \\nwith\\n \\nminimal\\n \\nperformance\\n \\nloss,\\n \\nachieving\\n \\nup\\n \\nto\\n \\n14x\\n \\nsize\\n \\nreduction\\n \\nand\\n \\nsignificant\\n \\nretrieval\\n \\nspeed-ups.\\n \\n  Quantization  reduces  memory  usage  by  compressing  embeddings  into  lower-bit  formats  \\nlike\\n \\nfloat8\\n \\nor\\n \\nint8.\\n \\nCombining\\n \\nMRL\\n \\nwith\\n \\nquantization\\n \\ncan\\n \\nyield\\n \\nup\\n \\nto\\n \\n8x\\n \\ncompression,\\n \\noptimizing\\n \\nstorage\\n \\nand\\n \\nretrieval\\n \\nefficiency\\n \\nwhile\\n \\nmaintaining\\n \\nhigh\\n \\naccuracy\\n \\nfor\\n \\nlarge-scale\\n \\nRAG\\n \\nsystems.\\n 21                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 22, 'page_label': '23', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                             Q58.  What  advantages  does  quantization  oÔ¨Äer  over  dimensionality  \\nreduction\\n \\nfor\\n \\nscaling\\n \\nembeddings?  Quantization  offers  several  advantages  over  dimensionality  reduction  for  scaling  \\nembeddings\\n \\nin\\n \\nRAG\\n \\nretrieval.\\n \\nIt\\n \\ncompresses\\n \\nembeddings\\n \\nby\\n \\nreducing\\n \\nthe\\n \\nprecision\\n \\nof\\n \\nnumerical\\n \\nvalues\\n \\n(e.g.,\\n \\nfrom\\n \\nfloat32\\n \\nto\\n \\nint8\\n \\nor\\n \\nfloat8),\\n \\nachieving\\n \\nup\\n \\nto\\n \\n4x\\n \\nstorage\\n \\nreduction\\n \\nwith\\n \\nminimal\\n \\nperformance\\n \\nloss.\\n \\n  Unlike  dimensionality  reduction,  which  may  discard  important  features  and  degrade  \\naccuracy,\\n \\nquantization\\n \\npreserves\\n \\nthe\\n \\nfull\\n \\ndimensionality\\n \\nof\\n \\nembeddings,\\n \\nmaintaining\\n \\nricher\\n \\nsemantic\\n \\ninformation.\\n \\nAdditionally,\\n \\nquantization\\n \\naccelerates\\n \\ncomputation\\n \\non\\n \\nhardware\\n \\noptimized\\n \\nfor\\n \\nlower-precision\\n \\nformats,\\n \\nimproving\\n \\nretrieval\\n \\nspeed.\\n \\n  This  makes  it  particularly  effective  for  large-scale  RAG  systems  where  storage  and  \\nlatency\\n \\nare\\n \\ncritical,\\n \\nwhile\\n \\ndimensionality\\n \\nreduction\\n \\nrisks\\n \\ncompromising\\n \\nretrieval\\n \\nquality.\\n  Q59.  Explain  the  pros  and  cons  of  quantized  embeddings  in  RAG  retrieval.  Quantized  embeddings  in  RAG  systems  offer  significant  benefits  such  as  drastically  \\nreduced\\n \\nmemory\\n \\nrequirements\\n \\nand\\n \\nmuch\\n \\nfaster\\n \\nretrieval\\n \\nspeeds.\\n \\nThis\\n \\nmakes\\n \\nRAG\\n \\nretrieval\\n \\nmore\\n \\nefficient\\n \\nand\\n \\nscalable\\n \\nwhen\\n \\ndealing\\n \\nwith\\n \\nlarge\\n \\nknowledge\\n \\nbases.\\n \\n  However,  the  trade-off  is  a  slight  drop  in  retrieval  accuracy  or  relevance.  Additionally,  \\nquantization\\n \\neffectiveness\\n \\ncan\\n \\nvary\\n \\ndepending\\n \\non\\n \\nthe\\n \\nembedding\\n \\nmodel.\\n \\n  Overall,  quantized  embeddings  enable  cost-effective,  high-speed  retrieval  but  require  \\nmanaging\\n \\na\\n \\ncontrolled\\n \\ntrade-off\\n \\nbetween\\n \\nresource\\n \\nsavings\\n \\nand\\n \\naccuracy.\\n  Q60.  Compare  scalar  and  binary  quantization  for  embeddings  in  RAG  \\nretrieval.  Scalar  quantization  in  RAG  retrieval  compresses  embeddings  by  reducing  the  bit  \\nprecision\\n \\n(commonly\\n \\nto\\n \\nint8),\\n \\noffering\\n \\na\\n \\nmoderate\\n \\n4x\\n \\nreduction\\n \\nin\\n \\nmemory\\n \\nusage\\n \\nwhile\\n \\nmaintaining\\n \\na\\n \\ngood\\n \\nbalance\\n \\nbetween\\n \\nretrieval\\n \\naccuracy\\n \\nand\\n \\nspeed.\\n \\n  Binary  quantization,  on  the  other  hand,  converts  embeddings  to  1-bit  vectors,  achieving  \\nup\\n \\nto\\n \\n32x\\n \\ncompression\\n \\nand\\n \\nsignificantly\\n \\nfaster\\n \\nretrieval\\n \\nbut\\n \\nat\\n \\nthe\\n \\ncost\\n \\nof\\n \\ngreater\\n \\naccuracy\\n \\nloss.\\n  \\n 22                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 23, 'page_label': '24', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            Overall,  scalar  quantization  suits  use  cases  prioritizing  accuracy  with  some  compression,  \\nwhile\\n \\nbinary\\n \\nquantization\\n \\nexcels\\n \\nin\\n \\nlarge-scale,\\n \\nspeed-critical\\n \\nscenarios\\n \\nwhere\\n \\nmaximal\\n \\nmemory\\n \\nefficiency\\n \\noutweighs\\n \\nsome\\n \\nloss\\n \\nof\\n \\nprecision.\\n                                   \\n   \\n                     üöÄ\\n \\nAIxFunda  Newsletter  (free) Join  üöÄ  AIxFunda  free  newsletter  to  get  the  latest  updates  and  interesting  \\ntutorials\\n \\nrelated\\n \\nto\\n \\nGenerative\\n \\nAI,\\n \\nLLMs,\\n \\nAgents\\n \\nand\\n \\nRAG.\\n  ‚ú®  Weekly  GenAI  updates.  üìÑ  Weekly  LLM,  Agents  and  RAG  paper  updates.  üìù  1  fresh  blog  post  on  an  interesting  topic  every  week.                         \\n23                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 24, 'page_label': '25', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            \\nQ61.  How  does  re-ranking  diÔ¨Äer  from  the  initial  retrieval  process  in  RAG?  The  initial  retrieval  process  typically  uses  a  bi-encoder  that  encodes  queries  and  \\ndocuments\\n \\nindependently\\n \\nand\\n \\nthen\\n \\nfetches\\n \\na\\n \\nbroad\\n \\nset\\n \\nof\\n \\ncandidates\\n \\nquickly.\\n \\n  The  re-ranking  process  reorders  the  retrieval  results  by  taking  the  query  and  each  \\nretrieved\\n \\ndocument\\n \\nchunk\\n \\nas\\n \\na\\n \\nsingle\\n \\ncombined\\n \\ninput,\\n \\nscoring\\n \\ntheir\\n \\nrelevance\\n \\nthrough\\n \\ndeep\\n \\ninteraction.\\n \\nThis\\n \\nimproves\\n \\nthe\\n \\nfinal\\n \\nranking\\n \\nquality\\n \\nat\\n \\nthe\\n \\ncost\\n \\nof\\n \\nhigher\\n \\ncomputational\\n \\noverhead\\n  This  two-stage  approach  balances  efficiency  and  accuracy  by  separating  fast,  broad  \\nretrieval\\n \\nfrom\\n \\nslower,\\n \\nmore\\n \\nexact\\n \\nreranking.\\n  Q62.  Explain  the  pros  and  cons  of  using  re-rankers  in  RAG.  Re-rankers  reorder  search  results  by  taking  the  query  and  each  retrieved  document  \\nchunk\\n \\nas\\n \\na\\n \\nsingle\\n \\ncombined\\n \\ninput,\\n \\nscoring\\n \\ntheir\\n \\nrelevance\\n \\nthrough\\n \\ndeep\\n \\ninteraction\\n \\nwithin\\n \\none\\n \\nmodel\\n \\npass.\\n \\nThis\\n \\nhelps\\n \\nto\\n \\nprioritize\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\ninformation\\n \\nin\\n \\nthe\\n \\nlimited\\n \\ncontext\\n \\nwindows\\n \\nin\\n \\nLLMs.\\n  However,  re-rankers  introduce  increased  latency  and  higher  computational  costs  since  \\nthey\\n \\nperform\\n \\ndeep,\\n \\nquery-chunk\\n \\ninteraction\\n \\nat\\n \\nquery\\n \\ntime,\\n \\nmaking\\n \\nthem\\n \\nless\\n \\nsuitable\\n \\nfor\\n \\nreal-time\\n \\nor\\n \\nhigh-traffic\\n \\napplications.\\n  The  trade-off  between  enhanced  precision  and  increased  costs  makes  re-rankers  ideal  \\nfor\\n \\nspecialized\\n \\nuse\\n \\ncases\\n \\nbut\\n \\nless\\n \\nsuitable\\n \\nfor\\n \\napplications\\n \\nprioritizing\\n \\nspeed\\n \\nand\\n \\ncost-efficiency.\\n  Q63.  What  are  the  diÔ¨Äerent  types  of  re-ranker  models  that  can  be  used  in  \\nRAG?  The  different  types  of  re-ranker  models  used  in  Retrieval-Augmented  Generation  (RAG)  \\nare\\n  Cross-Encoder  Rerankers:  These  models  jointly  encode  the  query  and  document  chunk  \\npair\\n \\nto\\n \\nproduce\\n \\na\\n \\nhighly\\n \\naccurate\\n \\nrelevance\\n \\nscore,\\n \\noffering\\n \\nnuanced\\n \\nunderstanding\\n \\nof\\n \\nrelationships\\n \\nbut\\n \\nwith\\n \\nmedium\\n \\ncomputational\\n \\ncost.\\n  \\n24                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 25, 'page_label': '26', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Multi-Vector  or  Late  Interaction  Models:  Such  as  ColBERT,  they  encode  queries  and  \\ndocument\\n \\nchunks\\n \\nseparately\\n \\nbut\\n \\nperform\\n \\nfine-grained\\n \\ninteraction\\n \\nlater,\\n \\nbalancing\\n \\nefficiency\\n \\nand\\n \\nperformance\\n \\nwith\\n \\nlower\\n \\ncost.\\n  Large  Language  Model  (LLM)  Rerankers:  Utilize  powerful  LLMs  to  reason  about  \\nquery-document\\n \\nchunk\\n \\nrelevance,\\n \\nachieving\\n \\ngreat\\n \\naccuracy\\n \\nbut\\n \\nincurring\\n \\nhigh\\n \\ncomputational\\n \\noverhead.\\n  These  models  vary  in  their  performance  and  computational  cost,  and  choice  depends  on  \\nthe\\n \\napplication's\\n \\naccuracy\\n \\nand\\n \\nlatency\\n \\nrequirements.\\n \\n Q64.  Compare  general  re-rankers  and  instruction-following  re-rankers  in  \\nRAG.   General  re-rankers  in  RAG  systems  primarily  focus  on  re-ranking  retrieved  document  \\nchunks\\n \\njust\\n \\nbased\\n \\non\\n \\ntheir\\n \\nsemantic\\n \\nrelevance\\n \\nto\\n \\nthe\\n \\nuser\\n \\nquery.\\n \\n  In  contrast,  instruction-following  re-rankers  go  a  step  further  by  dynamically  adjusting  \\nrankings\\n \\nbased\\n \\non\\n \\nadditional\\n \\nuser-provided\\n \\ninstructions\\n \\nsuch\\n \\nas\\n \\ndocument\\n \\nrecency,\\n \\nsource\\n \\nreliability,\\n \\nor\\n \\nmetadata\\n \\ncriteria.\\n \\n  Q65.  Why  is  the  cross-encoder  typically  used  as  the  re-ranker  rather  than  \\nthe\\n \\nbi-encoder? \\n The  cross-encoder  is  typically  used  as  the  re-ranker  rather  than  the  bi-encoder  because  \\nit\\n \\nprocesses\\n \\nthe\\n \\nquery\\n \\nand\\n \\ncandidate\\n \\ndocument\\n \\nchunks\\n \\ntogether,\\n \\nallowing\\n \\nit\\n \\nto\\n \\ncapture\\n \\nintricate\\n \\ncontextual\\n \\ninteractions\\n \\nand\\n \\nprovide\\n \\nmore\\n \\naccurate\\n \\nrelevance\\n \\nscores.\\n \\n  While  bi-encoders  encode  queries  and  document  chunks  separately,  enabling  fast  and  \\nscalable\\n \\nretrieval\\n \\nof\\n \\nbroad\\n \\ncandidate\\n \\nsets,\\n \\nthey\\n \\nmiss\\n \\ndetailed\\n \\nrelationships\\n \\nbetween\\n \\nquery-document\\n \\nchunk\\n \\npairs.\\n \\n  Cross-encoders,  though  slower  and  more  resource-intensive,  excel  in  precision,  making  \\nthem\\n \\nwell-suited\\n \\nfor\\n \\nre-ranking\\n \\na\\n \\nsmall\\n \\nset\\n \\nof\\n \\ntop\\n \\ncandidates\\n \\nidentified\\n \\nby\\n \\nthe\\n \\nbi-encoder.\\n \\nThis\\n \\ncombined\\n \\napproach\\n \\nbalances\\n \\nscalability\\n \\nwith\\n \\naccuracy,\\n \\nleveraging\\n \\nbi-encoders\\n \\nfor\\n \\nefficient\\n \\ncandidate\\n \\nretrieval\\n \\nand\\n \\ncross-encoders\\n \\nfor\\n \\nrefined\\n \\nfinal\\n \\nranking.\\n   \\n25                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 26, 'page_label': '27', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q66.   A  RAG  system  retrieves  20  candidate  document  chunks  but  can  only  \\nÔ¨Åt\\n \\n5\\n \\nin\\n \\nthe\\n \\nLLM\\'s\\n \\ncontext\\n \\nwindow.\\n \\nWithout\\n \\nre-ranking,\\n \\nhow\\n \\nmight\\n \\nthis\\n \\nlimitation\\n \\naÔ¨Äect\\n \\nresponse\\n \\nquality,\\n \\nand\\n \\nwhat\\n \\nspeciÔ¨Åc\\n \\nproblems\\n \\nwould\\n \\na\\n \\nre-ranker\\n \\nsolve?  When  a  RAG  system  retrieves  20  candidate  document  chunks  but  can  only  fit  5  in  the  \\nLLM\\'s\\n \\ncontext\\n \\nwindow,\\n \\nthe\\n \\nlimitation\\n \\ncan\\n \\ncause\\n \\nthe\\n \\nmodel\\n \\nto\\n \\nmiss\\n \\ncritical\\n \\ninformation\\n \\nfrom\\n \\nthe\\n \\ndiscarded\\n \\ndocument\\n \\nchunks.\\n \\nWithout\\n \\nre-ranking,\\n \\nthe\\n \\ntop\\n \\n5\\n \\ndocument\\n \\nchunks\\n \\nmay\\n \\nnot\\n \\nbe\\n \\nthe\\n \\nmost\\n \\nrelevant,\\n \\nleading\\n \\nto\\n \\nincomplete\\n \\nor\\n \\nless\\n \\naccurate\\n \\nanswers.\\n \\n  A  re-ranker  solves  this  by  analyzing  and  scoring  all  retrieved  document  chunks  based  on  \\nrelevance\\n \\nand\\n \\ncontextual\\n \\nalignment\\n \\nwith\\n \\nthe\\n \\nquery,\\n \\nensuring\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\nchunks\\n \\nare\\n \\nincluded\\n \\nin\\n \\nthe\\n \\nlimited\\n \\nwindow.\\n \\n  This  filtering  reduces  retrieval  noise,  enhances  coherence,  and  maximizes  the  usefulness  \\nof\\n \\nthe\\n \\ninput\\n \\nfor\\n \\nthe\\n \\ngenerative\\n \\nmodel,\\n \\nthereby\\n \\nimproving\\n \\nthe\\n \\noverall\\n \\nquality\\n \\nof\\n \\nthe\\n \\nresponse.\\n   Q67.  Describe  a  scenario  where  a  BM25  retrieval  might  return  relevant  \\nchunks\\n \\nbut\\n \\nin\\n \\npoor\\n \\nranking\\n \\norder.\\n \\nHow\\n \\nwould\\n \\na\\n \\nneural\\n \\nre-ranker\\n \\nspeciÔ¨Åcally\\n \\naddress\\n \\nthis\\n \\nlimitation?\\n \\n A  typical  scenario  where  BM25  retrieval  yields  relevant  document  chunks  but  in  poor  \\nranking\\n \\norder\\n \\narises\\n \\nwhen\\n \\nthe\\n \\nquery\\n \\nuses\\n \\nsynonyms\\n \\nor\\n \\nphrases\\n \\nthat\\n \\nvary\\n \\nfrom\\n \\nthose\\n \\nin\\n \\nthe\\n \\ndocuments.\\n \\nThis\\n \\nis\\n \\nbecause\\n \\nBM25‚Äôs\\n \\nexact\\n \\nkeyword\\n \\nmatching\\n \\nmay\\n \\nsurface\\n \\nall\\n \\nrelevant\\n \\nitems,\\n \\nbut\\n \\nfail\\n \\nto\\n \\nprioritize\\n \\nthose\\n \\nmost\\n \\ncontextually\\n \\naligned\\n \\ndue\\n \\nto\\n \\nits\\n \\nlack\\n \\nof\\n \\nsemantic\\n \\nunderstanding.\\n \\n  For  instance,  searching  for  \"car  maintenance\"  might  retrieve  document  chunks  about  \\n\"vehicle\\n \\nupkeep\"\\n \\nand\\n \\n\"automobile\\n \\nservicing,\"\\n \\nbut\\n \\nBM25\\n \\nmay\\n \\nrank\\n \\nless\\n \\nrelevant\\n \\ndocument\\n \\nchunks\\n \\nhigher\\n \\nif\\n \\nthey\\n \\nhave\\n \\nkeyword\\n \\noverlaps\\n \\nrather\\n \\nthan\\n \\nsemantic\\n \\ncloseness.\\n \\nNeural\\n \\nre-rankers\\n \\nexplicitly\\n \\naddress\\n \\nthis\\n \\nby\\n \\nleveraging\\n \\ndeep\\n \\ncontextual\\n \\nand\\n \\nsemantic\\n \\nsignals,\\n \\nreordering\\n \\nthe\\n \\ncandidate\\n \\nset\\n \\nto\\n \\nprioritize\\n \\ndocument\\n \\nchunks\\n \\nthat\\n \\nbest\\n \\nmatch\\n \\nthe\\n \\nquery‚Äôs\\n \\nintent\\n \\nand\\n \\nmeaning.\\n \\n \\n26                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q68.  If  your  RAG  system  serves  both  simple  factual  queries  and  complex  \\nanalytical\\n \\nquestions,\\n \\nhow\\n \\nwould\\n \\nyou\\n \\ndecide\\n \\nwhen\\n \\nto\\n \\nbypass\\n \\nthe\\n \\nre-ranker\\n \\nfor\\n \\neÔ¨Éciency\\n \\nwhile\\n \\nmaintaining\\n \\nquality?  To  decide  when  to  bypass  the  re-ranker  in  a  RAG  system,  queries  should  be  classified  \\nbased\\n \\non\\n \\ncomplexity.\\n \\nSimple\\n \\nfactual\\n \\nqueries\\n \\nlike\\n \\n\"What\\n \\nis\\n \\nthe\\n \\ncapital\\n \\nof\\n \\nFrance?\"\\n  \\nrequire\\n \\nstraightforward\\n \\nand\\n \\nwell-known\\n \\nanswers.\\n \\nRe-ranker\\n \\ncan\\n \\nbe\\n \\nskipped\\n \\nfor\\n \\nsimple\\n \\nfactual\\n \\nqueries,\\n \\nas\\n \\nthe\\n \\ninitial\\n \\nretrieval\\n \\nis\\n \\nlikely\\n \\nto\\n \\nyield\\n \\nhighly\\n \\nrelevant\\n \\nresults.\\n \\n  For  complex  analytical  questions,  such  as  those  requiring  synthesis  or  reasoning  across  \\nmultiple\\n \\nchunks,\\n \\nthe\\n \\nre-ranker\\n \\nshould\\n \\nbe\\n \\nused\\n \\nto\\n \\nensure\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\nchunks\\n \\nare\\n \\nprioritized.\\n  Q69.  Describe  the  vector  pre-computation  and  storage  strategy  in  a  \\nbi-encoder\\n \\n+\\n \\ncross-encoder\\n \\npipeline.\\n \\nWhy\\n \\ncan\\'t\\n \\ncross-encoders\\n \\npre-compute\\n \\ntext\\n \\nrepresentations\\n \\nlike\\n \\nbi-encoders\\n \\ncan?  The  RAG  pipeline  leverages  bi-encoders  for  fast  retrieval  and  cross-encoders  for  the  \\nprecise\\n \\nreranking\\n \\nof\\n \\ntop\\n \\ncandidates.\\n  Bi-encoders  pre-compute  chunk  representations  by  encoding  them  into  fixed-size  dense  \\nvectors\\n \\noffline\\n \\nand\\n \\nthen\\n \\nstoring\\n \\nthem\\n \\nin\\n \\na\\n \\nvector\\n \\ndatabase\\n \\nfor\\n \\nefficient\\n \\nretrieval.\\n \\n  Cross-encoders,  however,  cannot  pre-compute  chunk  representations  because  they  \\njointly\\n \\nencode\\n \\nquery-chunk\\n \\npairs,\\n \\ncapturing\\n \\nintricate\\n \\ninteractions\\n \\nthrough\\n \\nattention\\n \\nmechanisms,\\n \\nrequiring\\n \\nboth\\n \\ninputs\\n \\nat\\n \\ninference\\n \\ntime\\n \\nto\\n \\nproduce\\n \\na\\n \\nrelevance\\n \\nscore.\\n \\n   Q70.  Compare  the  noise  reduction  capabilities  of  re-rankers  versus  simply  \\nincreasing\\n \\nthe\\n \\nsimilarity\\n \\nthreshold\\n \\nin\\n \\ninitial\\n \\nretrieval.\\n \\nWhen\\n \\nwould\\n \\neach\\n \\napproach\\n \\nbe\\n \\nmore\\n \\nappropriate?  Increasing  the  similarity  threshold  in  initial  retrieval  reduces  noise  by  filtering  out  less  \\nsimilar\\n \\nchunks\\n \\nbut\\n \\nrisks\\n \\nmissing\\n \\nrelevant\\n \\nones\\n \\ndue\\n \\nto\\n \\nembedding\\n \\nlimitations.\\n  \\nRe-rankers\\n \\nreduce\\n \\nnoise\\n \\nby\\n \\nprioritizing\\n \\nrelevant\\n \\nchunks\\n \\nby\\n \\ndeeply\\n \\nunderstanding\\n \\nquery-chunk\\n \\nrelevance.\\n  The  choice  depends  on  the  trade-off  between  computational  cost  and  precision  \\nrequirements.\\n \\nRe-rankers\\n \\nare\\n \\npreferred\\n \\nfor\\n \\nhigh-stakes\\n \\napplications\\n \\nlike\\n \\nlegal\\n \\nor\\n \\nmedical\\n \\n27                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 28, 'page_label': '29', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           searches.   Increasing  the  similarity  threshold  is  simpler  and  faster,  suitable  for  \\nresource-constrained\\n \\nenvironments.\\n \\n Q71.  What  challenges  do  re-rankers  face  regarding  computational  \\noverhead\\n \\nand\\n \\nlatency?\\n \\n \\n Re-rankers  in  RAG  systems  face  significant  challenges  related  to  increased  \\ncomputational\\n \\noverhead\\n \\nand\\n \\nlatency,\\n \\nas\\n \\neach\\n \\nquery-chunk\\n \\npair\\n \\nmust\\n \\nbe\\n \\nprocessed.\\n  \\nThis\\n \\nlatency\\n \\nincrease\\n \\ncan\\n \\nhinder\\n \\nhigh-throughput\\n \\nenvironments,\\n \\nmaking\\n \\nre-rankers\\n \\ncomputationally\\n \\nexpensive\\n \\ncompared\\n \\nto\\n \\ninitial\\n \\nvector\\n \\nsearches\\n \\nand\\n \\nlimiting\\n \\nscalability.\\n   Q72.  In  real-time  applications  with  strict  latency  requirements,  describe  \\ntwo\\n \\nspeciÔ¨Åc\\n \\noptimization\\n \\nstrategies\\n \\nyou\\n \\ncould\\n \\nimplement\\n \\nto\\n \\nreduce\\n \\nre-ranking\\n \\noverhead\\n \\nwhile\\n \\npreserving\\n \\nmost\\n \\nof\\n \\nthe\\n \\nquality\\n \\ngains.  Two  effective  strategies  to  reduce  re-ranking  overhead  while  preserving  quality  gains  in  \\nreal-time\\n \\nRAG\\n \\napplications\\n \\nare\\n \\n  1)  Query  classifier:  Deploy  a  query  classifier  to  identify  complex  or  analytical  queries,  \\ninvoking\\n \\nthe\\n \\nre-ranker\\n \\nonly\\n \\nfor\\n \\nthese\\n \\nwhile\\n \\nbypassing\\n \\nit\\n \\nfor\\n \\nsimple\\n \\nfactual\\n \\nqueries.\\n \\n  2)  Model  distillation:  Train  a  smaller,  faster  re-ranking  model  to  mimic  the  behavior  of  a  \\nlarger,\\n \\nmore\\n \\naccurate\\n \\nmodel,\\n \\nenabling\\n \\nquicker\\n \\ninference\\n \\nwith\\n \\nminimal\\n \\nquality\\n \\nloss.\\n \\n  These  approaches  balance  latency  and  quality  by  minimizing  computational  load  \\nwithout\\n \\nsignificantly\\n \\ncompromising\\n \\nthe\\n \\nrelevance\\n \\nof\\n \\nretrieved\\n \\nresults.\\n   Q73.  How  would  you  evaluate  the  eÔ¨Äectiveness  of  a  reranker  in  a  RAG  \\nsystem?\\n \\nWhich\\n \\nmetrics\\n \\n(e.g.,\\n \\nMRR,\\n \\nMAP,\\n \\nNDCG)\\n \\nwould\\n \\nyou\\n \\nprioritize\\n \\nand\\n \\nwhy?  The  effectiveness  of  a  re-ranker  in  a  RAG  system  is  best  evaluated  using  ranking  metrics  \\nthat\\n \\ncapture\\n \\nhow\\n \\nwell\\n \\nit\\n \\nprioritizes\\n \\nrelevant\\n \\nchunks.\\n \\nMean\\n \\nReciprocal\\n \\nRank\\n \\n(MRR)\\n \\nis\\n \\nkey\\n \\nwhen\\n \\nthe\\n \\nfocus\\n \\nis\\n \\non\\n \\nhow\\n \\nquickly\\n \\nthe\\n \\nfirst\\n \\nrelevant\\n \\nchunk\\n \\nappears,\\n \\nideal\\n \\nfor\\n \\nquestion-answering\\n \\nscenarios.\\n \\n  \\n28                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 29, 'page_label': '30', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Mean  Average  Precision  (MAP)  is  useful  when  multiple  relevant  chunks  matter,  \\nmeasuring\\n \\nboth\\n \\nprecision\\n \\nand\\n \\nranking\\n \\nquality\\n \\nacross\\n \\nresults.\\n \\nNormalized\\n \\nDiscounted\\n \\nCumulative\\n \\nGain\\n \\n(NDCG)\\n \\nexcels\\n \\nwhen\\n \\nrelevance\\n \\nis\\n \\ngraded\\n \\nrather\\n \\nthan\\n \\nbinary,\\n \\nrewarding\\n \\nthe\\n \\ncorrect\\n \\norder\\n \\nof\\n \\nhighly\\n \\nrelevant\\n \\nchunks.\\n \\n                                      \\nLLM  Survey  Papers  Collection \\n This  repo  is  highly  useful  to  stay  updated  with  LLM  research.                \\n29                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 30, 'page_label': '31', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q74.  Explain  the  diÔ¨Äerence  between  Precision@k  and  Recall@k  in  the  \\ncontext\\n \\nof\\n \\nRAG.\\n \\nWhen\\n \\nmight\\n \\nyou\\n \\nprefer\\n \\none\\n \\nover\\n \\nthe\\n \\nother? \\n Precision@k  focuses  on  accuracy  of  the  retrieval  by  measuring  the  proportion  of  the  \\ntop-k\\n \\nretrieved\\n \\nchunks\\n \\nthat\\n \\nare\\n \\nrelevant\\n \\nto\\n \\nthe\\n \\nquery.\\n  \\nRecall@k\\n \\nfocuses\\n \\non\\n \\ncompleteness\\n \\nof\\n \\nthe\\n \\nretrieval\\n \\nby\\n \\nmeasuring\\n \\nthe\\n \\nproportion\\n \\nof\\n \\nall\\n \\nrelevant\\n \\nchunks\\n \\nthat\\n \\nare\\n \\nretrieved\\n \\nwithin\\n \\nthe\\n \\ntop-k\\n \\nresults.\\n  You  might  choose  Precision@k  when  you  want  to  ensure  high-quality,  relevant  chunks  \\nto\\n \\nreduce\\n \\nnoise.\\n \\nOn\\n \\nthe\\n \\nother\\n \\nhand,\\n \\nyou\\n \\nmight\\n \\nchoose\\n \\nRecall@k\\n \\nwhen\\n \\nit\\n \\nis\\n \\ncrucial\\n \\nto\\n \\ncapture\\n \\nas\\n \\nmany\\n \\nrelevant\\n \\nchunks\\n \\nas\\n \\npossible.\\n  Q75.  Why  is  MRR  unsuitable  when  there  are  multiple  relevant  chunks  per  \\nquery,\\n \\nand\\n \\nhow\\n \\ndoes\\n \\nMAP\\n \\naddress\\n \\nthis\\n \\nlimitation? \\n MRR  (Mean  Reciprocal  Rank)  considers  the  rank  of  the  first  relevant  chunk  and  \\ndisregards\\n \\nthe\\n \\nranks\\n \\nand\\n \\npresence\\n \\nof\\n \\nother\\n \\nrelevant\\n \\nchunks.\\n \\nThis\\n \\nlimitation\\n \\nmakes\\n \\nMRR\\n \\nmore\\n \\nappropriate\\n \\nfor\\n \\nscenarios\\n \\nwhere\\n \\na\\n \\nsingle\\n \\nchunk\\n \\nsufficiently\\n \\nanswers\\n \\nthe\\n \\nquery.\\n \\n  In  contrast,  MAP  (Mean  Average  Precision)  addresses  this  by  averaging  the  precision  \\nacross\\n \\nall\\n \\nrelevant\\n \\nranks,\\n \\naccounting\\n \\nfor\\n \\nthe\\n \\npresence\\n \\nand\\n \\norder\\n \\nof\\n \\nall\\n \\nrelevant\\n \\nchunks.\\n \\nHence,\\n \\nMAP\\n \\nis\\n \\npreferred\\n \\nover\\n \\nMRR\\n \\nfor\\n \\ncases\\n \\nwhere\\n \\nmultiple\\n \\nrelevant\\n \\nchunks\\n \\ncontribute\\n \\nto\\n \\nanswering\\n \\na\\n \\nquery\\n \\ncomprehensively.\\n  Q76.  Given  a  retrieval  result,  show  how  to  manually  calculate  the  MAP@5  \\n(Mean\\n \\nAverage\\n \\nPrecision\\n \\nat\\n \\n5).\\n \\nWhat\\n \\ndoes\\n \\nMAP\\n \\nreveal\\n \\nabout\\n \\nthe\\n \\nretrieval\\n \\nsystem\\n \\nthat\\n \\nraw\\n \\nPrecision\\n \\ndoes\\n \\nnot?  To  manually  calculate  MAP@5,  list  the  top  5  retrieved  items  for  each  query  and  note  the  \\npositions\\n \\nwhere\\n \\nrelevant\\n \\nitems\\n \\nappear;\\n \\nthen,\\n \\ncompute\\n \\nprecision\\n \\nat\\n \\neach\\n \\nrelevant\\n \\nposition\\n \\n(e.g.,\\n \\nif\\n \\nthe\\n \\nfirst\\n \\nrelevant\\n \\nitem\\n \\nappears\\n \\nat\\n \\nrank\\n \\n2,\\n \\nprecision\\n \\n=\\n \\n1/2)\\n \\nand\\n \\naverage\\n \\nthese\\n \\nvalues\\n \\nto\\n \\nget\\n \\nthe\\n \\nAverage\\n \\nPrecision\\n \\n(AP)\\n \\nfor\\n \\nthat\\n \\nquery.\\n \\nRepeat\\n \\nthis\\n \\nfor\\n \\nall\\n \\nqueries\\n \\nand\\n \\ntake\\n \\nthe\\n \\nmean\\n \\nof\\n \\ntheir\\n \\nAPs\\n \\nfor\\n \\nMAP@5.\\n  MAP@5  reveals  a  retrieval  system's  ability  to  rank  relevant  items  higher.  In  contrast,  \\nraw\\n \\nPrecision\\n \\nonly\\n \\nmeasures\\n \\nthe\\n \\nproportion\\n \\nof\\n \\nrelevant\\n \\nitems\\n \\nretrieved,\\n \\nignoring\\n \\ntheir\\n \\norder.\\n \\nThis\\n \\nmakes\\n \\nMAP@5\\n \\na\\n \\nbetter\\n \\nindicator\\n \\nof\\n \\nhow\\n \\nwell\\n \\nthe\\n \\nsystem\\n \\nprioritizes\\n \\nrelevance\\n \\nat\\n \\nthe\\n \\ntop\\n \\nof\\n \\nthe\\n \\nresult\\n \\nlist.\\n  \\n30                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 31, 'page_label': '32', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            \\nQ77.  If  all  the  relevant  chunks  are  at  the  very  bottom,  how  would  \\nthis\\n \\naÔ¨Äect\\n \\nMRR,\\n \\nMAP,\\n \\nand\\n \\nNDCG\\n \\nmetrics?\\n \\nExplain\\n \\neach.  If  all  relevant  chunks  are  at  the  bottom  of  a  ranked  list  for  a  search  query,  MRR  (Mean  \\nReciprocal\\n \\nRank)\\n \\nwould\\n \\nbe\\n \\nlow,\\n \\nas\\n \\nit\\n \\nmeasures\\n \\nthe\\n \\nreciprocal\\n \\nof\\n \\nthe\\n \\nrank\\n \\nof\\n \\nthe\\n \\nfirst\\n \\nrelevant\\n \\nchunk.\\n \\nMAP\\n \\n(Mean\\n \\nAverage\\n \\nPrecision)\\n \\nwould\\n \\nalso\\n \\nbe\\n \\nlow,\\n \\nas\\n \\nit\\n \\naverages\\n \\nprecision\\n \\nacross\\n \\nall\\n \\nrelevant\\n \\nchunks,\\n \\npenalizing\\n \\nlate\\n \\nappearances\\n \\nheavily\\n \\ndue\\n \\nto\\n \\nincreasing\\n \\ndenominators\\n \\nin\\n \\nprecision\\n \\ncalculations.\\n \\n  NDCG  (Normalized  Discounted  Cumulative  Gain)  would  similarly  be  low,  as  it  discounts  \\nthe\\n \\nrelevance\\n \\nscores\\n \\nof\\n \\nchunks\\n \\nappearing\\n \\nlater\\n \\nin\\n \\nthe\\n \\nranking,\\n \\nreducing\\n \\nthe\\n \\ncumulative\\n \\ngain.\\n \\n  \\nQ78.  Suppose  your  RAG  retriever  gets  perfect  Recall@10  but  low  \\nPrecision@10.\\n \\nWhat\\n \\nproblems\\n \\ncould\\n \\nthis\\n \\ncause\\n \\nfor\\n \\nthe\\n \\ndownstream\\n \\ngenerator?  Perfect  Recall@10  means  all  relevant  chunks  are  retrieved  within  the  top  10  results.  Low  \\nPrecision@10\\n \\nindicates\\n \\nmany\\n \\nof\\n \\nthose\\n \\nretrieved\\n \\nchunks\\n \\nare\\n \\nirrelevant.\\n \\nIf\\n \\na\\n \\nRAG\\n \\nretriever\\n \\nachieves\\n \\nperfect\\n \\nRecall@10\\n \\nbut\\n \\nlow\\n \\nPrecision@10,\\n \\nthe\\n \\ndownstream\\n \\ngenerator\\n \\nreceives\\n \\nall\\n \\nthe\\n \\nrelevant\\n \\ninformation\\n \\nmixed\\n \\nwith\\n \\nmuch\\n \\nirrelevant\\n \\ncontent.\\n \\n  This  will  confuse  the  generator  model  and  increase  the  chance  of  generating  off-topic  or  \\ninaccurate\\n \\nresponses.\\n \\n  Q79.  Compare  and  contrast  ‚Äúorder-aware‚Äù  and  ‚Äúorder-unaware‚Äù  retrieval  \\nmetrics\\n \\nin\\n \\nRAG,\\n \\ngiving\\n \\nexamples\\n \\nfor\\n \\neach\\n \\nfrom\\n \\nthe\\n \\nset\\n \\n(Precision,\\n \\nRecall,\\n \\nMRR,\\n \\nMAP,\\n \\nNDCG).  Order-aware  retrieval  metrics  consider  the  ranking  of  retrieved  items,  emphasizing  the  \\nimportance\\n \\nof\\n \\nhigher-ranked\\n \\nrelevant\\n \\nresults.\\n \\nFor\\n \\nexample,\\n \\nMean\\n \\nReciprocal\\n \\nRank\\n \\n(MRR)\\n \\nand\\n \\nNormalized\\n \\nDiscounted\\n \\nCumulative\\n \\nGain\\n \\n(NDCG)\\n \\nare\\n \\norder-aware,\\n \\nas\\n \\nMRR\\n \\nevaluates\\n \\nthe\\n \\nrank\\n \\nof\\n \\nthe\\n \\nfirst\\n \\nrelevant\\n \\nitem\\n \\nand\\n \\nNDCG\\n \\naccounts\\n \\nfor\\n \\nrelevance\\n \\nscores\\n \\nand\\n \\nranking\\n \\npositions.\\n  Order-unaware  metrics  focus  solely  on  whether  relevant  items  are  retrieved  and  ignore  \\nthe\\n \\norder.\\n  \\nPrecision\\n \\nand\\n \\nRecall\\n \\nare\\n \\norder-unaware,\\n \\nmeasuring\\n \\nthe\\n \\nproportion\\n \\nof\\n \\n31                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 32, 'page_label': '33', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           relevant  items  retrieved  (Precision)  and  the  proportion  of  relevant  items  found  out  of  all  \\nrelevant\\n \\nitems\\n \\n(Recall),\\n \\nwithout\\n \\nconsidering\\n \\ntheir\\n \\norder.\\n \\n \\n Q80.  How  would  the  value  of  NDCG@k  change  if  all  relevant  chunks  are  \\nretrieved\\n \\nbut\\n \\nin\\n \\nthe\\n \\nreverse\\n \\norder\\n \\n(least\\n \\nto\\n \\nmost\\n \\nrelevant)? \\n NDCG@k  rewards  placing  highly  relevant  chunks  at  earlier  ranks  and  applies  a  \\nlogarithmic\\n \\ndiscount\\n \\nto\\n \\nrelevance\\n \\nscores\\n \\nat\\n \\nlower\\n \\npositions.\\n \\nSo\\n \\nreversing\\n \\nthe\\n \\norder\\n \\npushes\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\nchunks\\n \\nfurther\\n \\ndown\\n \\nthe\\n \\nlist‚Äîmaking\\n \\nthem\\n \\nless\\n \\nvaluable\\n \\nin\\n \\nthe\\n \\nNDCG\\n \\ncalculation.\\n \\n  While  all  relevant  items  are  present,  their  suboptimal  positions  reduce  the  overall  score  \\nsince\\n \\nNDCG\\n \\nis\\n \\nsensitive\\n \\nto\\n \\nboth\\n \\nthe\\n \\npresence\\n \\nand\\n \\norder\\n \\nof\\n \\nrelevant\\n \\nitems\\n \\nin\\n \\nthe\\n \\ntop\\n \\nk.\\n \\nThe\\n \\nvalue\\n \\nof\\n \\nNDCG@k\\n \\nwill\\n \\ndecrease\\n \\ncompared\\n \\nto\\n \\nthe\\n \\nideal\\n \\nranking,\\n \\nbut\\n \\nwill\\n \\nremain\\n \\nhigher\\n \\nthan\\n \\na\\n \\nranking\\n \\nwith\\n \\nirrelevant\\n \\nchunks\\n \\nat\\n \\nthe\\n \\ntop.\\n \\n  Q81.  What  is  the  signiÔ¨Åcance  of  Context  Precision@K  in  evaluating  a  RAG  \\nretriever,\\n \\nand\\n \\nhow\\n \\ndoes\\n \\nit\\n \\ndiÔ¨Äer\\n \\nfrom\\n \\nstandard\\n \\nPrecision@k\\n \\nin\\n \\ntraditional\\n \\ninformation\\n \\nretrieval?  The  standard  Precision@k  in  traditional  information  retrieval  just  measures  the  \\nproportion\\n \\nof\\n \\nrelevant\\n \\nitems\\n \\namong\\n \\nthe\\n \\ntop-k\\n \\nresults\\n \\nand\\n \\nignores\\n \\nthe\\n \\norder.\\n \\nUnlike\\n \\nstandard\\n \\nPrecision@k,\\n  \\nContext\\n \\nPrecision@K\\n \\nnot\\n \\nonly\\n \\nchecks\\n \\nwhether\\n \\nrelevant\\n \\nchunks\\n \\nare\\n \\nretrieved,\\n \\nbut\\n \\nalso\\n \\nwhether\\n \\nthey\\n \\nappear\\n \\nat\\n \\nhigher\\n \\nranks\\n \\nin\\n \\nthe\\n \\ncontext.\\n \\n  Context  Precision@K  ensures  useful  information  is  prioritized  in  the  retrieved  context  \\nwhich\\n \\ngreatly\\n \\nimpacts\\n \\nthe\\n \\nquality\\n \\nof\\n \\nthe\\n \\ngenerated\\n \\nanswer.\\n \\n \\n Q82.  Why  does  Context  Precision@K  use  a  weighted  sum  approach  with  \\nrelevance\\n \\nindicators,\\n \\nand\\n \\nhow\\n \\ndoes\\n \\nthis\\n \\nbetter\\n \\nreÔ¨Çect\\n \\nRAG\\n \\nretriever\\n \\nperformance? \\n Context  Precision  is  computed  as  the  weighted  sum  of  Precision@k,  normalized  by  the  \\nnumber\\n \\nof\\n \\nrelevant\\n \\nchunks.\\n  \\nHere\\n \\nthe\\n \\nweighted\\n \\nsum\\n \\naccounts\\n \\nfor\\n \\nboth\\n \\nthe\\n \\npresence\\n \\nand\\n \\nthe\\n \\nrank\\n \\nof\\n \\nrelevant\\n \\nchunks\\n \\nin\\n \\nthe\\n \\nretrieved\\n \\ncontext.\\n \\nBy\\n \\nmultiplying\\n \\nPrecision@k\\n \\nwith\\n \\nthe\\n \\nrelevance\\n \\nindicator\\n \\nat\\n \\neach\\n \\nposition,\\n \\nthe\\n \\nmetric\\n \\nrewards\\n \\ncases\\n \\nwhere\\n \\nrelevant\\n \\ninformation\\n \\nappears\\n \\nearlier,\\n \\nreflecting\\n \\nthe\\n \\nimportance\\n \\nof\\n \\nranking\\n \\nquality.\\n \\n  \\n32                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 33, 'page_label': '34', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           This  approach  better  evaluates  RAG  retrievers,  since  in  generative  settings,  not  just  \\nretrieving\\n \\nrelevant\\n \\nchunks\\n \\nbut\\n \\nplacing\\n \\nthem\\n \\nat\\n \\nhigher\\n \\nranks\\n \\nsignificantly\\n \\nimpacts\\n \\nthe\\n \\nmodel‚Äôs\\n \\nability\\n \\nto\\n \\nproduce\\n \\naccurate\\n \\nanswers.\\n  Q83.  Given  a  retrieval  result  where  relevant  chunks  appear  at  positions  2,  \\n4,\\n \\n6,\\n \\nand\\n \\n8\\n \\nout\\n \\nof\\n \\n10\\n \\ntotal\\n \\nchunks,\\n \\nmanually\\n \\ncalculate\\n \\nthe\\n \\nContext\\n \\nPrecision@10.\\n \\nWhat\\n \\ndoes\\n \\nthis\\n \\nscore\\n \\ntell\\n \\nus\\n \\nabout\\n \\nthe\\n \\nretriever's\\n \\nranking\\n \\nability?  To  calculate  Context  Precision@10  with  relevant  chunks  at  positions  2,  4,  6,  and  8,  first  \\nassign\\n \\nrelevance\\n \\nindicators\\n \\nv_k=1\\n \\nat\\n \\nthese\\n \\nranks\\n \\nand\\n \\n0\\n \\nelsewhere.\\n \\nCalculate\\n \\nPrecision@k\\n \\nat\\n \\neach\\n \\nrelevant\\n \\nrank:\\n \\nat\\n \\n2,\\n \\nPrecision@2\\n \\n=\\n \\n1/2\\n \\n=\\n \\n0.5;\\n \\nat\\n \\n4,\\n \\nPrecision@4\\n \\n=\\n \\n2/4\\n \\n=\\n \\n0.5;\\n \\nat\\n \\n6,\\n \\nPrecision@6\\n \\n=\\n \\n3/6\\n \\n=\\n \\n0.5;\\n \\nat\\n \\n8,\\n \\nPrecision@8\\n \\n=\\n \\n4/8\\n \\n=\\n \\n0.5.\\n \\nThe\\n \\nweighted\\n \\nsum\\n \\nis\\n  \\n0.5+0.5+0.5+0.5=2.\\n \\n  Dividing  the  weighted  sum  by  the  total  number  of  relevant  chunks  (4)  gives  Context  \\nPrecision@10\\n \\n=\\n \\n0.5.\\n \\nThis\\n \\nscore\\n \\nindicates\\n \\nthe\\n \\nretriever\\n \\nhas\\n \\nmoderate\\n \\nranking\\n \\nability,\\n \\nretrieving\\n \\nrelevant\\n \\nchunks\\n \\nbut\\n \\nnot\\n \\nconsistently\\n \\nranking\\n \\nthem\\n \\nat\\n \\nthe\\n \\nvery\\n \\ntop,\\n \\nthus\\n \\nlimiting\\n \\noptimal\\n \\nprioritization\\n \\nof\\n \\nuseful\\n \\ncontext.\\n  Q84.  A  RAG  system  achieves  Context  Precision@5  =  0.8.  What  are  the  \\npossible\\n \\nscenarios\\n \\nthat\\n \\ncould\\n \\nlead\\n \\nto\\n \\nthis\\n \\nscore?  A  Context  Precision@5  score  of  0.8  in  a  RAG  system  indicates  that  not  all  of  the  top  five  \\nretrieved\\n \\nchunks\\n \\nare\\n \\nrelevant\\n \\nto\\n \\nthe\\n \\nground\\n \\ntruth.\\n \\nThis\\n \\ncould\\n \\noccur\\n \\nif,\\n \\nfor\\n \\ninstance,\\n \\nfour\\n \\nout\\n \\nof\\n \\nfive\\n \\nchunks\\n \\nare\\n \\nrelevant\\n \\n(v_k\\n \\n=\\n \\n1)\\n \\nand\\n \\none\\n \\nis\\n \\nirrelevant\\n \\n(v_k\\n \\n=\\n \\n0),\\n \\nleading\\n \\nto\\n \\na\\n \\nlower\\n \\nweighted\\n \\nsum\\n \\nof\\n \\nPrecision@k\\n \\nwhen\\n \\nnormalized\\n \\nby\\n \\nthe\\n \\ntotal\\n \\nnumber\\n \\nof\\n \\nrelevant\\n \\nchunks.\\n \\n  Another  scenario  might  involve  three  relevant  chunks  and  two  irrelevant  ones,  with  the  \\nrelevant\\n \\nchunks\\n \\nranked\\n \\nhigher\\n \\nbut\\n \\nstill\\n \\nresulting\\n \\nin\\n \\na\\n \\nscore\\n \\nless\\n \\nthan\\n \\n1\\n \\ndue\\n \\nto\\n \\nthe\\n \\npresence\\n \\nof\\n \\nirrelevant\\n \\nchunks.\\n \\n  Q85.  Explain  the  possible  reasons  for  a  RAG  retrieval  system  with  \\nconsistently\\n \\nlow\\n \\ncontext\\n \\nprecision.\\n \\n \\n Context  Precision  is  computed  as  the  weighted  sum  of  Precision@k,  normalized  by  the  \\nnumber\\n \\nof\\n \\nrelevant\\n \\nchunks.\\n \\nSo\\n \\nlow\\n \\nContext\\n \\nPrecision@k\\n \\nscores\\n \\nreflect\\n \\nthe\\n \\npresence\\n \\nof\\n \\n33                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 34, 'page_label': '35', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           high  proportion  of  irrelevant  chunks  or  poor  ranking  of  relevant  chunks  within  the  top  K  \\nresults.\\n \\n  This  could  stem  from  ineffective  query  understanding,  where  the  system  misinterprets  \\nthe\\n \\nuser‚Äôs\\n \\nintent,\\n \\nor\\n \\na\\n \\npoorly\\n \\ndesigned\\n \\nretrieval\\n \\nalgorithm\\n \\nthat\\n \\nfails\\n \\nto\\n \\nprioritize\\n \\nchunks\\n \\nmatching\\n \\nthe\\n \\nground\\n \\ntruth.\\n \\nAdditionally,\\n \\na\\n \\nnoisy\\n \\nor\\n \\nlow-quality\\n \\ndocument\\n \\ncorpus\\n \\nmight\\n \\ncontain\\n \\nfew\\n \\nrelevant\\n \\nchunks,\\n \\ncausing\\n \\nirrelevant\\n \\nones\\n \\nto\\n \\ndominate\\n \\nthe\\n \\nretrieved\\n \\nset.\\n \\n  Q86.  Compare  Context  Recall  with  traditional  information  retrieval  recall.  \\nWhy\\n \\nis\\n \\nContext\\n \\nRecall\\n \\ncomputed\\n \\nusing\\n \\n\"ground\\n \\ntruth\\n \\nclaims\"\\n \\nrather\\n \\nthan\\n \\nsimply\\n \\ncounting\\n \\nrelevant\\n \\ndocuments?  Context  Recall  in  RAG  retrieval  differs  from  traditional  information  retrieval  recall  by  \\nfocusing\\n \\non\\n \\nthe\\n \\ncompleteness\\n \\nof\\n \\ninformation\\n \\nthrough\\n \\nground\\n \\ntruth\\n \\nclaims\\n \\nrather\\n \\nthan\\n \\nmerely\\n \\ncounting\\n \\nrelevant\\n \\ndocuments.\\n \\n  While  traditional  recall  counts  how  many  relevant  documents  are  retrieved,  Context  \\nRecall\\n \\ndecomposes\\n \\nthe\\n \\nreference\\n \\nanswer\\n \\ninto\\n \\nindividual\\n \\nclaims\\n \\nand\\n \\nchecks\\n \\nif\\n \\nthese\\n \\nspecific\\n \\nclaims\\n \\nare\\n \\nfound\\n \\nin\\n \\nthe\\n \\nretrieved\\n \\ncontext.\\n \\n  This  approach  ensures  a  more  fine-grained  evaluation  of  whether  all  necessary  pieces  of  \\ninformation\\n \\nrequired\\n \\nto\\n \\nanswer\\n \\nthe\\n \\nquery\\n \\nare\\n \\npresent\\n \\nin\\n \\nthe\\n \\ncontext\\n \\nor\\n \\nnot.\\n \\n   Q87.  What  does  context  precision  measure  in  a  RAG  retriever,  and  how  \\ndoes\\n \\nit\\n \\ndiÔ¨Äer\\n \\nfrom\\n \\ncontext\\n \\nrecall?  Context  Precision  in  a  RAG  retriever  measures  how  well  the  system  ranks  relevant  \\nchunks\\n \\nof\\n \\ninformation\\n \\nhigher\\n \\nthan\\n \\nirrelevant\\n \\nones\\n \\nwithin\\n \\nthe\\n \\nretrieved\\n \\ncontext,\\n \\nemphasizing\\n \\nthe\\n \\nprioritization\\n \\nof\\n \\nuseful\\n \\ndata.\\n \\nIn\\n \\ncontrast,\\n \\nContext\\n \\nRecall\\n \\nassesses\\n \\nthe\\n \\ncompleteness\\n \\nof\\n \\nthe\\n \\nretrieved\\n \\ncontext,\\n \\nevaluating\\n \\nwhether\\n \\nall\\n \\nthe\\n \\nrelevant\\n \\npieces\\n \\nof\\n \\ninformation\\n \\nnecessary\\n \\nto\\n \\nanswer\\n \\nthe\\n \\nquery\\n \\nare\\n \\npresent.\\n \\n  Together,  they  provide  complementary  insights:  context  precision  ensures  useful  \\ninformation\\n \\nis\\n \\nprioritized,\\n \\nwhereas\\n \\ncontext\\n \\nrecall\\n \\nensures\\n \\nthat\\n \\nno\\n \\nimportant\\n \\ninformation\\n \\nis\\n \\nmissed.\\n  Q88.  In  a  RAG  pipeline,  how  might  context  recall  impact  the  completeness  \\nof\\n \\ngenerated\\n \\nanswers?\\n \\nDescribe\\n \\na\\n \\nscenario\\n \\nillustrating\\n \\nthis\\n \\nrelationship.  \\n34                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 35, 'page_label': '36', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            Context  Recall  in  a  RAG  pipeline  directly  impacts  the  completeness  of  generated  \\nanswers\\n \\nby\\n \\nmeasuring\\n \\nhow\\n \\nwell\\n \\nthe\\n \\nretriever\\n \\ngathers\\n \\nall\\n \\nrelevant\\n \\npieces\\n \\nof\\n \\ninformation\\n \\nrequired\\n \\nto\\n \\nanswer\\n \\nthe\\n \\nuser\\n \\nquery.\\n  For  instance,  if  a  question  about  a  historical  event  requires  multiple  claims  or  facts,  a  \\nlow\\n \\nContext\\n \\nRecall\\n \\nscore\\n \\nindicates\\n \\nthat\\n \\nsome\\n \\nkey\\n \\nfacts\\n \\nwere\\n \\nmissed\\n \\nin\\n \\nthe\\n \\nretrieved\\n \\ncontext,\\n \\nleading\\n \\nto\\n \\nincomplete\\n \\nanswers.\\n \\n  A  high  Context  Recall  ensures  the  generator  (LLM)  has  access  to  all  necessary  \\ninformation\\n \\nto\\n \\nproduce\\n \\na\\n \\ncomplete\\n \\nand\\n \\nwell-informed\\n \\nresponse.\\n \\n  Q89.  If  your  retriever  achieves  high  context  precision  but  low  context  \\nrecall,\\n \\nwhat\\n \\ntypes\\n \\nof\\n \\nuser\\n \\nqueries\\n \\nwould\\n \\nlikely\\n \\nsuÔ¨Äer\\n \\nmost?  If  a  retriever  in  a  RAG  pipeline  achieves  high  context  precision  but  low  context  recall,  \\nuser\\n \\nqueries\\n \\nthat\\n \\nrequire\\n \\nmultiple\\n \\ndistinct\\n \\npieces\\n \\nof\\n \\ninformation\\n \\nor\\n \\ncomprehensive\\n \\ncoverage\\n \\nare\\n \\nlikely\\n \\nto\\n \\nsuffer\\n \\nmost.\\n \\n  Queries,  like  complex  multi-fact  questions  or  those  needing  extensive  context  to  answer  \\nfully,\\n \\nwill\\n \\nsuffer\\n \\nbecause\\n \\ndespite\\n \\nthe\\n \\nretrieved\\n \\nchunks\\n \\nbeing\\n \\nrelevant\\n \\n(high\\n \\nprecision),\\n \\nmany\\n \\nessential\\n \\nrelevant\\n \\nchunks\\n \\nare\\n \\nmissing\\n \\noverall\\n \\n(low\\n \\nrecall).\\n \\n  This  results  in  incomplete  answers,  as  important  claims  or  facts  are  absent,  limiting  the  \\nmodel‚Äôs\\n \\nability\\n \\nto\\n \\ngenerate\\n \\na\\n \\nthorough\\n \\nresponse.\\n \\n  Q90.  In  what  situations  would  you  prioritize  Context  Precision  over  \\nContext\\n \\nRecall\\n \\nin\\n \\na\\n \\nRAG\\n \\nretriever,\\n \\nand\\n \\nhow\\n \\nwould\\n \\nthis\\n \\nimpact\\n \\nthe\\n \\ngenerator‚Äôs\\n \\nperformance?  In  situations  where  precision  is  critical,  such  as  in  high-risk  domains  like  healthcare,  \\nfinance,\\n \\nor\\n \\nlegal\\n \\napplications,\\n \\nprioritizing\\n \\nContext\\n \\nPrecision\\n \\nover\\n \\nContext\\n \\nRecall\\n \\nin\\n \\na\\n \\nRAG\\n \\nretriever\\n \\nis\\n \\nessential.\\n \\nThis\\n \\nensures\\n \\nthat\\n \\nonly\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\nand\\n \\ntrustworthy\\n \\ninformation\\n \\nis\\n \\nretrieved,\\n \\nminimizing\\n \\nthe\\n \\nrisk\\n \\nof\\n \\nincluding\\n \\nirrelevant\\n \\nor\\n \\nmisleading\\n \\ncontent\\n \\nthat\\n \\ncould\\n \\nnegatively\\n \\nimpact\\n \\nthe\\n \\ngenerator's\\n \\nresponse.\\n \\n  While  this  may  limit  the  breadth  of  information  (lower  recall),  it  improves  the  quality  \\nand\\n \\nreliability\\n \\nof\\n \\nthe\\n \\ngenerated\\n \\nanswers\\n \\nby\\n \\nreducing\\n \\nnoise.\\n \\n  \\n35                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q91.  Describe  a  scenario  where  a  RAG  system  might  achieve  high  Context  \\nRecall\\n \\nbut\\n \\nstill\\n \\nproduce\\n \\npoor\\n \\nanswers.\\n \\nWhat\\n \\ncomplementary\\n \\nmetrics\\n \\nwould\\n \\nyou\\n \\nuse\\n \\nalongside\\n \\nContext\\n \\nRecall\\n \\nto\\n \\nget\\n \\na\\n \\ncomplete\\n \\npicture\\n \\nof\\n \\nretriever\\n \\nperformance?  A  RAG  system  might  achieve  high  Context  Recall  by  retrieving  most  or  all  relevant  \\ninformation\\n \\npieces\\n \\nbut\\n \\nstill\\n \\nproduce\\n \\npoor\\n \\nanswers\\n \\nif\\n \\nthe\\n \\nretrieved\\n \\ncontext\\n \\ncontains\\n \\nnoisy\\n \\nor\\n \\nirrelevant\\n \\ndata\\n \\nthat\\n \\nconfuses\\n \\nthe\\n \\ngenerator.\\n \\n  To  get  a  complete  picture  of  retriever  performance,  complementary  metrics  like  Context  \\nPrecision\\n \\nshould\\n \\nbe\\n \\nused\\n \\nalongside\\n \\nContext\\n \\nRecall.\\n \\nContext\\n \\nRecall\\n \\nalong\\n \\nwith\\n \\nContext\\n \\nPrecision\\n  \\nensures\\n \\nretrieved\\n \\ncontent\\n \\nis\\n \\nnot\\n \\nonly\\n \\ncomprehensive\\n \\nbut\\n \\nis\\n \\nalso\\n \\nrelevant\\n \\nand\\n \\nwell-ranked.\\n  Q92.  If  your  RAG  retriever  consistently  shows  Context  Recall  scores  below  \\n0.6,\\n \\nwhat\\n \\nare\\n \\nthe\\n \\nthree\\n \\npotential\\n \\nroot\\n \\ncauses?  Context  Recall  scores  below  0.6  mean  that  the  retriever  is  missing  a  significant  portion  \\nof\\n \\nthe\\n \\nrelevant\\n \\ninformation\\n \\nrequired\\n \\nto\\n \\nanswer\\n \\nuser\\n \\nqueries.\\n \\n  The  three  potential  root  causes  are   (1)  an  incomplete  or  outdated  knowledge  base  lacking  necessary  information,   (2)  ineffective  embedding  model  or  ranking  algorithms  causing  semantically  relevant  \\nchunks\\n \\nto\\n \\nbe\\n \\nmissed,\\n \\nand\\n \\n (3)  poor  chunking  strategy  leading  to  loss  of  key  information.    Q93.  Why  is  it  important  for  RAG  systems  to  optimize  both  context  \\nprecision\\n \\nand\\n \\ncontext\\n \\nrecall\\n \\nsimultaneously?\\n \\nWhat\\n \\ntrade-oÔ¨Äs\\n \\nmight\\n \\noccur?  It  is  important  for  RAG  systems  to  optimize  both  context  precision  and  context  recall  \\nsimultaneously.\\n \\nThis\\n \\nis\\n \\nbecause\\n \\ncontext\\n \\nprecision\\n \\nensures\\n \\nthat\\n \\nthe\\n \\nretrieved\\n \\ninformation\\n \\nis\\n \\nhighly\\n \\nrelevant\\n \\nand\\n \\nranked\\n \\nappropriately.\\n \\nThe\\n \\nContext\\n \\nRecall\\n \\nmetric\\n \\nensures\\n \\nthat\\n \\nall\\n \\nnecessary\\n \\ninformation\\n \\nis\\n \\nincluded\\n \\nin\\n \\nthe\\n \\nretrieved\\n \\ncontext\\n \\nso\\n \\nthat\\n \\nthe\\n \\ngenerator\\n \\ncan\\n \\noutput\\n \\na\\n \\ncomplete\\n \\nanswer.\\n \\n  The  trade-off  often  arises  because  increasing  recall  by  retrieving  more  chunks  may  \\nintroduce\\n \\nirrelevant\\n \\nchunks,\\n \\nlowering\\n \\nprecision.\\n \\nAt\\n \\nthe\\n \\nsame\\n \\ntime,\\n \\nfocusing\\n \\nsolely\\n \\non\\n \\nprecision\\n \\nmight\\n \\nomit\\n \\nimportant\\n \\ninformation,\\n \\nleading\\n \\nto\\n \\nincomplete\\n \\nresponses.\\n \\n  \\n36                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 37, 'page_label': '38', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Balancing  these  metrics  helps  create  a  RAG  system  that  retrieves  relevant  content  \\nefficiently\\n \\nwhile\\n \\ncovering\\n \\nthe\\n \\nquery\\n \\ncomprehensively,\\n \\nresulting\\n \\nin\\n \\naccurate\\n \\nand\\n \\nthorough\\n \\ngenerated\\n \\nanswers.\\n  Q94.  Explain  why  Context  Relevancy  is  considered  a  \"reference-free\"  metric  \\nwhile\\n \\nContext\\n \\nPrecision\\n \\nand\\n \\nContext\\n \\nRecall\\n \\nare\\n \\n\"reference-dependent.\"\\n \\nWhen\\n \\nwould\\n \\nyou\\n \\nprefer\\n \\nusing\\n \\nContext\\n \\nRelevancy\\n \\nover\\n \\nthe\\n \\nother\\n \\ntwo\\n \\nmetrics?  Context  Relevancy  is  considered  a  \"reference-free\"  metric  because  it  evaluates  how  \\nrelevant\\n \\nthe\\n \\nretrieved\\n \\ncontext\\n \\nis\\n \\nto\\n \\nthe\\n \\nuser‚Äôs\\n \\nquery\\n \\nwithout\\n \\nneeding\\n \\na\\n \\nreference\\n \\nanswer.\\n \\nIt\\n \\nmeasures\\n \\nthe\\n \\nproportion\\n \\nof\\n \\nstatements\\n \\nin\\n \\nthe\\n \\nretrieved\\n \\ncontext\\n \\nthat\\n \\nare\\n \\nrelevant\\n \\nto\\n \\nthe\\n \\nquery.\\n \\n  In  contrast,  Context  Precision  and  Context  Recall  are  \"reference-dependent\"  as  they  \\nrequire\\n \\na\\n \\nreference\\n \\nanswer\\n \\nto\\n \\ndetermine\\n \\nrelevance\\n \\nand\\n \\ncompleteness\\n \\nof\\n \\nretrieval.\\n \\n  Context  Relevancy  is  preferred  when  reference  answers  are  unavailable.  The  Context  \\nRelevancy\\n \\nmetric\\n \\noffers\\n \\na\\n \\nway\\n \\nto\\n \\nassess\\n \\nretrieval\\n \\nquality\\n \\nbased\\n \\nsolely\\n \\non\\n \\nthe\\n \\nquery\\n \\nand\\n \\nretrieved\\n \\ncontext\\n \\nitself.\\n \\nThis\\n \\nis\\n \\nuseful\\n \\nfor\\n \\nreal-time\\n \\nscenarios\\n \\nwhere\\n \\nground\\n \\ntruth\\n \\nmay\\n \\nnot\\n \\nexist.\\n   Q95.  Describe  a  scenario  where  a  RAG  retriever  achieves  high  Context  \\nRelevancy\\n \\nbut\\n \\nlow\\n \\nContext\\n \\nPrecision.\\n \\nWhat\\n \\ndoes\\n \\nthis\\n \\nimply\\n \\nabout\\n \\nthe\\n \\nretriever‚Äôs\\n \\nperformance?  A  RAG  retriever  achieves  high  Context  Relevancy  but  low  Context  Precision  when  it  \\nretrieves\\n \\na\\n \\ncontext\\n \\nwhere\\n \\nmost\\n \\nstatements\\n \\nare\\n \\nrelevant\\n \\nto\\n \\nthe\\n \\nuser‚Äôs\\n \\nquery,\\n \\nbut\\n \\nthe\\n \\nrelevant\\n \\nchunks\\n \\nare\\n \\nranked\\n \\nlower\\n \\nin\\n \\nthe\\n \\nretrieved\\n \\nlist,\\n \\novershadowed\\n \\nby\\n \\nirrelevant\\n \\nones.\\n \\n  For  example,  if  a  query  about  \"machine  learning  algorithms\"  retrieves  a  context  with  \\nmany\\n \\nrelevant\\n \\nstatements\\n \\nbut\\n \\nplaces\\n \\nthem\\n \\nafter\\n \\nless\\n \\nrelevant\\n \\nor\\n \\nnoisy\\n \\nchunks,\\n \\nContext\\n \\nRelevancy\\n \\nis\\n \\nhigh\\n \\n(most\\n \\nstatements\\n \\nare\\n \\nquery-related),\\n \\nbut\\n \\nContext\\n \\nPrecision@K\\n \\nis\\n \\nlow\\n \\ndue\\n \\nto\\n \\npoor\\n \\nranking\\n \\nof\\n \\nrelevant\\n \\nchunks.\\n \\n  This  implies  the  retriever  is  effective  at  fetching  relevant  content  but  struggles  to  \\nprioritize\\n \\nrelevant\\n \\nchunks\\n \\nover\\n \\nirrelevant\\n \\nones.\\n \\n  \\n37                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 38, 'page_label': '39', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                            Q96.  Suppose  a  RAG  retriever  retrieves  all  relevant  chunks  but  includes  \\nmany\\n \\nirrelevant\\n \\nones,\\n \\nleading\\n \\nto\\n \\nlow\\n \\nContext\\n \\nRelevancy.\\n \\nHow\\n \\nwould\\n \\nyou\\n \\nimprove\\n \\nthe\\n \\nretriever\\n \\nto\\n \\naddress\\n \\nthis\\n \\nissue?  A  RAG  retriever  retrieving  all  relevant  chunks  along  with  many  irrelevant  ones  results  in  \\nlow\\n \\ncontext\\n \\nrelevancy\\n \\nscores.\\n \\nThis\\n \\ncan\\n \\nbe\\n \\naddressed\\n \\nby\\n \\nimproving\\n \\nthe\\n \\nretriever\\n \\nby\\n \\nrefining\\n \\nits\\n \\nfiltering\\n \\nand\\n \\nranking\\n \\nmechanisms.\\n \\nTechniques\\n \\nsuch\\n \\nas\\n \\nenhancing\\n \\nembedding\\n \\nmodel\\n \\nquality,\\n \\napplying\\n \\nstricter\\n \\nsimilarity\\n \\nthresholds,\\n \\nor\\n \\nintegrating\\n \\na\\n \\nre-ranking\\n \\nmodel\\n \\ncan\\n \\nhelp\\n \\nprioritize\\n \\nhighly\\n \\nrelevant\\n \\nchunks\\n \\nand\\n \\nsuppress\\n \\nnoise.\\n \\n  Additionally,  improving  the  chunking  strategy  to  create  more  precise  and  semantically  \\ncoherent\\n \\nchunks\\n \\ncan\\n \\nreduce\\n \\nirrelevant\\n \\nretrievals.\\n \\nThese\\n \\noptimizations\\n \\nensure\\n \\nretrieved\\n \\ncontext\\n \\nis\\n \\nboth\\n \\ncomprehensive\\n \\nand\\n \\nfocused\\n \\non\\n \\nthe\\n \\nmost\\n \\nrelevant\\n \\ninformation.\\n  Q97.  How  does  the  Faithfulness  metric  assess  the  quality  of  a  RAG  \\ngenerator?\\n \\n The  Faithfulness  metric  assesses  the  quality  of  a  RAG  generator  by  measuring  how  \\nfactually\\n \\nconsistent\\n \\nthe\\n \\ngenerated\\n \\nresponse\\n \\nis\\n \\nwith\\n \\nthe\\n \\nretrieved\\n \\ncontext.\\n \\nIt\\n \\nis\\n \\ncomputed\\n \\nas\\n \\nthe\\n \\nratio\\n \\nof\\n \\nclaims\\n \\nin\\n \\nthe\\n \\nresponse\\n \\nthat\\n \\nare\\n \\nsupported\\n \\nby\\n \\nthe\\n \\nretrieved\\n \\ncontext\\n \\nto\\n \\nthe\\n \\ntotal\\n \\nnumber\\n \\nof\\n \\nclaims.\\n  \\n  A  score  of  1  indicates  all  claims  are  fully  supported,  reflecting  high  factual  accuracy.  A  \\nscore\\n \\nof\\n \\n0\\n \\nshows\\n \\nno\\n \\nclaims\\n \\nare\\n \\nsupported,\\n \\nindicating\\n \\ncomplete\\n \\nfactual\\n \\ninconsistency.\\n \\nThis\\n \\nmetric\\n \\nensures\\n \\nthe\\n \\nRAG\\n \\nsystem\\n \\ngenerates\\n \\nreliable\\n \\nand\\n \\ncontextually\\n \\ngrounded\\n \\nresponses.\\n  Q98.  Distinguish  between  Faithfulness  and  Context  Precision  metrics  in  RAG  \\nevaluation.\\n \\nWhy\\n \\nmight\\n \\na\\n \\nsystem\\n \\nhave\\n \\nhigh\\n \\nContext\\n \\nPrecision\\n \\nbut\\n \\nlow\\n \\nFaithfulness,\\n \\nand\\n \\nwhat\\n \\nwould\\n \\nthis\\n \\nindicate\\n \\nabout\\n \\nyour\\n \\npipeline?  Faithfulness  measures  how  factually  consistent  a  RAG  generator‚Äôs  response  is  with  the  \\nretrieved\\n \\ncontext.\\n \\nThis\\n \\nmetric\\n \\nis\\n \\ncomputed\\n \\nas\\n \\nthe\\n \\nratio\\n \\nof\\n \\nsupported\\n \\nclaims\\n \\nto\\n \\ntotal\\n \\nclaims\\n \\nin\\n \\nthe\\n \\nresponse.\\n \\nThe\\n \\nContext\\n \\nPrecision\\n \\nmetric\\n \\nfocuses\\n \\non\\n \\nthe\\n \\nprioritization\\n \\nof\\n \\nrelevant\\n \\ninformation\\n \\nby\\n \\nevaluating\\n \\nhow\\n \\nwell\\n \\na\\n \\nretriever\\n \\nranks\\n \\nrelevant\\n \\nchunks\\n \\nwithin\\n \\nthe\\n \\ntop\\n \\nK.\\n \\n  \\n38                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 39, 'page_label': '40', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content='üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           A  system  might  have  high  Context  Precision  but  low  Faithfulness  if  the  retriever  \\neffectively\\n \\nranks\\n \\nrelevant\\n \\nchunks\\n \\nhighly,\\n \\nbut\\n \\nthe\\n \\ngenerator\\n \\nintroduces\\n \\nunsupported\\n \\nor\\n \\ncontradictory\\n \\nclaims\\n \\nnot\\n \\ngrounded\\n \\nin\\n \\nthe\\n \\ncontext.\\n \\nThis\\n \\nindicates\\n \\na\\n \\nstrong\\n \\nretrieval\\n \\nstage\\n \\nbut\\n \\na\\n \\nflawed\\n \\ngeneration\\n \\nstage,\\n \\nwhere\\n \\nthe\\n \\nmodel\\n \\nfails\\n \\nto\\n \\naccurately\\n \\ninterpret\\n \\nor\\n \\nutilize\\n \\nthe\\n \\nretrieved\\n \\ninformation.\\n  Q99.  A  RAG  system  has  high  context  precision  but  low  faithfulness.  How  \\nwould\\n \\nyou\\n \\naddress\\n \\nthis?  A  RAG  system  with  high  context  precision  and  low  faithfulness  happens  when  the  \\nretriever\\n \\nis\\n \\nselecting\\n \\nrelevant\\n \\nchunks\\n \\naccurately,\\n \\nbut\\n \\nthe\\n \\ngenerator\\n \\nis\\n \\nproducing\\n \\nresponses\\n \\nwith\\n \\nunsupported\\n \\nclaims.\\n \\nTo\\n \\naddress\\n \\nthis,\\n \\none\\n \\nshould\\n \\nfocus\\n \\non\\n \\nimproving\\n \\nthe\\n \\ngenerator‚Äôs\\n \\ngrounding\\n \\nand\\n \\nclaim\\n \\nverification\\n \\nprocesses.\\n  Use  stronger  cross-checking  mechanisms  like  natural  language  inference  models  or  \\nfact-checking\\n \\nmodules\\n \\nagainst\\n \\nthe\\n \\nretrieved\\n \\ncontext.\\n \\nAdditionally,\\n \\ntuning\\n \\nthe\\n \\ngeneration\\n \\nprompts\\n \\nto\\n \\nencourage\\n \\nreliance\\n \\non\\n \\nthe\\n \\ncontext\\n \\ncan\\n \\nhelp\\n \\nincrease\\n \\nfaithfulness.\\n \\n  Q100.  Why  might  a  RAG  system  with  perfect  Context  Recall  still  fail  to  \\nproduce\\n \\naccurate\\n \\nresponses?\\n \\nHow\\n \\ndoes\\n \\nthe\\n \\nFaithfulness\\n \\nmetric\\n \\nhelp\\n \\ndiagnose\\n \\nthis\\n \\nissue?  Context  recall  evaluates  the  completeness  of  the  retrieved  context  in  a  RAG  pipeline.  \\nPerfect\\n \\nContext\\n \\nRecall\\n \\nmeans\\n \\nthe\\n \\nretrieved\\n \\ncontext\\n \\nincludes\\n \\nall\\n \\nthe\\n \\nground\\n \\ntruth\\n \\nclaims.\\n  \\nA\\n \\nRAG\\n \\nsystem\\n \\nwith\\n \\nperfect\\n \\nContext\\n \\nRecall\\n \\nmay\\n \\nstill\\n \\nfail\\n \\nto\\n \\nproduce\\n \\naccurate\\n \\nresponses.\\n \\nThis\\n \\nhappens\\n \\nwhen\\n \\nthe\\n \\ngenerator\\n \\nhallucinates\\n \\nand\\n  \\nincludes\\n \\nclaims\\n \\nunsupported\\n \\nby\\n \\nthe\\n \\nretrieved\\n \\ncontext.\\n \\n  The  Faithfulness  metric  helps  diagnose  this  issue  by  measuring  how  many  claims  in  the  \\ngenerated\\n \\nresponse\\n \\nare\\n \\nfactually\\n \\nsupported\\n \\nby\\n \\nthe\\n \\nretrieved\\n \\ncontext.\\n \\nA\\n \\nlow\\n \\nor\\n \\nmoderate\\n \\nfaithfulness\\n \\nmetric\\n \\nscore\\n \\nindicates\\n \\na\\n \\nless\\n \\naccurate\\n \\nresponse,\\n \\ni.e.,\\n \\nthe\\n \\nresponse\\n \\nincludes\\n \\nunsupported\\n \\nclaims.\\n  \\n  Q101.  Explain  how  hallucinations  in  LLMs  speciÔ¨Åcally  impact  the  \\nFaithfulness\\n \\nmetric.\\n \\nWhat\\n \\ntechniques\\n \\ncould\\n \\nyou\\n \\nimplement\\n \\nto\\n \\nimprove\\n \\nthe\\n \\nFaithfulness\\n \\nmetric\\n \\nscore?  The  faithfulness  metric  measures  the  proportion  of  claims  in  the  response  that  are  \\nbacked\\n \\nup\\n \\nby\\n \\ncontext.\\n  \\nHallucinations\\n \\nreduce\\n \\nthe\\n \\nfaithfulness\\n \\nmetric\\n \\nscore\\n \\nby\\n \\n39                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )'),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 40, 'page_label': '41', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           introducing  unsupported  claims  in  the  generated  response.  These  unsupported  claims  \\neither\\n \\ncontradict\\n \\nor\\n \\nhave\\n \\nno\\n \\nbasis\\n \\nin\\n \\nthe\\n \\nprovided\\n \\ncontext.\\n  To  improve  Faithfulness  scores,  techniques  such  as  incorporating  natural  language  \\ninference\\n \\n(NLI)\\n \\nor\\n \\nfact-checking\\n \\nmodels\\n \\nto\\n \\nverify\\n \\nclaims,\\n \\nusing\\n \\nprompt\\n \\nengineering\\n \\nto\\n \\ndiscourage\\n \\nunsupported\\n \\ngeneration,\\n \\netc.,\\n \\ncan\\n \\nbe\\n \\nused.\\n  Q102.  How  does  Response  Relevancy  diÔ¨Äer  from  Context  Relevancy,  and  \\nwhy\\n \\ndo\\n \\nyou\\n \\nneed\\n \\nboth\\n \\nmetrics\\n \\nto\\n \\nproperly\\n \\nevaluate\\n \\na\\n \\nRAG\\n \\nsystem?   Response  Relevancy  measures  how  well  a  RAG  system's  generated  response  aligns  with  \\nthe\\n \\nuser‚Äôs\\n \\nquery\\n \\nby\\n \\ncalculating\\n \\nthe\\n \\nratio\\n \\nof\\n \\nrelevant\\n \\nstatements\\n \\nin\\n \\nthe\\n \\nresponse\\n \\nto\\n \\nthe\\n \\ntotal\\n \\nstatements.\\n \\nContext\\n \\nRelevancy\\n \\nevaluates\\n \\nthe\\n \\nrelevance\\n \\nof\\n \\nretrieved\\n \\ncontext\\n \\nto\\n \\nthe\\n \\nquery\\n \\nby\\n \\nmeasuring\\n \\nthe\\n \\nproportion\\n \\nof\\n \\nrelevant\\n \\nstatements\\n \\nin\\n \\nthe\\n \\ncontext.\\n \\n  Both  metrics  are  essential  because  Context  Relevancy  ensures  the  retriever  fetches  \\nrelevant\\n \\ncontext,\\n \\nwhile\\n \\nResponse\\n \\nRelevancy\\n \\nverifies\\n \\nthat\\n \\nthe\\n \\ngenerator\\n \\nproduces\\n \\nan\\n \\nanswer\\n \\ndirectly\\n \\naddressing\\n \\nthe\\n \\nquery.\\n \\n  A  RAG  system  could  retrieve  relevant  context  but  generate  an  off-topic  response,  or  vice  \\nversa.\\n \\nHence,\\n \\nevaluating\\n \\nboth\\n \\nensures\\n \\nthe\\n \\nentire\\n \\nRAG\\n \\npipeline‚Äîretrieval\\n \\nand\\n \\ngeneration‚Äîperforms\\n \\neffectively.\\n \\n  Q103.  The  generator‚Äôs  response  mentions  facts  not  present  in  the  retrieved  \\ncontext.\\n \\nDescribe\\n \\nhow\\n \\nfaithfulness\\n \\nand\\n \\nresponse\\n \\nrelevancy\\n \\nmetrics\\n \\nwould\\n \\nbe\\n \\nimpacted.  The  faithfulness  metric  measures  the  proportion  of  claims  in  the  response  that  are  \\nbacked\\n \\nup\\n \\nby\\n \\ncontext.\\n \\nTherefore,\\n \\nthe\\n \\nscore\\n \\nwill\\n \\ndecrease\\n \\nif\\n \\nthe\\n \\nLLM-generated\\n \\nanswer\\n \\ncontains\\n \\nunsupported\\n \\nclaims.\\n  In  the  case  of  the  Response  Relevancy  metric,  the  score  will  decrease  only  if  the  \\nunsupported\\n \\nfacts\\n \\nare\\n \\nirrelevant\\n \\nto\\n \\nthe\\n \\nuser\\n \\nquery.\\n \\nOtherwise,\\n \\nthe\\n \\nscore\\n \\nwill\\n \\nremain\\n \\nhigh.\\n  This  underscores  a  key  difference  between  these  two  metrics:  Faithfulness  metric  looks  \\nfor\\n \\nanswer‚Äôs\\n \\nfactual\\n \\nconsistency\\n \\nwith\\n \\nthe\\n \\ncontext,\\n \\nwhile\\n \\nResponse\\n \\nRelevancy\\n \\nassesses\\n \\nanswer‚Äôs\\n \\nrelevancy\\n \\nwith\\n \\nthe\\n \\nquery.\\n  \\n40                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'iLovePDF', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-10-12T05:46:27+00:00', 'source': '..\\\\data\\\\pdf\\\\AI.pdf', 'total_pages': 42, 'page': 41, 'page_label': '42', 'source_file': 'AI.pdf', 'file_type': 'pdf'}, page_content=\"üöÄ AIxFunda  Newsletter                                                           aixfunda.substack.com                           Q104.  How  does  the  Response  Relevancy  metric  help  evaluate  whether  a  \\nRAG\\n \\ngenerator\\n \\nis\\n \\naddressing\\n \\nthe\\n \\nuser‚Äôs\\n \\nquery\\n \\neÔ¨Äectively?  The  Response  Relevancy  metric  is  computed  as  the  ratio  of  relevant  statements  in  the  \\nresponse\\n \\nto\\n \\nthe\\n \\ntotal\\n \\nnumber\\n \\nof\\n \\nstatements.\\n \\nSo,\\n \\nthis\\n \\nmetric\\n \\nchecks\\n \\nthe\\n \\neffectiveness\\n \\nof\\n \\nthe\\n \\nRAG\\n \\ngenerator\\n \\nby\\n \\nmeasuring\\n \\nhow\\n \\nwell\\n \\nthe\\n \\nresponse\\n \\naligns\\n \\nwith\\n \\nthe\\n \\nuser\\n \\nquery.\\n  A  score  close  to  1  means  the  answer  directly  addresses  the  query  with  little  to  no  \\nirrelevant\\n \\ncontent.\\n \\nA\\n \\nscore\\n \\nclose\\n \\nto\\n \\n0\\n \\nmeans\\n \\nthat\\n \\nthe\\n \\nanswer\\n \\ncontains\\n \\ninformation\\n \\nthat\\n \\nis\\n \\nnot\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nquestion.\\n  \\n \\n Q105.  When  evaluating  RAG  generator  output,  what  are  the  risks  of  relying  \\nsolely\\n \\non\\n \\nresponse\\n \\nrelevancy?\\n \\nHow\\n \\ncan\\n \\nincluding\\n \\nthe\\n \\nfaithfulness\\n \\nmetric\\n \\nimprove\\n \\nreliability?  The  Response  Relevancy  metric  tells  you  how  relevant  the  answer  is  to  the  user  query.  \\nBut\\n \\nthis\\n \\nmetric\\n \\ndoesn't\\n \\ncheck\\n \\nif\\n \\nthe\\n \\nanswer\\n \\nis\\n \\nbased\\n \\non\\n \\nthe\\n \\nretrieved\\n \\ncontext,\\n \\nso\\n \\nit\\n \\nmisses\\n \\nfactual\\n \\nerrors.\\n  The  faithfulness  metric  is  the  number  of  supported  claims  divided  by  the  total  number  \\nof\\n \\nclaims.\\n \\nAdding\\n \\nthe\\n \\nfaithfulness\\n \\nmetric\\n \\nmakes\\n \\nthe\\n \\nsystem\\n \\nmore\\n \\nreliable\\n \\nby\\n \\nmaking\\n \\nsure\\n \\nthat\\n \\nthe\\n \\nclaims\\n \\nin\\n \\nthe\\n \\nLLM-generated\\n \\nresponse\\n \\nare\\n \\nsupported\\n \\nby\\n \\nthe\\n \\ncontext.\\n    This  dual  evaluation  ensures  the  RAG  system  delivers  both  relevant  and  factual  \\nresponses,\\n \\nreducing\\n \\nthe\\n \\nrisk\\n \\nof\\n \\nmisleading\\n \\noutputs.\\n  \\n         \\n41                                                           Kalyan  KS  (Follow  on  Twitter  and  LinkedIn )\"),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2026-02-01T22:33:23+05:30', 'author': 'HP', 'moddate': '2026-02-01T22:33:23+05:30', 'source': '..\\\\data\\\\pdf\\\\attachment.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'attachment.pdf', 'file_type': 'pdf'}, page_content='ABHISHEK PANT \\n   Phone:  8126961106                                                                                                                | Email:- abhishekpant.as@gmail.com    \\n   LinkedIn:-www.linkedin.com/in/abhishek-pant-2b2129236                                                                             \\n  GitHub:-https://github.com/Rusty-user365                                                                              \\n \\nTechnical Skills \\n \\n     Programming:Python (Pandas, NumPy, Seaborn, Matplotlib, Scikit-Learn  Regression, Classification, Model Evaluation), SQL \\n     Machine Learning & Deep Learning:TensorFlow, Transformers (Hugging Face), OpenCV \\n     Natural Language Processing: LLM Fine‚ÄëTuning, RAG Systems, Embeddings, LangChain, ChromaDB \\n     Data & Visualization: Data Cleaning, Feature Engineering, Exploratory Data Analysis (EDA), Matplotlib, Seaborn \\n     MLOps & Deployment: MLflow, DVC, Docker, Kubernetes, CI/CD (GitHub Actions), Model Registry, \\n     Experiment Tracking,Monitoring \\n     Tools & Platforms: Git, Jupyter Notebook, VS Code, Aws \\nProjects  \\n  Streamlit Automation for Machine Learning Tasks (Project Link)                                                         Nov 2025  ‚Äì  Dec 2025 \\n‚Ä¢ Developed a no‚Äëcode Streamlit application to automate basic machine learning workflows. \\n‚Ä¢ Implemented dataset selection, preprocessing (scaling, train/test split), and model training pipelines. \\n‚Ä¢ Integrated multiple ML algorithms (Logistic Regression, SVM, Random Forest, XGBoost, KNN) for experimentation. \\n‚Ä¢ Enabled interactive visualizations, classification reports, and accuracy metrics for model evaluation. \\n‚Ä¢ Designed modular utility functions (ml_utility) to streamline data handling, training, and evaluation. \\n    Tech Used: Python, Streamlit \\n Reddit Persona Generation Tool (LLM-based) (PROJECT LINK)                                                                June 2025  ‚Äì  July 2025 \\n‚Ä¢ Designed a system to analyze a user‚Äôs Reddit posts and comments to generate a detailed persona profile. \\n‚Ä¢ Implemented persona extraction using a local LLM via Ollama. \\n‚Ä¢ Extracted attributes such as interests, profession, personality traits, and age group with source citations. \\n‚Ä¢ Structured outputs for explainability and traceability. \\nTech Used: Python, LLM (Ollama), NLP \\nFinancial RAG Chatbot ‚Äì Intelligent Document Processing & Retrieval (PROJECT LINK)                         Feb 2025 ‚Äì Aug 2025 \\n‚Ä¢ Designed a financial document processing pipeline using a fine-tuned Qwen family LLM to extract key financial \\nentities from PDFs into structured JSON. \\n‚Ä¢ Integrated the financial LLM into a Retrieval-Augmented Generation (RAG) chatbot for domain-specific Q&A. \\n‚Ä¢ Embedded curated financial content into ChromaDB and implemented semantic search with LangChain for context \\nretrieval. \\n‚Ä¢ Enabled grounded, explainable responses by combining extracted structured data with retrieved knowledge. \\nTech Used: Python, LLM (Qwen, Ollama), LangChain, ChromaDB, NLP, Jupyter Notebook \\nMLOps Pipeline ‚Äì Experiment Tracking & Deployment (PROJECT LINK)                                                     Jan 2026- Feb 2026 \\n‚Ä¢ Implemented an end-to-end machine learning workflow for a classification task using MLflow for experiment \\ntracking and model registry. \\n‚Ä¢ Automated hyperparameter tuning with GridSearchCV, logging each run‚Äôs parameters, metrics, and artifacts. \\n‚Ä¢ Containerized the application with Docker and deployed it locally for reproducible environments. \\n‚Ä¢ Integrated GitHub Actions CI/CD to automate testing and deployment on every commit. \\n‚Ä¢ Demonstrated model lifecycle management by promoting models from Staging to Production in MLflow Registry. \\nTech Used: Python, Scikit-Learn, MLflow, Docker, GitHub Actions, Git \\n \\n \\n  Education \\n \\nUttaranchal University                                                                                                                                             Aug 2023 ‚Äì June 2025 \\nMaster of Computer Applications(AI/ML)                                                                                                                  Dehradun, Uttarakhand'),\n",
       " Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2026-02-01T22:33:23+05:30', 'author': 'HP', 'moddate': '2026-02-01T22:33:23+05:30', 'source': '..\\\\data\\\\pdf\\\\attachment.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'attachment.pdf', 'file_type': 'pdf'}, page_content='Certifications  \\n‚Ä¢ Oracle Cloud Infrastructure 2025 Generative AI Professional (1Z0-1127-25) ‚Äì Oracle \\n‚Ä¢ Oracle Cloud Infrastructure 2025  Data Science Professional (1Z0-1110-25 ) ‚Äì Oracle \\n‚Ä¢ Advanced AI and Machine Learning Techniques and Capstone ‚Äì Microsoft \\n‚Ä¢ Python ‚Äì HackerRank \\n‚Ä¢ Software Engineer Intern (Skill Certification) ‚Äì HackerRank \\n‚Ä¢ AWS ‚Äì Scaler'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 0, 'page_label': '1', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 1, 'page_label': '2', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='CONTENTS\\n\\xa0\\n\\xa0\\n1. How will you improve the performance of a program in Python?\\n2. What are the benefits of using Python?\\n3. How will you specify source code encoding in a Python source file?\\n4. What is the use of PEP 8 in Python?\\n5. What is Pickling in Python?\\n6. How does memory management work in Python?\\n7. How will you perform Static Analysis on a Python Script?\\n8. What is the difference between a Tuple and List in Python?\\n9. What is a Python Decorator?\\n10. How are arguments passed in a Python method? By value or by\\nreference?\\n11. What is the difference between List and Dictionary data types in\\nPython?\\n12. What are the different built-in data types available in Python?\\n13. What is a Namespace in Python?\\n14. How will you concatenate multiple strings together in Python?\\n15. What is the use of Pass statement in Python?\\n16. What is the use of Slicing in Python?\\n17. What is the difference between Docstring in Python and Javadoc in\\nJava?\\n18. How do you perform unit testing for Python code?\\n19. What is the difference between an Iterator and Iterable in Python?\\n20. What is the use of Generator in Python?'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 2, 'page_label': '3', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='21. What is the significance of functions that start and end with _\\nsymbol in Python?\\n22. What is the difference between xrange and range in Python?\\n23. What is lambda expression in Python?\\n24. How will you copy an object in Python?\\n25. What are the main benefits of using Python?\\n26. What is a metaclass in Python?\\n27. What is the use of frozenset in Python?\\n28. What is Python Flask?\\n29. What is None in Python?\\n30. What is the use of zip() function in Python?\\n31. What is the use of // operator in Python?\\n32. What is a Module in Python?\\n33. How can we create a dictionary with ordered set of keys in Python?\\n34. Python is an Object Oriented programming language or a\\nfunctional programming language?\\n35. How can we retrieve data from a MySQL database in a Python\\nscript?\\n36. What is the difference between append() and extend() functions of a\\nlist in Python?\\n37. How will you handle an error condition in Python code?\\n38. What is the difference between split() and slicing in Python?\\n39. How will you check in Python, if a class is subclass of another class?\\n40. How will you debug a piece of code in Python?\\n41. How do you profile a Python script?'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 3, 'page_label': '4', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='42. What is the difference between ‚Äòis‚Äô and ‚Äò==‚Äô in Python?\\n43. How will you share variables across modules in Python?\\n44. How can we do Functional programming in Python?\\n45. What is the improvement in enumerate() function of Python?\\n46. How will you execute a Python script in Unix?\\n47. What are the popular Python libraries used in Data analysis?\\n48. What is the output of following code in Python?\\n49. What is the output of following code in Python?\\n50. If you have data with name of customers and their location, which\\ndata type will you use to store it in Python?\\n\\xa0\\n \\nACKNOWLEDGMENTS\\n\\xa0\\n\\xa0\\nWe thank our readers who constantly send feedback\\nand reviews to motivate us in creating these useful\\nbooks with the latest information!'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 4, 'page_label': '5', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='INTRODUCTION\\n\\xa0\\nThis book contains basic to expert level Python interview questions that an\\ninterviewer asks. Each question is accompanied with an answer so that you\\ncan prepare for job interview in short time.\\nWe have compiled this list after attending dozens of technical interviews in\\ntop-notch companies like- Google, Facebook, Netflix, Amazon etc.\\nOften, these questions and concepts are used in our daily programming\\nwork. But these are most helpful when an Interviewer is trying to test your\\ndeep knowledge of Python.\\nThe difficulty rating on these Questions varies from a Fresher level\\nsoftware programmer to a Senior software programmer.\\nOnce you go through them in the first pass, mark the questions that you\\ncould not answer by yourself. Then, in second pass go through only the\\ndifficult questions.\\nAfter going through this book 2-3 times, you will be well prepared to face a\\ntechnical interview on Python for an experienced programmer.\\n \\n\\xa0\\nPython Interview Questions'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 5, 'page_label': '6', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='1. How will you improve the performance\\nof a program in Python?\\nThere are many ways to improve the performance of a Python\\nprogram. Some of these are as follows:\\n\\xa0\\ni. Data Structure: We have to select the right data structure\\nfor our purpose in a Python program.\\nii. Standard Library: Wherever possible, we should use\\nmethods from standard library. Methods implemented in\\nstandard library have much better performance than user\\nimplementation.\\niii. Abstraction: At times, a lot of abstraction and\\nindirection can cause slow performance of a program.\\nWe should remove the redundant abstraction in code.\\niv. Algorithm: Use of right algorithm can make a big\\ndifference in a program. We have to find and select the\\nsuitable algorithm to solve our problem with high\\nperformance.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 6, 'page_label': '7', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='2. What are the benefits of using Python?\\nPython is strong that even Google uses it. Some of the benefits of using\\nPython are as follows:\\ni. Efficient: Python is very efficient in memory\\nmanagement. For a large data set like Big Data, it is much\\neasier to program in Python.\\nii. Faster: Though Python code is interpreted, still Python\\nhas very fast performance.\\niii. Wide usage: Python is widely used among different\\norganizations for different projects. Due to this wide\\nusage, there are thousands of add-ons available for use\\nwith Python.\\niv. Easy to learn: Python is quite easy to learn. This is the\\nbiggest benefit of using Python. Complex tasks can be\\nvery easily implemented in Python.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 7, 'page_label': '8', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='3. How will you specify source code\\nencoding in a Python source file?\\nBy default, every source code file in Python is in UTF-8 encoding. But we\\ncan also specify our own encoding for source files. This can be done by\\nadding following line after #! line in the source file.\\n# -*- coding: encoding -*-\\n\\xa0\\nIn the above line we can replace encoding with the encoding that we want\\nto use.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 8, 'page_label': '9', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='4. What is the use of PEP 8 in Python?\\n\\xa0\\nPEP 8 is a style guide for Python code. This document provides the coding\\nconventions for writing code in Python. Coding conventions are about\\nindentation, formatting, tabs, maximum line length, imports organization,\\nline spacing etc. We use PEP 8 to bring consistency in our code. We\\nconsistency it is easier for other developers to read the code.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 9, 'page_label': '10', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='5. What is Pickling in Python?\\n\\xa0\\nPickling is a process by which a Python object hierarchy can be converted\\ninto a byte stream. The reverse operation of Pickling is Unpickling.\\nPython has a module named pickle. This module has the implementation of\\na powerful algorithm for serialization and de-serialization of Python object\\nstructure.\\nSome people also call Pickling as Serialization or Marshalling.\\nWith Serialization we can transfer Python objects over the network. It is\\nalso used in persisting the state of a Python object. We can write it to a file\\nor a database.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 10, 'page_label': '11', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='6. How does memory management work in\\nPython?\\n\\xa0\\nThere is a private heap space in Python that contains all the Python objects\\nand data structures. In CPython there is a memory manager responsible for\\nmanaging the heap space.\\nThere are different components in Python memory manager that handle\\nsegmentation, sharing, caching, memory pre-allocation etc.\\nPython memory manager also takes care of garbage collection by using\\nReference counting algorithm.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 11, 'page_label': '12', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='7. How will you perform Static Analysis on\\na Python Script?\\n\\xa0\\nWe can use Static Analysis tool called PyChecker for this purpose.\\nPyChecker can detect errors in Python code.\\nPyChecker also gives warnings for any style issues.\\n\\xa0\\nSome other tools to find bugs in Python code are pylint and pyflakes.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 12, 'page_label': '13', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='8. What is the difference between a Tuple\\nand List in Python?\\n\\xa0\\nIn Python, Tuple and List are built-in data structures.\\nSome of the differences between Tuple and List are as follows:\\nI. Syntax: A Tuple is enclosed in parentheses:\\nE.g. myTuple = (10, 20, ‚Äúapple‚Äù);\\nA List is enclosed in brackets:\\nE.g. myList = [10, 20, 30];\\n\\xa0\\nII. Mutable: Tuple is an immutable data structure. Whereas, a List is\\na mutable data structure.\\n\\xa0\\nIII. Size: A Tuple takes much lesser space than a List in Python.\\n\\xa0\\nIV. Performance: Tuple is faster than a List in Python. So it gives us\\ngood performance.\\n\\xa0\\nV. Use case: Since Tuple is immutable, we can use it in cases like\\nDictionary creation. Whereas, a List is preferred in the use case\\nwhere data can alter.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 13, 'page_label': '14', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='9. What is a Python Decorator?\\n\\xa0\\nA Python Decorator is a mechanism to wrap a Python function and modify\\nits behavior by adding more functionality to it. We can use @ symbol to call\\na Python Decorator function.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 14, 'page_label': '15', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='10. How are arguments passed in a Python\\nmethod? By value or by reference?\\n\\xa0\\nEvery argument in a Python method is an Object. All the variables in\\nPython have reference to an Object. Therefore arguments in Python method\\nare passed by Reference.\\nSince some of the objects passed as reference are mutable, we can change\\nthose objects in a method. But for an Immutable object like String, any\\nchange done within a method is not reflected outside.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 15, 'page_label': '16', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='11. What is the difference between List and\\nDictionary data types in Python?\\n\\xa0\\nMain differences between List and Dictionary data types in Python are as\\nfollows:\\n\\xa0\\nI. Syntax: In a List we store objects in a sequence. In a Dictionary\\nwe store objects in key-value pairs.\\n\\xa0\\nII. Reference: In List we access objects by index number. It starts\\nfrom 0 index. In a Dictionary we access objects by key specified at\\nthe time of Dictionary creation.\\n\\xa0\\nIII. Ordering: In a List objects are stored in an ordered sequence. In a\\nDictionary objects are not stored in an ordered sequence.\\n\\xa0\\nIV. Hashing: In a Dictionary, keys have to be hashable. In a List there\\nis no need for hashing.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 16, 'page_label': '17', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='12. What are the different built-in data\\ntypes available in Python?\\n\\xa0\\nSome of the built-in data types available in Python are as follows:\\nNumeric types: These are the data types used to represent numbers in\\nPython.\\nint: It is used for Integers\\nlong: It is used for very large integers of non-limited length.\\nfloat: It is used for decimal numbers.\\ncomplex: This one is for representing complex numbers\\nSequence types: These data types are used to represent sequence of\\ncharacters or objects.\\nstr: This is similar to String in Java. It can represent a sequence of\\ncharacters.\\nbytes: This is a sequence of integers in the range of 0-255.\\nbyte array: like bytes, but mutable (see below); only available in Python 3.x\\nlist: This is a sequence of objects.\\ntuple: This is a sequence of immutable objects.\\nSets: These are unordered collections.\\nset: This is a collection of unique objects.\\n\\xa0\\nfrozen set: This is a collection of unique immutable objects.\\nMappings: This is similar to a Map in Java.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 17, 'page_label': '18', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='dict: This is also called hashmap. It has key value pair to store information\\nby using hashing.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 18, 'page_label': '19', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='13. What is a Namespace in Python?\\n\\xa0\\nA Namespace in Python is a mapping between a name and an object. It is\\ncurrently implemented as Python dictionary.\\nE.g. the set of built-in exception names, the set of built-in names, local\\nnames in a function\\nAt different moments in Python, different Namespaces are created. Each\\nNamespace in Python can have a different lifetime.\\nFor the list of built-in names, Namespace is created when Python interpreter\\nstarts.\\nWhen Python interpreter reads the definition of a module, it creates global\\nnamespace for that module.\\nWhen Python interpreter calls a function, it creates local namespace for that\\nfunction.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 19, 'page_label': '20', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='14. How will you concatenate multiple\\nstrings together in Python?\\n\\xa0\\nWe can use following ways to concatenate multiple string together in\\nPython:\\nI. use + operator:\\nE.g.\\n>>> fname=\"John\"\\n>>> lname=\"Ray\"\\n>>> print fname+lname\\nJohnRay\\nII. use join function:\\nE.g.\\n>>> \\'\\'.join([\\'John\\',\\'Ray\\'])\\n\\'JohnRay\\''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 20, 'page_label': '21', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='15. What is the use of Pass statement in\\nPython?\\n\\xa0\\nThe use of Pass statement is to do nothing. It is just a placeholder for a\\nstatement that is required for syntax purpose. It does not execute any code\\nor command.\\nSome of the use cases for pass statement are as follows:\\nI. Syntax purpose:\\n>>> while True:\\n... pass # Wait till user input is received\\nII. Minimal Class: It can be used for creating minimal classes:\\n>>> class MyMinimalClass:\\n... pass\\nIII. Place-holder for TODO work:\\nWe can also use it as a placeholder for TODO work on a function or code\\nthat needs to be implemented at a later point of time.\\n>>> def initialization():\\n... pass # TODO'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 21, 'page_label': '22', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='16. What is the use of Slicing in Python?\\n\\xa0\\nWe can use Slicing in Python to get a substring from a String.\\nThe syntax of Slicing is very convenient to use.\\nE.g. In following example we are getting a substring out of the name John.\\n>>> name=\"John\"\\n>>> name[1:3]\\n\\'oh\\'\\nIn Slicing we can give two indices in the String to create a Substring. If we\\ndo not give first index, then it defaults to 0.\\nE.g.\\n>>> name=\"John\"\\n>>> name[:2]\\n\\'Jo\\'\\nIf we do not give second index, then it defaults to the size of the String.\\n>>> name=\"John\"\\n>>> name[3:]\\n\\'n\\''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 22, 'page_label': '23', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content=\"17. What is the difference between\\nDocstring in Python and Javadoc in Java?\\n\\xa0\\nA Docstring in Python is a string used for adding comments or\\nsummarizing a piece of code in Python.\\nThe main difference between Javadoc and Docstring is that docstring is\\navailable during runtime as well. Whereas, Javadoc is removed from the\\nBytecode and it is not present in .class file.\\nWe can even use Docstring comments at run time as an interactive help\\nmanual.\\nIn Python, we have to specify docstring as the first statement of a code\\nobject, just after the def or class statement.\\nThe docstring for a code object can be accessed from the '__doc__' attribute\\nof that object.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 23, 'page_label': '24', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='18. How do you perform unit testing for\\nPython code?\\n\\xa0\\nWe can use the unit testing modules unittest or unittest2 to create and run\\nunit tests for Python code.\\nWe can even do automation of tests with these modules. Some of the main\\ncomponents of unittest are as follows:\\nI. Test fixture: We use test fixture to create preparation methods\\nrequired to run a test. It can even perform post-test cleanup.\\n\\xa0\\nII. Test case: This is main unit test that we run on a piece of code. We\\ncan use Testcase base class to create new test cases.\\n\\xa0\\nIII. Test suite: We can aggregate our unit test cases in a Test suite.\\n\\xa0\\nIV. Test runner: We use test runner to execute unit tests and produce\\nreports of the test run.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 24, 'page_label': '25', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='19. What is the difference between an\\nIterator and Iterable in Python?\\n\\xa0\\nAn Iterable is an object that can be iterated by an Iterator.\\nIn Python, Iterator object provides _iter_() and next() methods.\\nIn Python, an Iterable object has _iter_ function that returns an Iterator\\nobject.\\n\\xa0\\nWhen we work on a map or a for loop in Python, we can use next() method\\nto get an Iterable item from the Iterator.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 25, 'page_label': '26', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='20. What is the use of Generator in\\nPython?\\n\\xa0\\nWe can use Generator to create Iterators in Python. A Generator is written\\nlike a regular function. It can make use yield statement to return data during\\nthe function call. In this way we can write complex logic that works as an\\nIterator.\\nA Generator is more compact than an Iterator due to the fact that _iter_()\\nand next() functions are automatically created in a Generator.\\nAlso within a Generator code, local variables and execution state are saved\\nbetween multiple calls. Therefore, there is no need to add extra variables\\nlike self.index etc to keep track of iteration.\\nGenerator also increases the readability of the code written in Python. It is a\\nvery simple implementation of an Iterator.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 26, 'page_label': '27', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='21. What is the significance of functions\\nthat start and end with _ symbol in\\nPython?\\n\\xa0\\nPython provides many built-in functions that are surrounded by _ symbol at\\nthe start and end of the function name. As per Python documentation,\\ndouble _ symbol is used for reserved names of functions.\\nThese are also known as System-defined names.\\nSome of the important functions are:\\nObject._new_\\nObject._init_\\nObject._del_'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 27, 'page_label': '28', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='22. What is the difference between xrange\\nand range in Python?\\n\\xa0\\nIn Python, we use range(0,10) to create a list in memory for 10 numbers.\\nPython provides another function xrange() that is similar to range() but\\nxrange() returns a sequence object instead of list object. In xrange() all the\\nvalues are not stored simultaneously in memory. It is a lazy loading based\\nfunction.\\nBut as per Python documentation, the benefit of xrange() over range() is\\nvery minimal in regular scenarios.\\nAs of version 3.1, xrange is deprecated.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 28, 'page_label': '29', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='23. What is lambda expression in Python?\\n\\xa0\\nA lambda expression in Python is used for creating an anonymous function.\\nWherever we need a function, we can also use a lambda expression.\\nWe have to use lambda keyword for creating a lambda expression. Syntax\\nof lambda function is as follows:\\nlambda argumentList: expression\\nE.g. lambda a,b: a+b\\nThe above mentioned lambda expression takes two arguments and returns\\ntheir sum.\\n\\xa0\\nWe can use lambda expression to return a function.\\nA lambda expression can be used to pass a function as an argument in\\nanother function.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 29, 'page_label': '30', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='24. How will you copy an object in Python?\\n\\xa0\\nIn Python we have two options to copy an object. It is similar to cloning an\\nobject in Java.\\nI. Shallow Copy: To create a shallow copy we call copy.copy(x). In\\na shallow copy, Python creates a new compound object based on\\nthe original object. And it tries to put references from the original\\nobject into copy object.\\n\\xa0\\nII. Deep Copy: To create a deep copy, we call copy.deepcopy(x). In a\\ndeep copy, Python creates a new object and recursively creates and\\ninserts copies of the objects from original object into copy object.\\nIn a deep copy, we may face the issue of recursive loop due to\\ninfinite recursion.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 30, 'page_label': '31', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='25. What are the main benefits of using\\nPython?\\n\\xa0\\nSome of the main benefits of using Python are as follows:\\nI. Easy to learn: Python is simple language. It is easy to learn for a\\nnew programmer.\\n\\xa0\\nII. Large library: There is a large library for utilities in Python that\\ncan be used for different kinds of applications.\\n\\xa0\\nIII. Readability: Python has a variety of statements and expressions\\nthat are quite readable and very explicit in their use. It increases\\nthe readability of overall code.\\n\\xa0\\nIV. Memory management: In Python, memory management is built\\ninto the Interpreter. So a developer does not have to spend effort on\\nmanaging memory among objects.\\n\\xa0\\nV. Complex built-in Data types: Python has built-in Complex data\\ntypes like list, set, dict etc. These data types give very good\\nperformance as well as save time in coding new features.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 31, 'page_label': '32', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='26. What is a metaclass in Python?\\n\\xa0\\nA metaclass in Python is also known as class of a class. A class defines the\\nbehavior of an instance. A metaclass defines the behavior of a class.\\nOne of the most common metaclass in Python is type. We can subclass type\\nto create our own metaclass.\\nWe can use metaclass as a class-factory to create different types of classes.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 32, 'page_label': '33', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='27. What is the use of frozenset in Python?\\n\\xa0\\nA frozenset is a collection of unique values in Python. In addition to all the\\nproperties of set, a frozenset is immutable and hashable.\\nOnce we have set the values in a frozenset, we cannot change. So we cannot\\nuse and update methods from set on frozenset.\\nBeing hashable, we can use the objects in frozenset as keys in a Dictionary.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 33, 'page_label': '34', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='28. What is Python Flask?\\n\\xa0\\nPython Flask is a micro-framework based on Python to develop a web\\napplication.\\nIt is a very simple application framework that has many extensions to build\\nan enterprise level application.\\nFlask does not provide a data abstraction layer or form validation by\\ndefault. We can use external libraries on top of Flask to perform such tasks.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 34, 'page_label': '35', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='29. What is None in Python?\\n\\xa0\\nNone is a reserved keyword used in Python for null objects. It is neither a\\nnull value nor a null pointer. It is an actual object in Python. But there is\\nonly one instance of None in a Python environment.\\nWe can use None as a default argument in a function.\\nDuring comparison we have to use ‚Äúis‚Äù operator instead of ‚Äú==‚Äù for None.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 35, 'page_label': '36', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content=\"30. What is the use of zip() function in\\nPython?\\n\\xa0\\nIn Python, we have a built-in function zip() that can be used to aggregate all\\nthe Iterable objects of an Iterator.\\nWe can use it to aggregate Iterable objects from two iterators as well.\\nE.g.\\nlist_1 = ['a', 'b', 'c']\\nlist_2 = ['1', '2', '3']\\nfor a, b in zip(list_1, list_2):\\nprint a, b\\n\\xa0\\nOutput:\\na1\\nb2\\nc3\\n\\xa0\\nBy using zip() function we can divide our input data from different sources\\ninto fixed number of sets.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 36, 'page_label': '37', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='31. What is the use of // operator in\\nPython?\\n\\xa0\\nPython provides // operator to perform floor division of a number by\\nanother. The result of // operator is a whole number (without decimal part)\\nquotient that we get by dividing left number with right number.\\nIt can also be used floordiv(a,b).\\nE.g.\\n10// 4 = 2\\n-10//4 = -3'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 37, 'page_label': '38', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='32. What is a Module in Python?\\n\\xa0\\nA Module is a script written in Python with import statements, classes,\\nfunctions etc. We can use a module in another Python script by importing it\\nor by giving the complete namespace.\\nWith Modules, we can divide the functionality of our application in smaller\\nchunks that can be easily managed.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 38, 'page_label': '39', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='33. How can we create a dictionary with\\nordered set of keys in Python?\\n\\xa0\\nIn a normal dictionary in Python, there is no order maintained between\\nkeys. To solve this problem, we can use OrderDict class in Python. This\\nclass is available for use since version 2.7.\\nIt is similar to a dictionary in Python, but it maintains the insertion order of\\nkeys in the dictionary collection.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 39, 'page_label': '40', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='34. Python is an Object Oriented\\nprogramming language or a functional\\nprogramming language?\\n\\xa0\\nPython uses most of the Object Oriented programming concepts. But we\\ncan also do functional programming in Python. As per the opinion of\\nexperts, Python is a multi-paradigm programming language.\\nWe can do functional, procedural, object-oriented and imperative\\nprogramming with the help of Python.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 40, 'page_label': '41', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='35. How can we retrieve data from a\\nMySQL database in a Python script?\\n\\xa0\\nTo retrieve data from a database we have to make use of the module\\navailable for that database. For MySQL database, we import MySQLdb\\nmodule in our Python script.\\nWe have to first connect to a specific database by passing URL, username,\\npassword and the name of database.\\nOnce we establish the connection, we can open a cursor with cursor()\\nfunction. On an open cursor, we can run fetch() function to execute queries\\nand retrieve data from the database tables.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 41, 'page_label': '42', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='36. What is the difference between\\nappend() and extend() functions of a list in\\nPython?\\n\\xa0\\nIn Python, we get a built-in sequence called list. We can call standard\\nfunctions like append() and extend() on a list.\\nWe call append() method to add an item to the end of a list.\\nWe call extend() method to add another list to the end of a list.\\nIn append() we have to add items one by one. But in extend() multiple items\\nfrom another list can be added at the same time.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 42, 'page_label': '43', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='37. How will you handle an error condition\\nin Python code?\\n\\xa0\\nWe can implement exception handling to handle error conditions in Python\\ncode. If we are expecting an error condition that we cannot handle, we can\\nraise an error with appropriate message.\\nE.g.\\n>>> if student_score < 0: raise ValueError(‚ÄúScore can not be negative‚Äù)\\n\\xa0\\nIf we do not want to stop the program, we can just catch the error condition,\\nprint a message and continue with our program.\\n\\xa0\\nE.g. In following code snippet we are catching the error and continuing\\nwith the default value of age.\\n\\xa0\\n#!/usr/bin/python\\ntry:\\nage=18+\\'duration\\'\\nexcept:\\nprint(\"duration has to be a number\")\\nage=18\\nprint(age)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 43, 'page_label': '44', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content=\"38. What is the difference between split()\\nand slicing in Python?\\n\\xa0\\nBoth split() function and slicing work on a String object. By using split()\\nfunction, we can get the list of words from a String.\\nE.g. 'a b c '.split() returns [‚Äòa‚Äô, ‚Äòb‚Äô, ‚Äòc‚Äô]\\nSlicing is a way of getting substring from a String. It returns another String.\\nE.g. >>> 'a b c'[2:3] returns b\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 44, 'page_label': '45', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='39. How will you check in Python, if a class\\nis subclass of another class?\\n\\xa0\\nPython provides a useful method issubclass(a,b) to check whether class a is\\na subclass of b.\\nE.g. int is not a subclass of long\\n>>> issubclass(int,long)\\nFalse\\nbool is a subclass of int\\n>>> issubclass(bool,int)\\nTrue'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 45, 'page_label': '46', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='40. How will you debug a piece of code in\\nPython?\\n\\xa0\\nIn Python, we can use the debugger pdb for debugging the code. To start\\ndebugging we have to enter following lines on the top of a Python script.\\nimport pdb\\npdb.set_trace()\\nAfter adding these lines, our code runs in debug mode. Now we can use\\ncommands like breakpoint, step through, step into etc for debugging.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 46, 'page_label': '47', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='41. How do you profile a Python script?\\n\\xa0\\nPython provides a profiler called cProfile that can be used for profiling\\nPython code.\\nWe can call it from our code as well as from the interpreter.\\nIt gives use the number of function calls as well as the total time taken to\\nrun the script.\\nWe can even write the profile results to a file instead of standard out.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 47, 'page_label': '48', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='42. What is the difference between ‚Äòis‚Äô and\\n‚Äò==‚Äô in Python?\\n\\xa0\\nWe use ‚Äòis‚Äô to check an object against its identity.\\nWe use ‚Äò==‚Äô to check equality of two objects.\\n\\xa0\\nE.g.\\n>>> lst = [10,20, 20]\\n>>> lst == lst[:]\\nTrue\\n>>> lst is lst[:]\\nFalse'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 48, 'page_label': '49', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='43. How will you share variables across\\nmodules in Python?\\n\\xa0\\nWe can create a common module with variables that we want to share.\\nThis common module can be imported in all the modules in which we want\\nto share the variables.\\nIn this way, all the shared variables will be in one module and available for\\nsharing with any new module as well.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 49, 'page_label': '50', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='44. How can we do Functional\\nprogramming in Python?\\n\\xa0\\nIn Functional Programming, we decompose a program into functions. These\\nfunctions take input and after processing give an output. The function does\\nnot maintain any state.\\nPython provides built-in functions that can be used for Functional\\nprogramming. Some of these functions are:\\nI. Map()\\nII. reduce()\\nIII. filter()\\nEvent iterators and generators can be used for Functional programming in\\nPython.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 50, 'page_label': '51', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content=\"45. What is the improvement in\\nenumerate() function of Python?\\n\\xa0\\nIn Python, enumerate() function is an improvement over regular iteration.\\nThe enumerate() function returns an iterator that gives (0, item[0]).\\nE.g.\\n>>> thelist=['a','b']\\n>>> for i,j in enumerate(thelist):\\n... print i,j\\n...\\n0 a\\n1 b\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 51, 'page_label': '52', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='46. How will you execute a Python script in\\nUnix?\\n\\xa0\\nTo execute a Python script in Unix, we need to have Python executor in\\nUnix environment.\\nIn addition to that we have to add following line as the first line in a Python\\nscript file.\\n#!/usr/local/bin/python\\n\\xa0\\nThis will tell Unix to use Python interpreter to execute the script.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 52, 'page_label': '53', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='47. What are the popular Python libraries\\nused in Data analysis?\\n\\xa0\\nSome of the popular libraries of Python used for Data analysis are:\\nI. Pandas: Powerful Python Data Analysis Toolkit\\nII. SciKit: This is a machine learning library in Python.\\nIII. Seaborn: This is a statistical data visualization library in Python.\\nIV. SciPy: This is an open source system for science, mathematics and\\nengineering implemented in Python.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 53, 'page_label': '54', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content=\"48. What is the output of following code in\\nPython?\\n\\xa0\\n>>> thelist=['a','b']\\n>>> print thelist[3:]\\nAns: The output of this code is following:\\n[]\\nEven though the list has only 2 elements, the call to thelist with index 3\\ndoes not give any index error.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 54, 'page_label': '55', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='49. What is the output of following code in\\nPython?\\n\\xa0\\n>>>name=‚ÄôJohn Smith‚Äô\\n>>>print name[:5] + name[5:]\\nAns: Output of this will be\\nJohn Smith\\nThis is an example of Slicing. Since we are slicing at the same index, the\\nfirst name[:5] gives the substring name upto 5th location excluding 5th\\nlocation. The name[5:] gives the rest of the substring of name from the 5th\\nlocation. So we get the full name as output.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf', 'total_pages': 56, 'page': 55, 'page_label': '56', 'source_file': 'Python Interview Questions.pdf', 'file_type': 'pdf'}, page_content='50. If you have data with name of\\ncustomers and their location, which data\\ntype will you use to store it in Python?\\n\\xa0\\nIn Python, we can use dict data type to store key value pairs. In this\\nexample, customer name can be the key and their location can be the value\\nin a dict data type.\\nDictionary is an efficient way to store data that can be looked up based on a\\nkey.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 0, 'page_label': '1', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content=\"100 Python Interview Questions & \\nAnswers for Data Analyst and \\nData Science\\n1. Question: Reverse a string\\n   Answer:\\n   s = 'hello'\\n   print(s[::-1])\\n2. Question: Check if a number is prime\\n   Answer:\\n   n = 7\\n   print(all(n % i != 0 for i in range(2, \\nint(n**0.5) + 1)))\\n3. Question: Find factorial using recursion\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 1, 'page_label': '2', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content=\"Answer:\\n   def fact(n):\\n    return 1 if n == 0 else n * fact(n-1)\\n   print(fact(5))\\n4. Question: Check palindrome string\\n   Answer:\\n   s = 'madam'\\n   print(s == s[::-1])\\n5. Question: Find largest number in a list\\n   Answer:\\n   nums = [1, 5, 2]\\n   print(max(nums))\\n6. Question: Swap two variables\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 2, 'page_label': '3', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content=\"Answer:\\n   a, b = 5, 10\\n   a, b = b, a\\n   print(a, b)\\n7. Question: Count vowels in a string\\n   Answer:\\n   s = 'hello'\\n   print(sum(1 for ch in s if ch in 'aeiou'))\\n8. Question: Check Armstrong number\\n   Answer:\\n   n = 153\\n   print(sum(int(d) ** 3 for d in str(n)) == n)\\n9. Question: Generate Fibonacci series\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 3, 'page_label': '4', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content=\"Answer:\\n   a, b = 0, 1\\n   for _ in range(5):\\n    print(a)\\n    a, b = b, a + b\\n10. Question: Find GCD of two numbers\\n    Answer:\\n    import math\\n    print(math.gcd(12, 15))\\n11. Question: Check anagram\\n    Answer:\\n    s1, s2 = 'listen', 'silent'\\n    print(sorted(s1) == sorted(s2))\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 4, 'page_label': '5', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content=\"12. Question: Reverse words in a sentence\\n    Answer:\\n    s = 'hello world'\\n    print(' '.join(s.split()[::-1]))\\n13. Question: Find duplicates in a list\\n    Answer:\\n    nums = [1, 2, 2, 3]\\n    print([x for x in set(nums) if nums.count(x) \\n> 1])\\n14. Question: Find second largest number in \\na list\\n    Answer:\\n    nums = [1, 2, 3, 4]\\n    print(sorted(set(nums))[-2])\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 5, 'page_label': '6', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content=\"15. Question: Check if string contains only \\ndigits\\n    Answer:\\n    print('123'.isdigit())\\n16. Question: Flatten nested list\\n    Answer:\\n    nested = [[1, 2], [3, 4]]\\n    Ô¨Çat = [x for sub in nested for x in sub]\\n    print(Ô¨Çat)\\n17. Question: Sort dictionary by values\\n    Answer:\\n    d = {'a': 2, 'b': 1}\\n    print(dict(sorted(d.items(), key=lambda x: \\nx[1])))\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 6, 'page_label': '7', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content=\"18. Question: Count words in a string\\n    Answer:\\n    s = 'this is test'\\n    print(len(s.split()))\\n19. Question: Check leap year\\n    Answer:\\n    y = 2024\\n    print(y % 4 == 0 and (y % 100 != 0 or y % \\n400 == 0))\\n20. Question: Merge two dictionaries\\n    Answer:\\n    d1 = {'a': 1}\\n    d2 = {'b': 2}\\n    d1.update(d2)\\n    print(d1)\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 7, 'page_label': '8', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='21. Question: Group words by Ô¨Årst letter\\n    Answer:\\n    from collections import defaultdict\\n    words = \\n[\"apple\",\"ant\",\"banana\",\"ball\",\"cat\",\"car\"]\\n    grouped = defaultdict(list)\\n    for word in words:\\n     grouped[word[0]].append(word)\\n    print(dict(grouped))\\n22. Question: Find all pairs in list whose sum \\nequals target\\n    Answer:\\n    nums = [1,2,3,4,5,6,7]\\n    target = 8'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 8, 'page_label': '9', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='pairs = [(a,b) for i,a in enumerate(nums) for \\nb in nums[i+1:] if a+b==target]\\n    print(pairs)\\n23. Question: Convert list of tuples into \\ndictionary\\n    Answer:\\n    pairs = [(\"a\",1),(\"b\",2),(\"c\",3)]\\n    print(dict(pairs))\\n24. Question: Find top 3 most frequent \\nelements in a list\\n    Answer:\\n    from collections import Counter\\n    nums = [1,2,2,3,3,3,4,4,4,4,5]\\n    print(Counter(nums).most_common(3))'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 9, 'page_label': '10', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='25. Question: Transpose a matrix\\n    Answer:\\n    matrix = [[1,2,3],[4,5,6],[7,8,9]]\\n    transpose = [[row[i] for row in matrix] for i \\nin range(len(matrix[0]))]\\n    print(transpose)\\n26. Question: Implement stack using Python \\nlist\\n    Answer:\\n    stack = []\\n    stack.append(10)\\n    stack.append(20)\\n    print(stack.pop())\\n27. Question: Implement queue using \\ncollections.deque'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 10, 'page_label': '11', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='Answer:\\n    from collections import deque\\n    queue = deque()\\n    queue.append(10)\\n    queue.append(20)\\n    print(queue.popleft())\\n28. Question: Generate random numbers \\nwithout random module\\n    Answer:\\n    import time\\n    def pseudo_random(seed=1):\\n     seed = (seed*9301+49297) % 233280\\n     return seed/233280.0\\n    print(pseudo_random(int(time.time())))\\n29. Question: Convert string to title case'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 11, 'page_label': '12', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='Answer:\\n    text = \"data science with python\"\\n    print(text.title())\\n30. Question: Remove punctuation from \\nstring\\n    Answer:\\n    import string\\n    text = \"Hello, World! Data@Science.\"\\n    clean = \"\".join(c for c in text if c not in \\nstring.punctuation)\\n    print(clean)\\n31. Question: Load CSV in pandas and \\ndisplay Ô¨Årst 10 rows\\n    Answer:\\n    import pandas as pd'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 12, 'page_label': '13', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='df = pd.read_csv(\"data.csv\")\\n    print(df.head(10))\\n32. Question: Select rows where column \\nvalue > 100\\n    Answer:\\n    print(df[df[\"Sales\"] > 100])\\n33. Question: Drop rows with missing values\\n    Answer:\\n    print(df.dropna())\\n34. Question: Fill missing values with column \\nmean\\n    Answer:\\n    df[\"Sales\"].Ô¨Ållna(df[\"Sales\"].mean(), \\ninplace=True)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 13, 'page_label': '14', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='35. Question: Merge two DataFrames on a \\ncolumn\\n    Answer:\\n    df1 = pd.DataFrame({\"ID\":[1,2],\"Name\":\\n[\"A\",\"B\"]})\\n    df2 = pd.DataFrame({\"ID\":[1,2],\"Age\":\\n[25,30]})\\n    merged = pd.merge(df1,df2,on=\"ID\")\\n    print(merged)\\n36. Question: Group data by column and \\ncalculate mean\\n    Answer:\\n    print(df.groupby(\"Category\")\\n[\"Sales\"].mean())'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 14, 'page_label': '15', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='37. Question: Sort DataFrame by multiple \\ncolumns\\n    Answer:\\n    print(df.sort_values(by=\\n[\"Category\",\"Sales\"], ascending=[True,False]))\\n38. Question: Apply custom function to \\ncolumn\\n    Answer:\\n    df[\"Discounted\"] = \\ndf[\"Sales\"].apply(lambda x: x*0.9)\\n39. Question: Find number of unique values \\nin a column\\n    Answer:\\n    print(df[\"CustomerID\"].nunique())'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 15, 'page_label': '16', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='40. Question: Rename multiple columns in \\nDataFrame\\n    Answer:\\n    df.rename(columns=\\n{\"Sales\":\"Total_Sales\",\"Date\":\"Order_Date\"}, \\ninplace=True)\\n41. Question: Create a NumPy array of zeros\\n    Answer:\\n    import numpy as np\\n    arr = np.zeros((3,3))\\n    print(arr)\\n42. Question: Create a NumPy array from 1 \\nto 10\\n    Answer:\\n    arr = np.arange(1,11)\\n    print(arr)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 16, 'page_label': '17', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='43. Question: Reshape 1D NumPy array to 2D\\n    Answer:\\n    arr = np.arange(1,7).reshape(2,3)\\n    print(arr)\\n44. Question: Find max and min in NumPy \\narray\\n    Answer:\\n    arr = np.array([3,7,1,9,2])\\n    print(arr.max(), arr.min())\\n45. Question: Compute mean, median, std of \\narray\\n    Answer:\\n    arr = np.array([1,2,3,4,5,6])\\n    print(arr.mean(), np.median(arr), arr.std())'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 17, 'page_label': '18', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='46. Question: Slice Ô¨Årst 3 elements of array\\n    Answer:\\n    arr = np.array([10,20,30,40,50])\\n    print(arr[:3])\\n47. Question: Find index of max in NumPy \\narray\\n    Answer:\\n    arr = np.array([4,8,2,9,6])\\n    print(arr.argmax())\\n48. Question: Create diagonal matrix in \\nNumPy\\n    Answer:\\n    arr = np.diag([1,2,3])\\n    print(arr)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 18, 'page_label': '19', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='49. Question: Multiply two NumPy arrays \\nelement-wise\\n    Answer:\\n    a = np.array([1,2,3])\\n    b = np.array([4,5,6])\\n    print(a*b)\\n50. Question: Perform matrix multiplication\\n    Answer:\\n    a = np.array([[1,2],[3,4]])\\n    b = np.array([[5,6],[7,8]])\\n    print(np.dot(a,b))\\n51. Question: Create random NumPy array of \\nshape 2x3\\n    Answer:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 19, 'page_label': '20', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='arr = np.random.rand(2,3)\\n    print(arr)\\n52. Question: Normalize a NumPy array\\n    Answer:\\n    arr = np.array([10,20,30])\\n    norm = (arr-arr.min())/(arr.max()-arr.min())\\n    print(norm)\\n53. Question: Get unique values from NumPy \\narray\\n    Answer:\\n    arr = np.array([1,2,2,3,3,3,4])\\n    print(np.unique(arr))\\n54. Question: Replace negative values with \\nzero'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 20, 'page_label': '21', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='Answer:\\n    arr = np.array([-2,5,-7,8])\\n    arr[arr < 0] = 0\\n    print(arr)\\n55. Question: Check for NaN in NumPy array\\n    Answer:\\n    arr = np.array([1,np.nan,3])\\n    print(np.isnan(arr))\\n56. Question: Convert NumPy array to \\nPython list\\n    Answer:\\n    arr = np.array([1,2,3])\\n    print(arr.tolist())'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 21, 'page_label': '22', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='57. Question: Filter rows in pandas with \\nmultiple conditions\\n    Answer:\\n    df = pd.DataFrame({\"A\":[1,2,3,4],\"B\":\\n[10,20,30,40]})\\n    print(df[(df[\"A\"] > 2) & (df[\"B\"] < 40)])\\n58. Question: Create pivot table in pandas\\n    Answer:\\n    df = pd.DataFrame({\"Category\":\\n[\"A\",\"A\",\"B\",\"B\"],\"Sales\":[100,200,300,400]})\\n    pivot = df.pivot_table(values=\"Sales\", \\nindex=\"Category\", aggfunc=\"sum\")\\n    print(pivot)\\n59. Question: Get correlation matrix\\n    Answer:\\n    print(df.corr())'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 22, 'page_label': '23', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='60. Question: Save DataFrame to CSV\\n    Answer:\\n    df.to_csv(\"output.csv\", index=False)\\n61. Question: Select speciÔ¨Åc columns\\n    Answer:\\n    df = pd.DataFrame({\"A\":[1,2,3],\"B\":\\n[4,5,6],\"C\":[7,8,9]})\\n    print(df[[\"A\",\"C\"]])\\n62. Question: Sort DataFrame by column \\ndescending\\n    Answer:\\n    print(df.sort_values(\"B\", ascending=False))\\n63. Question: Rename columns'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 23, 'page_label': '24', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='Answer:\\n    df = df.rename(columns=\\n{\"A\":\"Col1\",\"B\":\"Col2\"})\\n    print(df)\\n64. Question: Drop rows with missing values\\n    Answer:\\n    df = pd.DataFrame({\"A\":[1,None,3],\"B\":\\n[4,5,None]})\\n    print(df.dropna())\\n65. Question: Fill missing values with mean\\n    Answer:\\n    df[\"A\"].Ô¨Ållna(df[\"A\"].mean(), inplace=True)\\n    print(df)\\n66. Question: Convert column to datetime'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 24, 'page_label': '25', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='Answer:\\n    df = pd.DataFrame({\"Date\":[\"2023-01-\\n01\",\"2023-02-01\"]})\\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\\n    print(df.dtypes)\\n67. Question: Extract year and month from \\ndatetime\\n    Answer:\\n    df[\"Year\"] = df[\"Date\"].dt.year\\n    df[\"Month\"] = df[\"Date\"].dt.month\\n    print(df)\\n68. Question: Remove duplicate rows\\n    Answer:\\n    df = pd.DataFrame({\"A\":[1,2,2,3],\"B\":\\n[4,5,5,6]})'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 25, 'page_label': '26', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='print(df.drop_duplicates())\\n69. Question: Group by column and sum\\n    Answer:\\n    df = pd.DataFrame({\"Category\":\\n[\"A\",\"A\",\"B\"],\"Sales\":[100,200,300]})\\n    print(df.groupby(\"Category\")\\n[\"Sales\"].sum())\\n70. Question: Apply custom function to \\ncolumn\\n    Answer:\\n    df = pd.DataFrame({\"A\":[1,2,3]})\\n    df[\"Square\"] = df[\"A\"].apply(lambda x:x**2)\\n    print(df)\\n71. Question: Merge two DataFrames on key'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 26, 'page_label': '27', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='Answer:\\n    df1 = pd.DataFrame({\"ID\":[1,2],\"Name\":\\n[\"A\",\"B\"]})\\n    df2 = pd.DataFrame({\"ID\":[1,2],\"Age\":\\n[25,30]})\\n    print(pd.merge(df1,df2,on=\"ID\"))\\n72. Question: Concatenate two DataFrames \\nvertically\\n    Answer:\\n    df1 = pd.DataFrame({\"A\":[1,2]})\\n    df2 = pd.DataFrame({\"A\":[3,4]})\\n    print(pd.concat([df1,df2]))\\n73. Question: Find top 2 rows by column\\n    Answer:\\n    df = pd.DataFrame({\"A\":[10,40,30,20]})'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 27, 'page_label': '28', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='print(df.nlargest(2,\"A\"))\\n74. Question: Find bottom 2 rows by column\\n    Answer:\\n    print(df.nsmallest(2,\"A\"))\\n75. Question: Count missing values\\n    Answer:\\n    df = pd.DataFrame({\"A\":[1,None,3],\"B\":\\n[None,5,6]})\\n    print(df.isnull().sum())\\n76. Question: Replace speciÔ¨Åc value\\n    Answer:\\n    df = pd.DataFrame({\"A\":[1,2,2,3]})\\n    df[\"A\"].replace(2, 99, inplace=True)\\n    print(df)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 28, 'page_label': '29', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='77. Question: Convert categorical to dummy \\nvariables\\n    Answer:\\n    df = pd.DataFrame({\"Fruit\":\\n[\"Apple\",\"Banana\",\"Apple\"]})\\n    print(pd.get_dummies(df, columns=\\n[\"Fruit\"]))\\n78. Question: Calculate cumulative sum\\n    Answer:\\n    df = pd.DataFrame({\"Sales\":\\n[100,200,300]})\\n    df[\"Cumulative\"] = df[\"Sales\"].cumsum()\\n    print(df)\\n79. Question: Calculate percentage of total\\n    Answer:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 29, 'page_label': '30', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='df[\"Percentage\"] = \\ndf[\"Sales\"]/df[\"Sales\"].sum()*100\\n    print(df)\\n80. Question: Detect outliers using IQR\\n    Answer:\\n    Q1 = df[\"Sales\"].quantile(0.25)\\n    Q3 = df[\"Sales\"].quantile(0.75)\\n    IQR = Q3 - Q1\\n    outliers = df[(df[\"Sales\"] < Q1 - 1.5*IQR) | \\n(df[\"Sales\"] > Q3 + 1.5*IQR)]\\n    print(outliers)\\n81. Question: Standardize a numeric column \\nusing z-score\\n    Answer:\\n    df[\"Zscore\"] = (df[\"Sales\"] - \\ndf[\"Sales\"].mean()) / df[\"Sales\"].std()'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 30, 'page_label': '31', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='print(df)\\n82. Question: Normalize values between 0 \\nand 1\\n    Answer:\\n    df[\"Normalized\"] = (df[\"Sales\"]-\\ndf[\"Sales\"].min())/(df[\"Sales\"].max()-\\ndf[\"Sales\"].min())\\n    print(df)\\n83. Question: Split dataset into train and test\\n    Answer:\\n    from sklearn.model_selection import \\ntrain_test_split\\n    X = df[[\"Sales\"]]\\n    y = [0,1,0,1]\\n    X_train,X_test,y_train,y_test = \\ntrain_test_split(X,y,test_size=0.2,random_stat'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 31, 'page_label': '32', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='e=42)\\n    print(X_train,y_train)\\n84. Question: Train simple linear regression\\n    Answer:\\n    from sklearn.linear_model import \\nLinearRegression\\n    X = np.array([[1],[2],[3],[4]])\\n    y = np.array([2,4,6,8])\\n    model = LinearRegression()\\n    model.Ô¨Åt(X,y)\\n    print(model.coef_, model.intercept_)\\n85. Question: Make predictions\\n    Answer:\\n    pred = model.predict([[5]])\\n    print(pred)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 32, 'page_label': '33', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='86. Question: Evaluate regression using R2 \\nscore\\n    Answer:\\n    from sklearn.metrics import r2_score\\n    y_true = [2,4,6,8]\\n    y_pred = model.predict(X)\\n    print(r2_score(y_true,y_pred))\\n87. Question: Train logistic regression\\n    Answer:\\n    from sklearn.linear_model import \\nLogisticRegression\\n    X = np.array([[1],[2],[3],[4]])\\n    y = np.array([0,0,1,1])\\n    clf = LogisticRegression()\\n    clf.Ô¨Åt(X,y)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 33, 'page_label': '34', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='print(clf.predict([[2.5]]))\\n88. Question: Create confusion matrix\\n    Answer:\\n    from sklearn.metrics import \\nconfusion_matrix\\n    y_true = [0,0,1,1]\\n    y_pred = [0,1,1,1]\\n    print(confusion_matrix(y_true,y_pred))\\n89. Question: Calculate accuracy, precision, \\nrecall, f1-score\\n    Answer:\\n    from sklearn.metrics import \\nclassiÔ¨Åcation_report\\n    print(classiÔ¨Åcation_report(y_true,y_pred))'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 34, 'page_label': '35', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='90. Question: Perform k-fold cross-\\nvalidation\\n    Answer:\\n    from sklearn.model_selection import \\ncross_val_score\\n    scores = cross_val_score(clf,X,y,cv=3)\\n    print(scores.mean())\\n91. Question: Train decision tree classiÔ¨Åer\\n    Answer:\\n    from sklearn.tree import \\nDecisionTreeClassiÔ¨Åer\\n    clf = DecisionTreeClassiÔ¨Åer()\\n    clf.Ô¨Åt(X,y)\\n    print(clf.predict([[2]]))\\n92. Question: Train random forest classiÔ¨Åer'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 35, 'page_label': '36', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='Answer:\\n    from sklearn.ensemble import \\nRandomForestClassiÔ¨Åer\\n    clf = RandomForestClassiÔ¨Åer()\\n    clf.Ô¨Åt(X,y)\\n    print(clf.predict([[3]]))\\n93. Question: Train support vector machine \\nclassiÔ¨Åer\\n    Answer:\\n    from sklearn.svm import SVC\\n    clf = SVC()\\n    clf.Ô¨Åt(X,y)\\n    print(clf.predict([[2.5]]))\\n94. Question: Scale features using \\nStandardScaler'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 36, 'page_label': '37', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='Answer:\\n    from sklearn.preprocessing import \\nStandardScaler\\n    scaler = StandardScaler()\\n    X_scaled = scaler.Ô¨Åt_transform(X)\\n    print(X_scaled)\\n95. Question: Encode categorical using \\nLabelEncoder\\n    Answer:\\n    from sklearn.preprocessing import \\nLabelEncoder\\n    fruits = [\"Apple\",\"Banana\",\"Apple\",\"Orange\"]\\n    encoder = LabelEncoder()\\n    encoded = encoder.Ô¨Åt_transform(fruits)\\n    print(encoded)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 37, 'page_label': '38', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='96. Question: One-hot\\nencode categorical feature\\nAnswer:\\nfrom sklearn.preprocessing import \\nOneHotEncoder\\nencoder = OneHotEncoder(sparse=False)\\ndata = np.array(fruits).reshape(-1,1)\\nencoded = encoder.Ô¨Åt_transform(data)\\nprint(encoded)\\n97. Question: Reduce dimensions using PCA\\n    Answer:\\n    from sklearn.decomposition import PCA\\n    pca = PCA(n_components=2)\\n    X_reduced = pca.Ô¨Åt_transform(X_scaled)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 38, 'page_label': '39', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='print(X_reduced)\\n98. Question: Save trained model with joblib\\n    Answer:\\n    import joblib\\n    joblib.dump(clf,\"model.pkl\")\\n99. Question: Load trained model with joblib\\n    Answer:\\n    model = joblib.load(\"model.pkl\")\\n    print(model.predict([[2]]))\\n100. Question: Create pipeline with scaler \\nand classiÔ¨Åer\\n     Answer:\\n     from sklearn.pipeline import Pipeline'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf', 'total_pages': 40, 'page': 39, 'page_label': '40', 'source_file': 'Python Questions -3.pdf', 'file_type': 'pdf'}, page_content='pipeline = Pipeline([(\"scaler\", \\nStandardScaler()),(\"clf\", \\nLogisticRegression())])\\n     pipeline.Ô¨Åt(X,y)\\n     print(pipeline.predict([[2.5]]))\\n---'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='RAG\\nCheat\\nSheet\\nTajamul Khan\\n@Tajamulkhann'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='LangChain\\nRetrieval-Augmented Generation\\nenhances language model outputs by\\nfetching relevant, up-to-date\\ninformation from external knowledge\\nsources and combining it with\\ngenerative AI capability.\\nLangChain is a framework designed for\\nbuilding language model (LLM)-\\npowered applications with modular,\\nproduction-ready components.\\nRAG\\nInstallation\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Components\\nDocument Loaders ‚Üí  Bring in data from\\ndifferent sources.\\nText Splitters ‚Üí  Break documents into\\nsmaller, manageable chunks.\\nEmbeddings ‚Üí  Turn text into numerical\\nvectors for understanding.\\nVector Stores ‚Üí  Save and efficiently\\nsearch these vectors.\\nRetrievers ‚Üí  Find the most relevant\\nchunks for a query.\\nGenerators ‚Üí  Generate answers or\\ninsights using the retrieved information.\\nEvaluation ‚Üí  Check how accurate or\\nuseful the results are.\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Preprocessing\\nLangChain doesn‚Äôt include built-in\\npreprocessing‚Äîyou need to handle it\\nyourself.\\nUse: Documents often have noise and\\nformatting like headers, footers, or page\\nnumbers, so cleaning and standardizing\\ntext is essential.\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Document Loaders\\nImport data from multiple sources.\\nPDF\\nText\\nWebsites\\nCSV\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Text Splitters\\nBreak large documents into\\nmanageable, context-preserving\\nchunks.\\nBest Practice: Use 300 character chunks\\nwith 50 character overlap.\\nRecursive Character Text Splitter\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content=\"Embeddings\\nConvert text chunks to semantic vectors.\\nCommon Models:\\nHuggingFace ('all-mpnet-base-v2',\\n'all-MiniLM-L6-v2')\\nOpenAI ('text-embedding-ada-002')\\nHugging Face Embedding\\nOpen AI Embedding\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Vector Stores\\nStores embeddings and enables fast\\nsimilarity search (usually using cosine\\nsimilarity) to find relevant chunks.\\nPopular: \\nFAISS for fast local search\\nChroma/Pinecone for scalable\\noptions.\\nFAISS\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Retrievers\\nFinds the most relevant document\\nchunks based on user query similarity.\\nHow it works: \\nConvert query to embedding \\nSearch vector database for similar\\nchunks \\nReturn top-k most relevant results\\nKey Settings:\\nk: Number of chunks to retrieve (3-10) \\nsearch_type: \" similarity \" or \" mmr \" (for\\ndiversity) \\nscore_threshold: Min. similarity score\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='retrieval and generation‚Äîprocess user\\nquery, fetch relevant info, and generate\\nan informed response with LLM.\\nChain Types:\\nstuff: fast, for small context\\nmap-reduce: for longer docs\\nrefine: slowest, highest quality\\nGenerators\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Evaluates RAG system performance to\\nmaintain quality and highlight areas for\\nimprovement.\\nKey Metrics:\\nFaithfulness ‚Üí  Response accurately\\nreflects the source documents.\\nRelevance ‚Üí  Response directly answers\\nthe question.\\nRetrieval Quality ‚Üí  Retrieved chunks are\\nuseful and on-topic.\\nEvaluation Types:\\nAutomated: BLEU, ROUGE, BERTScore \\nHuman: User ratings, comparative analysis\\nContinuous: A/B testing, feedback loops\\nEvaluation\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Success Tips for RAG\\nChunking ‚Üí  ~300 chars + 50‚Äì100\\noverlap.\\nRetrieval ‚Üí  Try k=3‚Äì10 based on\\nquery.\\nLLM ‚Üí  Temperature = 0 for factual\\noutput.\\nCommon Issues\\nPoor retrieval ‚Üí  adjust chunk\\nsize/embeddings.\\nHallucination ‚Üí  improve context\\nquality.\\nSlow response ‚Üí  reduce chunk size\\nor k.\\nProduction Ready\\nCaching frequent embeddings.\\nMonitor performance & feedback.\\nAdd fallbacks for errors.\\nSecure API keys & data.\\n RAG Tips\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='RAG Pipeline\\nDocument Loading\\nPreprocessing\\nChunking\\nEmbeddings\\nVector DB\\nRetrieval\\nGeneration\\nEvaluation\\n@\\nT a\\nj a\\nm\\nu\\nl k\\nh\\na\\nn\\nn\\n@ T a j a m u l k h a n n\\n@Tajamul.datascientist'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'RAG Cheat Sheet.pdf', 'file_type': 'pdf'}, page_content='Found\\nHelpful ?\\nRepost\\nFollow for more!\\n@Tajamulkhann\\n@Tajamul.datascientist')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4024269",
   "metadata": {},
   "source": [
    "## embedding And vectorStoreDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15676b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b92e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833e546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac29e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
