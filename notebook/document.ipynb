{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26788714",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Documnet loader\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6ecd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample document'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(page_content=\"This is a sample document\",\n",
    "              metadata={\n",
    "                  \"source\":\"sample.txt\",\n",
    "                  \"author\":\"Abhishek Pant\",\n",
    "                  \"pages\":1,\n",
    "                  \"Date Created\":\"2026-02-18\"\n",
    "\n",
    "                  })\n",
    "doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc09ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/pdf\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982a63e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Mlops\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 1/6 [00:41<03:28, 41.61s/it]Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:48<01:24, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:48<00:34, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:49<00:14,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:50<00:05,  5.13s/it]Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:51<00:00,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "# load all the PDF files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "\n",
    "pdf_documents=dir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c903837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\pdf\\\\Abhishek_Pant_AI_ML_Resume.docx.pdf'}, page_content='ABHISHEK PANT\\n\\nPhone: 8126961106 | Email:- abhishekpant.as@gmail.com LinkedIn:-www.linkedin.com/in/abhishek-pant-2b2129236 GitHub:-https://github.com/Rusty-user365\\n\\nTechnical Skills Programming: Python (Pandas, NumPy, Seaborn, Matplotlib, Scikit-Learn Regression, Classification, Model Evaluation), SQL Machine Learning & Deep Learning: TensorFlow, Transformers (Hugging Face), Experiment Tracking,Monitoring Natural Language Processing: LLM Fineâ€‘Tuning, RAG Systems, Embeddings, LangChain, ChromaDB Data & Visualization: Data Cleaning, Feature Engineering, Exploratory Data Analysis (EDA), Matplotlib, Seaborn MLOps & Deployment: MLflow, DVC, Docker, Kubernetes, CI/CD (GitHub Actions), Model Registry Tools & Platforms: Git, Jupyter Notebook, VS Code, Aws\\n\\nProjects\\n\\nAI Interview Assistant (Project Link) Jan 2026 â€“ Feb 2026 â€¢ Developed an AI Interview Assistant using Streamlit to simulate real interview scenarios with voice-based interaction Implemented live transcription pipelines with Whisper and other ASR models to capture candidate responses in real time.Includes 3 rounds: Introduction,Technical,Coding. With post analysis with Ai. Integrated LLMs for dynamic question generation and feedback, enabling adaptive interview practice.\\n\\n\\n\\n\\n\\nDesigned dashboard visualizations for tracking strengths, weaknesses, and progress trends across multiple interviews.\\n\\nTech Used: Python,Gemma3:250M\\n\\nMLOps Pipeline â€“ Experiment Tracking & Deployment (PROJECT LINK) Jan 2026 â€“ Feb 2026\\n\\n\\n\\nImplemented an end-to-end machine learning workflow for a classification task using MLflow for experiment tracking and model registry. Demonstrated model lifecycle management.\\n\\nContainerized the application with Docker and deployed it locally for reproducible environments.\\n\\n\\n\\nIntegrated GitHub Actions CI/CD to automate testing and deployment on every commit.\\n\\nTech Used: Python, Scikit-Learn, MLflow, Docker, GitHub Actions, Git\\n\\nReddit Persona Generation Tool (LLM-based) (PROJECT LINK) June 2025 â€“ July 2025\\n\\nDesigned a system to analyze a userâ€™s Reddit posts and comments to generate a detailed persona profile.\\n\\n\\n\\nImplemented persona extraction using a local LLM via Ollama.\\n\\nExtracted attributes such as interests, profession, personality traits, and age group with source citations.\\n\\nTech Used: Python, LLM (Ollama), NLP\\n\\nFinancial LLM â€“ Data Extraction from PDFs (PROJECT LINK) Feb 2025 â€“ Aug 2025 Implemented a financial document processing pipeline using a fine-tuned Qwen family LLM to extract key financial entities from PDFs into structured JSON format. Integrated the financial LLM into a Retrieval-Augmented Generation (RAG) chatbot for domain-specific Q&A.\\n\\n\\n\\n\\n\\nEmbedded curated financial content into ChromaDB and implemented semantic search with LangChain for context retrieval.\\n\\nEnabled grounded, explainable responses by combining extracted structured data with retrieved knowledge.\\n\\nTech Used: Python, LLM (Qwen, Ollama), LangChain, ChromaDB, NLP, Jupyter Notebook\\n\\nEducation Uttaranchal University Aug 2023 â€“ June 2025 Master of Computer Applications(AI/ML) Dehradun, Uttarakhand\\n\\nCertifications\\n\\nâ€¢ Oracle Cloud Infrastructure 2025 Generative AI Professional (1Z0-1127-25) â€“ Oracle â€¢ Oracle Cloud Infrastructure 2025 Data Science Professional (1Z0-1110-25 ) â€“ Oracle â€¢ Advanced AI and Machine Learning Techniques and Capstone â€“ Microsoft â€¢ Python â€“ HackerRank and AWS â€“ Scaler â€¢ Software Engineer Intern (Skill Certification) â€“ HackerRank'),\n",
       " Document(metadata={'source': '..\\\\data\\\\pdf\\\\AI.pdf'}, page_content='ðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ1. Explain the requirement of RAG when LLMs are already powerful.\\n\\nLLMs are powerful, as they are trained on large volumes of data using sophisticated techniques. However, LLMs because of knowledge cutoff (static knowledge), struggle to answer queries related to the latest events or the data not present in their training corpus.\\n\\nRAG addresses this challenge by retrieving relevant context from external knowledge sources, which allows LLMs to provide accurate responses. This is why RAG is essential for LLM-based applications that need to be accurate. Otherwise, LLMs alone might provide you answers that are incomplete or outdated.\\n\\nQ2. Is RAG still relevant in the era of long context LLMs?\\n\\nRAG is still important even with long context LLMs. This is because long-context LLMs without RAG have three big problems: \"lost in the middle,\", high API costs, and increased latency.\\n\\nLong-context LLMs often struggle to find the most relevant information in large contexts, which hurts the quality of generated responses. Furthermore, processing lengthy sequences in each API call results in high latency and high API costs.\\n\\nRAG addresses these issues by providing the most relevant information from external knowledge sources. So, you still need RAG to get accurate and cost-efficient responses, even with long context LLMs.\\n\\nQ3. What are the fundamental challenges of RAG systems?\\n\\nRAG is powerful, but it has to deal with the following challenges:\\n\\nScalability: Searching and retrieving from large, dynamic knowledge sources quickly and efficiently requires a lot of computing power and well-optimized indexing, which can be expensive or take a long time.\\n\\nLatency - The two-step process (retrieval then generation) can cause delays, making it less suitable for real-time applications without careful optimization.\\n\\nHallucination Risk - Even with retrieval, the model might generate plausible but unsupported details if the retrieved data is ambiguous or insufficient.\\n\\n1 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nBias and Noise - Retrieved content might carry biases, errors, or irrelevant noise from the web or other sources, which can propagate into the output.\\n\\nQ4. What are eï¬€ective strategies to reduce latency in RAG systems?\\n\\nCaching, embedding quantization, selective query rewriting, and selective re-ranking are some of the ways to reduce RAG latency. Caching stores retrieved results or generated responses to avoid redundant computation. Embedding quantization to lower bit precision reduces memory and computational load, speeding up retrieval.\\n\\nSelective query rewriting enhances recall and relevance by refining queries prior to retrieval, primarily utilized for complex or ambiguous queries. Selective re-ranking is only used for complicated queries, which cuts down on unnecessary computation for simpler ones.\\n\\nQ5. Explain R, A, and G in RAG.\\n\\nRAG stands for Retrieval-Augmented Generation. The \"R\" or Retrieval, refers to the process of searching and fetching the most relevant information from external knowledge sources for the given user query.\\n\\nThe \"A\" or Augmented, involves including the retrieved relevant context in the LLM prompt having the user query and instructions so that the LLM can generate a response based on the provided context.\\n\\nFinally, the \"G\" or Generation is the phase during which the generator LLM processes the prompt having instructions, a query, and context to generate a response that is coherent, accurate, and contextually relevant.\\n\\nQ6. How does RAG help reduce hallucinations in LLM generated responses?\\n\\nWithout RAG, LLM answers user questions based on what it learned from the training corpus, which may not be up-to-date or complete. This could lead to hallucinated responses, which are answers that sound right but are wrong.\\n\\nRetrieval-Augmented Generation (RAG) helps cut down on hallucinations in LLM-generated responses by adding an external retrieval system that pulls relevant, factual information from trusted, up-to-date external knowledge sources.\\n\\n2 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nBy combining retrieval with generation, RAG ensures that answers are more accurate, contextually relevant, and less prone to fabrications or false information, significantly enhancing the reliability of the output.\\n\\nQ7. Why is re-ranking important in the RAG pipeline after initial document retrieval?\\n\\nThe top K chunks fetched by the RAG retriever may have irrelevant chunks ahead of relevant ones. Passing these results directly to the LLM hurts the quality of the answers because LLMs mostly look at the top-ranked chunks that are given as context.\\n\\nRe-ranking uses cross-encoder models to deeply measure the semantic relevance of query-chunk pairs and then brings relevant chunks ahead of irrelevant chunks. This reduces the noise and helps the generator LLM to generate more accurate and coherent answers.\\n\\nQ8. What is the purpose of character overlap during chunking in a RAG pipeline?\\n\\nIn a RAG pipeline, chunk overlap during chunking ensures contextual continuity and prevents loss of information at the boundaries of chunks. This improves the retrieval accuracy and maintains coherence in the text fed to the LLM.\\n\\nTypically, an overlap of about 10-20% of the chunk size is used to strike a balance between preserving context and computational efficiency in RAG applications.\\n\\nQ9. What role does cosine similarity play in relevant chunk retrieval within\\n\\na RAG pipeline?\\n\\nCosine similarity measures how similar the query embedding is to the embeddings of chunks in the vector database. It finds the cosine of the angle between two vectors and provides a score that shows how closely related the query is to each chunk. Higher scores mean that the chunk is more relevant.\\n\\nThis enables the RAG system to retrieve the most relevant chunks for the query, which is then used by the generator LLM to generate accurate answers.\\n\\nQ10. Can you give examples of real-world applications where RAG systems have demonstrated value?\\n\\n3 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nAI search engines are a great example of how RAG systems have changed the way people find information online. AI search engines give you accurate, relevant answers by combining information retrieval with generative AI.\\n\\nFor instance, RAG-based AI search platforms like Perplexity AI improve the user experience by fetching the most recent and relevant information from large knowledge bases and then giving it back in the format that the user wants.\\n\\nQ11. Explain the steps in the indexing process in a RAG pipeline.\\n\\nThere are four steps in the indexing process of a RAG pipeline: parsing, chunking, encoding, and storing. The parsing step deals with extracting the document content. Then, the chunking step splits the extracted content into smaller pieces called chunks.\\n\\nThe encoding step uses an embedding model to convert chunks into dense numerical vectors called embeddings. Finally, these embeddings are saved in a vector database for efficient search and retrieval.\\n\\nAll these steps in the indexing process are performed offline.\\n\\nQ12. Explain the importance of chunking in RAG.\\n\\nChunking in Retrieval-Augmented Generation (RAG) is crucial because it breaks down large texts into smaller and semantically coherent segments called chunks. Proper chunking helps to find relevant information efficiently by creating focused chunks that maintain context and avoid irrelevant noise.\\n\\nChoosing the right chunk size balances detail and context, optimizing both retrieval accuracy and computational efficiency. Ineffective chunking can lead to poor retrieval results and incoherent responses, which makes it a foundational step for successful RAG performance in real-world applications.\\n\\nQ13. How do you choose the chunk size for a RAG system?\\n\\nChoosing the chunk size for a RAG system involves balancing granularity, context completeness, and computational efficiency. Smaller chunks (e.g., 100-200 tokens) allow precise retrieval but may lack sufficient context. Larger chunks (e.g., 500-1000 tokens) provide more context at the cost of increased computational load and potential noise.\\n\\n4 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nThe optimal size depends on the use case, document structure, embedding model, and the generator (LLM) model. For example, smaller chunks are suitable for fact-based queries, and more complex queries benefit from larger ones.\\n\\nQ14. What are the potential consequences of having chunks that are too large versus chunks that are too small?\\n\\nLarge chunks often mix different topics into one chunk and reduce the chunk\\'s relevance. This can lead to coarse vector representations and less accurate retrieval. Large chunks can also add noise and confuse the model with irrelevant information that isn\\'t important, resulting in a less accurate answer.\\n\\nSmall chunk sizes in RAG systems can lead to fragmented context. This fragmentation often leads to poor retrieval quality because information that is semantically related may be split up into chunks that are not retrieved together. Furthermore, smaller chunks mean that there are more chunks in the vector database, which increases storage costs and slows down the similarity search.\\n\\nQ15. Explain the retrieval process step-by-step in a RAG pipeline.\\n\\nThe retrieval process in RAG systems starts by encoding the user query, i.e., converting it into a dense vector representation using an embedding model. This vector representation is then used to search the vector database, which has the embeddings of chunks. Based on the similarity scores, the vector database system returns the most relevant document chunks.\\n\\nQ16. What are the key considerations when choosing an LLM for a RAG system?\\n\\nfor a RAG system are The key considerations when choosing an LLM reading-comprehension ability, speed. inference context window Reading-comprehension ability reflects how effectively the model processes the retrieved context to generate accurate responses.\\n\\nsize, and\\n\\nContext window size is crucial, as longer context models enable RAG systems to effectively include more relevant chunks. However, this must be balanced against cost and latency requirements.\\n\\n5 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nAdditionally, inference speed, infrastructure compatibility, and licensing terms also play a key role in deployment decisions for real-world RAG solutions.\\n\\nQ17. How is the prompt provided to the LLM in a RAG system diï¬€erent from\\n\\na standard, non-RAG prompt?\\n\\nThe prompt provided to the LLM without a RAG setup includes only the user query and the optional instructions. Here, the LLM generates the response based on its knowledge gained during training.\\n\\nThe prompt provided to the LLM with the RAG setup includes the user query, instructions, and relevant context. Here, the LLM generates the response as per the instructions solely based on the provided relevant context.\\n\\nQ18. What are the key hyperparameters in a RAG pipeline?\\n\\nChunk size, chunk overlap, embedding dimensionality, retrieval top-k, and retrieval threshold are some of the most important hyperparameters for retrieval in RAG. Temperature and max output length are two important hyperparameters for RAG generation.\\n\\nThe chunk size determines how much text is put into a segment before embedding, influencing the context granularity retrieved. Chunk overlap repeats a set of tokens at chunk boundaries, helping preserve important context across segments. Embedding dimensionality is the vector size used to represent text, which affects retrieval precision and database efficiency.\\n\\nRetrieval top-k sets the number of most similar chunks returned, directly impacting recall and context diversity in the response. The retrieval threshold is a similarity cutoff that filters retrieved results, ensuring only relevant chunks are selected.\\n\\nTemperature controls the randomness of generated text, balancing creativity and determinism in model outputs. Max output length limits the number of tokens generated, managing the verbosity and computational cost of responses.\\n\\nQ19. What are the popular frameworks to implement a RAG system? Justify your choice of framework.\\n\\nLangChain, LlamaIndex, and Haystack are the most popular frameworks for RAG implementation. LangChain is great for custom pipelines, and LlamaIndex is great for\\n\\n6 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nefficient document indexing and retrieval. The Haystack framework provides excellent modularity for building RAG systems.\\n\\nI would recommend LangChain because of its comprehensive ecosystem, extensive documentation, active community support, and flexibility in handling various data sources and LLM integrations.\\n\\nQ20. Explain the hyperparameters.\\n\\ninï¬‚uence of LLM context window size on RAG\\n\\nThe size of the LLM context window has a big impact on RAG hyperparameters, like chunk size and the number of chunks that are retrieved. Larger context windows let you feed more retrieved chunks to the LLM, which increases the chance of including more relevant information. This could make the quality of the generated answers better.\\n\\nBut after a certain point, performance gains start to go down because of problems like \"lost in the middle\" and higher latency.\\n\\nQ21. How do you choose values for various LLM inference hyperparameters in a RAG system?\\n\\nTemperature controls randomnessâ€”lower values give more focused and deterministic responses suitable for technical or precise tasks, while higher values make output more creative and diverse.\\n\\nThe max tokens limit the length of the output, making sure that the answers are short or long enough depending on the use case, with a trade-off between completeness and latency. Optimal settings depend on the specific application context and are found through iterative experimentation.\\n\\nQ22. Compare reasoning vs. non-reasoning LLMs for RAG systems.\\n\\nReasoning LLMs such as GPT-4o1 and DeepSeek R1 are better generators in RAG systems because they have advanced \"test-time compute\" and chain-of-thought features. These unique abilities allow them to analyze the retrieved information more effectively and do multi-step reasoning to come up with better answers.\\n\\n7 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nBut non-reasoning LLMs are still cheaper and faster, which makes them a good choice for many applications. In the end, the choice between reasoning and non-reasoning models depends on the query complexity.\\n\\nQ23. What happens with a weak generator LLM in a RAG system?\\n\\nA weak generator LLM may find it difficult to understand the retrieved context, which could lead to answers that are incomplete or hallucinated. This makes the whole RAG system less useful because the final answers lack coherence and factual correctness, even though the context that was retrieved is good.\\n\\nSo, in a RAG setup, a strong generator LLM is necessary to convert retrieved knowledge into reliable, contextually relevant outputs.\\n\\nâ˜• Support the Author\\n\\nI hope you found this â€œRAG Interview Questions and Answersâ€ book highly\\n\\nuseful.\\n\\nIâ€™ve made this book freely available to help the AI and NLP community grow\\n\\nand to support learners like you. If you found it helpful and would like to\\n\\nshow your appreciation, you can buy me a coï¬€ee to keep me motivated in\\n\\ncreating more free resources like this.\\n\\nðŸ‘‰ Buy Me a Coï¬€ee\\n\\nYour small gesture goes a long way in supporting my workâ€”thank you for being part of this journey! ðŸ™\\n\\nâ€” Kalyan KS\\n\\n8 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ24. How do you handle ambiguous or vague user queries in RAG systems?\\n\\nIssues with ambiguous or vague user queries in RAG systems include retrieval of irrelevant information, incomplete answers, and increased risk of hallucination due to the lack of specificity.\\n\\nThe most common strategy to handle ambiguous or vague user queries is query rewriting. Query rewriting transforms unclear queries into precise and focused queries, thereby enhancing retrieval quality and leading to more accurate, grounded responses.\\n\\nQ25. What are the diï¬€erent query transformation techniques that enhance user queries in RAG?\\n\\nDifferent query transformation techniques in RAG include query rewriting, query expansion, query decomposition, and HyDE to enhance retrieval relevance and context precision.\\n\\nQuery Rewriting: Rewrites the initial user query to make it more specific and detailed, boosting retrieval accuracy.\\n\\nQuery Expansion using Step-Back Prompting: Generates a broader, generalized version of the query.\\n\\nQuery Decomposition: Divides complex queries into simpler sub-queries to ensure comprehensive coverage and more precise retrieval for each component question.\\n\\nHyDE (Hypothetical Document Embedding): Synthesizes a hypothetical answer to the query and uses it as a retrieval query to get more relevant document chunks.\\n\\nQ26. What are the pros and cons of query transformation techniques?\\n\\nQuery transformation techniques in RAG systems offer significant advantages, such as improved retrieval accuracy leading to more relevant and contextually accurate responses.\\n\\nHowever, their downsides include increased computational cost, added latency, and potential noise from overexpansion. Over expansion risks retrieving noisy or off-topic documents, while complex methods like query decomposition require careful handling to ensure subqueries align with the original intent.\\n\\n9 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nSome strategies may also require substantial prompt engineering and continuous optimization to match diverse query scenarios. Balancing effectiveness and efficiency is critical to avoid diminishing returns.\\n\\nQ27. Explain how the HyDE query transformation technique works.\\n\\nThe HyDE (Hypothetical Document Embedding) technique improves RAG retrieval by transforming the user query into a hypothetical answer before embedding it. Rather than directly searching with the query embedding, the HyDE technique utilizes a large language model (LLM) to create a brief, plausible document that could potentially answer the query.\\n\\nThis synthetic document is then encoded into an embedding and used for retrieval, leading to better semantic alignment with actual document chunks in the database. As a result, HyDE enhances retrieval quality, especially for vague or underspecified queries.\\n\\nQ28. Explain how the HyPE technique works in RAG.\\n\\nThe HyPE (Hypothetical Prompt Embedding) technique improves retrieval accuracy by addressing the semantic mismatch between user queries and document chunks.\\n\\nUnlike HyDE, which generates hypothetical answer documents at query time, HyPE precomputes hypothetical questions for each document chunk during the indexing phase. These questions are designed to capture the key concepts in the chunk, transforming retrieval into a \"question-to-question\" matching process, which reduces latency and improves retrieval.\\n\\nQ29. Compare HyPE and HyDE techniques in RAG.\\n\\nHyDE (Hypothetical Document Embedding) and HyPE (Hypothetical Prompt Embedding) enhance RAG by addressing the semantic gap between user queries and document chunks, but they differ in approach.\\n\\nTiming: HyPE generates hypothetical questions during indexing, while HyDE generates hypothetical answer documents at query time.\\n\\nEfficiency: HyPE reduces runtime latency by avoiding LLM calls during retrieval, unlike HyDE, which requires an LLM call per query.\\n\\n10 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nFocus: HyPE focuses on question-question matching, while HyDE focuses on answer-answer matching.\\n\\nWhile HyDE is flexible for diverse queries, HyPEâ€™s pre-indexed approach is more efficient for real-time applications.\\n\\nQ30. To minimize RAG system latency, which pre-retrieval enhancement technique will you choose?\\n\\nTo minimize RAG system latency, I would choose the HyPE (Hypothetical Prompt Embedding) technique. Unlike query transformation techniques such as query rewriting, query expansion, query decomposition, or HyDE, which require LLM calls at query time and increase latency, HyPE precomputes hypothetical questions for document chunks during the indexing phase.\\n\\nThis question-to-question matching approach reduces runtime latency by avoiding real-time LLM calls, making it more efficient for real-time applications while maintaining high retrieval accuracy. By shifting the computational effort to indexing, HyPE ensures faster and more precise document retrieval.\\n\\nQ31. What are the diï¬€erent chunk enhancement techniques in RAG?\\n\\nThe different chunk enhancement techniques in RAG are HyPE, Contextual Chunk Header, and Document Augmentation.\\n\\nHyPE (Hypothetical Prompt Embedding) precomputes hypothetical questions for each document chunk at indexing time, enabling retrieval by question-to-question matching, which improves semantic alignment and retrieval accuracy without adding query-time latency.\\n\\nContextual Chunk Header adds relevant contextual information such as document titles or section headings to each chunk before embedding, helping retrieval models understand and rank chunks better when chunk text alone is ambiguous.\\n\\nDocument Augmentation enhances chunks by including additional metadata and enhances retrieval quality.\\n\\n11 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ32. What are the pros and cons of chunk enhancement techniques in RAG?\\n\\nChunk enhancement techniques in RAG, such as HyPE, Contextual Chunk Header, and Document Augmentation, improve retrieval accuracy by enhancing semantic alignment, preserving context, and bridging query-document chunk gaps, leading to better generation performance.\\n\\nHyPE boosts relevance through precomputed question embeddings without query-time latency, Contextual Chunk Header clarifies ambiguous chunks with document or section titles, and Document Augmentation enriches chunks with additional metadata.\\n\\nHowever, these methods increase indexing complexity and storage requirements, potentially raising computational costs. Balancing enhanced retrieval quality with resource demands is a key consideration.\\n\\nQ33. Explain how the contextual chunk header technique enhances RAG\\n\\nretrieval.\\n\\nThe Contextual Chunk Header technique in RAG enhances retrieval by adding document titles, section headings, or summaries to each chunk before embedding, providing critical context that clarifies ambiguous or isolated chunk content. This additional information helps the retrieval model better understand the chunkâ€™s relevance to a query, improving semantic alignment and ranking accuracy.\\n\\nQ34. What are some common chunking methods used in RAG?\\n\\nCommon chunking methods used in RAG are fixed-size chunking, recursive chunking, semantic chunking, and agentic chunking.\\n\\nFixed-size chunking divides text into uniform segments based on a predefined token or character length, often incorporating overlap to maintain context.\\n\\nRecursive chunking iteratively splits text using natural separators like paragraphs or sentences to preserve logical boundaries. Semantic chunking groups text based on semantic similarity using embeddings, creating coherent, meaning-based chunks.\\n\\nAgentic chunking leverages AI agents to dynamically segment text into task-oriented, semantically coherent chunks, often with metadata to enhance retrieval relevance.\\n\\n12 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ35. What are the criteria to choose a speciï¬c chunking method in RAG?\\n\\nThe criteria for choosing a specific chunking method in RAG include the nature and structure of the source documents, capabilities of the embedding model, and the specific task or application needs.\\n\\nFor structured or well-formatted data, semantic or agentic chunking ensures logical boundaries and context preservation. The chunk size must balance between being large enough to capture meaningful context and small enough to fit within model constraints for efficient processing.\\n\\nTask specificity matters since complex tasks may require semantic or agentic chunking for better context and relevance, while simpler cases can use fixed-size chunking.\\n\\nUltimately, the chunking method should balance retrieval relevance, context completeness, and computational efficiency.\\n\\nQ36. Explain the pros and cons of semantic chunking.\\n\\nSemantic chunking groups text based on meaning, creating coherent chunks that enhance retrieval relevance and context preservation.\\n\\nPros: It aligns chunks with natural topic shifts, improving the quality of retrieved content for complex queries, and reduces information loss across boundaries.\\n\\nCons: It is computationally intensive, requiring embedding models. Additionally, it may struggle with highly complex or poorly structured documents where semantic boundaries are unclear.\\n\\nQ37. How does the chunking strategy diï¬€er when dealing with structured documents (like PDFs with tables and ï¬gures) versus plain text documents?\\n\\nChunking strategies for structured documents like PDFs with tables and figures differ significantly from plain text chunking due to the need to preserve complex layouts and relationships. For structured documents, the chunking strategy must respect document elements such as tables, figures, headers, and pages to maintain context and semantic meaning.\\n\\n13 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nAgentic and recursive chunking are more suitable for structured documents due to their flexibility in respecting structure and context. Fixed-size and semantic chunking are often better suited for plain text documents where semantic coherence and simplicity are prioritized.\\n\\nLLM Engineer Toolkit ðŸ¤– This repository contains a curated list of 120+ LLM libraries category wise.\\n\\nThis repository is highly useful for AI/ML Engineers.\\n\\n14 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ38. What are the possible reasons for the poor performance of a RAG retriever?\\n\\nThe possible reasons for the poor performance of a RAG retriever are an outdated or incomplete knowledge base, a weak retrieval model, low-quality embeddings, and lack of domain-specific fine-tuning.\\n\\nAn outdated or incomplete knowledge base prevents the retriever from accessing recent or relevant information, limiting answer accuracy. A weak retrieval model, such as using TF-IDF or BM25 instead of dense vector models, leads to less effective retrieval of relevant context.\\n\\nLow-quality embeddings reduce the semantic understanding between queries and document chunks, causing mismatches. Lack of domain-specific fine-tuning results in retrieval errors because the embedding model doesnâ€™t fully capture the nuances or terminology of the target domain.\\n\\nQ39. What happens with a weak retriever in Retrieval-Augmented Generation (RAG) systems?\\n\\nA weak retriever in RAG systems leads to the retrieval of irrelevant or noisy document chunks. This can significantly degrade the quality of generated answers, as the RAG generator relies heavily on the retrieved context. The presence of irrelevant or noisy document chunks in the context because of poor retrieval causes the generator model to produce answers that are inaccurate or hallucinated while still appearing fluent.\\n\\nTherefore, strong retrievers are necessary to provide the most relevant context and ensure factual and relevant outputs in RAG systems.\\n\\nQ40. What are the common retrieval approaches used in RAG systems?\\n\\nCommon retrieval approaches in RAG systems include dense retrieval, sparse retrieval, and hybrid retrieval. Dense retrieval uses embeddings to capture semantic similarity, enabling effective query matching to relevant document chunks. Sparse retrieval relies on traditional methods like TF-IDF or BM25, focusing on keyword-based matching for efficiency.\\n\\nHybrid retrieval combines dense and sparse methods to balance semantic understanding and computational speed. These approaches ensure relevant context is retrieved for generating accurate responses in RAG systems.\\n\\n15 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ41. What are some common challenges in RAG retrieval?\\n\\nCommon challenges in RAG retrieval include ineffective query understanding, scalability issues, context fragmentation, and handling multimodal data. Ineffective query understanding leads to misinterpreting user intent, resulting in irrelevant retrieved document chunks.\\n\\nScalability issues arise when large-scale data retrieval slows performance or overwhelms the system. Context fragmentation happens when retrieved chunks lack sufficient context, lowering response quality. Handling multimodal data is challenging due to complexities in integrating text, images, or other formats effectively.\\n\\nQ42. What are the key metrics for evaluating retrieval quality in RAG?\\n\\nKey metrics for evaluating retrieval quality in RAG systems are precision, recall, Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG). Precision measures the proportion of retrieved document chunks that are relevant, while recall assesses the proportion of relevant document chunks retrieved from the total available.\\n\\nMRR evaluates the ranking quality by considering the position of the first relevant document chunks, and NDCG accounts for the relevance and ranking of retrieved documents. These metrics collectively ensure the retriever effectively identifies and ranks relevant information.\\n\\nQ43. What are embeddings, and how are they utilized in RAG retrieval?\\n\\nEmbeddings are numerical vector representations of text that capture the semantic meaning and relationships of In Retrieval-Augmented Generation (RAG), embeddings are used to convert both the user query and document chunks into vectors, enabling semantic search by comparing these vectors for similarity.\\n\\nthe data\\n\\nin a high-dimensional space.\\n\\nThis process allows RAG systems to retrieve the most relevant and contextually appropriate document chunks from a knowledge base, which are then used as context to generate accurate and grounded responses. Thus, embeddings form the backbone of RAG retrieval by enabling efficient, meaning-driven retrieval beyond simple keyword matching.\\n\\n16 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ44. What are the key considerations when choosing an embedding model for a RAG system?\\n\\nWhen choosing an embedding model for a RAG system, key considerations include\\n\\n(i) the model\\'s domain relevance to ensure it accurately captures domain-specific semantics, (ii) embedding dimensionality, which balances retrieval precision against computational and storage costs, and (ii) embedding model performance on the specific dataset to ensure good retrieval quality. This is necessary, as the real-world data often differ from academic datasets. (iv) Additionally, factors such as embedding model size, API availability, latency, cost implications, and licensing should be considered to align with infrastructure constraints and use case requirements.\\n\\nChoosing the right embedding model directly impacts the effectiveness and scalability of the RAG system.\\n\\nQ45. What is a VectorDB, and how is it utilized in RAG retrieval?\\n\\nA vector database, or VectorDB for short, is a specialized database designed to store and retrieve high-dimensional vector embeddings. In RAG retrieval, VectorDB is utilized to efficiently perform semantic searches by matching the vector representation of a user query with the closest vectors stored in the database, thereby retrieving the most contextually relevant document chunks.\\n\\nVectorDBs enable scalable and fast similarity search, which is crucial for the RAG systems.\\n\\nQ46. Explain the role of ANN (Approximate Nearest Neighbor) search\\n\\nalgorithms in RAG retrieval.\\n\\nApproximate Nearest Neighbor (ANN) search algorithms play a crucial role in RAG retrieval by enabling fast and scalable search of relevant document chunks within large vector databases. Approximate Nearest Neighbor (ANN) algorithms enable fast search in RAG retrieval by quickly narrowing down the search space to a small subset of candidate vectors instead of scanning all vectors.\\n\\n17 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nThis reduces the number of comparisons needed, significantly speeding up retrieval while maintaining good enough accuracy for relevant document chunks matching. This balance of speed and precision is crucial for real-time and large-scale RAG systems.\\n\\nQ47. Explain the step-by-step working of ANN algorithms for fast search in RAG retrieval.\\n\\nANN algorithms for fast search in RAG retrieval involve four steps namely - Encoding, Indexing, Navigating, Retrieving.\\n\\n(i) Encoding: Convert document chunks and queries into vector representations. (ii) Indexing: Organize these vectors into a specialized data structure (like graphs or hash tables) for quick lookup. (iii) Navigating: Efficiently explore the index to find vectors close to the query without checking all data points. (iv) Retrieving: Return the closest approximate neighbors that provide relevant information for RAG retrieval.\\n\\nThis approach balances search speed and accuracy, enabling fast retrieval in large-scale RAG systems.\\n\\nQ48. What are the typical distance metrics used for similarity search in vector databases, and why are they chosen?\\n\\nTypical distance metrics used in vector databases for similarity search are Euclidean distance, cosine similarity, and dot product similarity. Euclidean distance measures the straight-line distance between vectors, making it intuitive for geometric closeness. Cosine similarity evaluates the angle between vectors, focusing on their direction (meaning) rather than magnitude.\\n\\nDot product similarity considers both magnitude and direction. These metrics are selected based on the data type and the underlying embedding model to ensure effective and accurate retrieval.\\n\\nQ49. Explain why cosine similarity is preferred over other distance metrics\\n\\nin RAG retrieval.\\n\\nCosine similarity is preferred in RAG retrieval because it measures the angle between vectors, focusing on their direction (meaning) rather than magnitude. This makes it\\n\\n18 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\neffective for textual data where the meaning lies more in the direction of the embedding than its length.\\n\\nUnlike Euclidean distance or dot product, cosine similarity is invariant to vector length, providing stable and interpretable similarity scores. This helps RAG systems retrieve relevant document chunks even when text lengths vary, improving accuracy and consistency in semantic search.\\n\\nQ50. Compare keyword-based retrieval and semantic retrieval in RAG systems.\\n\\nKeyword-based retrieval in RAG systems relies on the exact or partial matching of keywords in a query to fetch relevant document chunks. This offers high precision for queries with specific terms but may miss semantically related information. In contrast, semantic retrieval uses embeddings to understand the meaning behind the query and retrieves conceptually relevant content even when keywords differ.\\n\\nCombining both methods can balance precision and semantic understanding for effective retrieval in RAG systems.\\n\\nQ51. How does hybrid search work in the context of RAG retrieval?\\n\\nHybrid search in RAG systems combines keyword-based retrieval and semantic vector search to leverage the strengths of both methods. It uses a weighted formula to balance keyword relevance and semantic similarity scores. This allows precise matching on exact terms while also capturing conceptually related content.\\n\\nQ52. When do you opt for hybrid search instead of semantic search?\\n\\nHybrid search is preferred over pure semantic search when there is a need to balance exact keyword matches with semantic understanding, especially in scenarios where users require both precision and contextual relevance.\\n\\nIt is ideal for domains where queries may include specific terms, codes, or entities that must be matched exactly, while also benefiting from capturing synonyms or related concepts.\\n\\nQ53. How do you balance relevance and diversity when retrieving document chunks for RAG?\\n\\n19 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nThe retrieval step in RAG relies on cosine similarity to identify top-k relevant document chunks. However, one downside of this approach is that it can return highly similar document chunks, leading to redundancy.\\n\\nBalancing relevance and diversity is crucial in RAG retrieval to include contextually important yet diverse document chunks, preventing redundancy and capturing a broader range of perspectives. This balance helps when dealing with complex questions, as different viewpoints or unique insights can improve the answer\\'s quality while still being accurate.\\n\\nTechniques like Maximal Marginal Relevance (MMR) help to select document chunks that are both highly relevant to the query and diverse from each other, reducing redundancy.\\n\\nQ54. How do sparse embeddings diï¬€er from dense embeddings in terms of keyword matching and retrieval interpretability?\\n\\nSparse embeddings provide interpretability and excel at exact keyword matching. These embeddings represent text as high-dimensional vectors with many zeros. In this, each dimension corresponds to a specific term or feature, making retrieval results more understandable.\\n\\nIn contrast, dense embeddings are low-dimensional, continuous vectors with mostly non-zero values learned from neural networks, capturing semantic relationships and context beyond exact matches. This makes dense embeddings less interpretable but more effective for retrieving semantically related content where keywords do not exactly overlap.\\n\\nThus, sparse embeddings are favored for precise keyword-based retrieval and interpretability, while dense embeddings support richer, context-aware retrieval. Hybrid approaches leverage the strengths of both sparse and dense embeddings to enhance retrieval performance.\\n\\nQ55. How can ï¬ne-tuning embedding models improve the retrieverâ€™s\\n\\nperformance in RAG?\\n\\n20 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nGeneral embedding models in RAG systems are trained on broad and diverse datasets that capture wide-ranging language patterns. However, they often lack depth in vocabulary and context specific to domains.\\n\\nFine-tuning embedding models aligns the embedding space more closely with domain-specific language and context. This allows the embedding model to better represent domain-specific terminology and jargon, which results in more precise and relevant retrieval.\\n\\nQ56. Design a retrieval strategy for a RAG system that needs to handle\\n\\nboth structured data (knowledge graphs) and unstructured data (text documents) simultaneously.\\n\\nA retrieval strategy for a RAG system handling both structured (knowledge graphs) and unstructured data (text documents) involves a hybrid approach combining vector-based semantic search with graph-based retrieval techniques.\\n\\nThe system first indexes unstructured text document chunks using vector embeddings for semantic similarity search, while structured data from knowledge graphs is queried using graph traversal methods that leverage explicit entity relationships and schema metadata.\\n\\nThe results from both sources are then fused to ensure factual precision from structured data and contextual richness from unstructured text. This combined approach enhances completeness and reduces hallucinations in generated responses.\\n\\nQ57. Discuss the strategies to scale embeddings in RAG retrieval.\\n\\nTo scale embeddings in RAG retrieval, strategies like Matryoshka Representation Learning (MRL) and quantization are highly effective.\\n\\nMRL enables flexible embeddings by training a single model to produce nested representations of varying sizes, allowing truncation to smaller dimensions (e.g., 64 or 128) with minimal performance loss, achieving up to 14x size reduction and significant retrieval speed-ups.\\n\\nQuantization reduces memory usage by compressing embeddings into lower-bit formats like float8 or int8. Combining MRL with quantization can yield up to 8x compression, optimizing storage and retrieval efficiency while maintaining high accuracy for large-scale RAG systems.\\n\\n21 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ58. What advantages does quantization oï¬€er over dimensionality reduction for scaling embeddings?\\n\\nQuantization offers several advantages over dimensionality reduction for scaling embeddings in RAG retrieval. It compresses embeddings by reducing the precision of numerical values (e.g., from float32 to int8 or float8), achieving up to 4x storage reduction with minimal performance loss.\\n\\nUnlike dimensionality reduction, which may discard important features and degrade accuracy, quantization preserves the full dimensionality of embeddings, maintaining richer semantic information. Additionally, quantization accelerates computation on hardware optimized for lower-precision formats, improving retrieval speed.\\n\\nThis makes it particularly effective for large-scale RAG systems where storage and latency are critical, while dimensionality reduction risks compromising retrieval quality.\\n\\nQ59. Explain the pros and cons of quantized embeddings in RAG retrieval.\\n\\nQuantized embeddings in RAG systems offer significant benefits such as drastically reduced memory requirements and much faster retrieval speeds. This makes RAG retrieval more efficient and scalable when dealing with large knowledge bases.\\n\\nHowever, the trade-off is a slight drop in retrieval accuracy or relevance. Additionally, quantization effectiveness can vary depending on the embedding model.\\n\\nOverall, quantized embeddings enable cost-effective, high-speed retrieval but require managing a controlled trade-off between resource savings and accuracy.\\n\\nQ60. Compare scalar and binary quantization for embeddings in RAG retrieval.\\n\\nScalar quantization in RAG retrieval compresses embeddings by reducing the bit precision (commonly to int8), offering a moderate 4x reduction in memory usage while maintaining a good balance between retrieval accuracy and speed.\\n\\nBinary quantization, on the other hand, converts embeddings to 1-bit vectors, achieving up to 32x compression and significantly faster retrieval but at the cost of greater accuracy loss.\\n\\n22 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nOverall, scalar quantization suits use cases prioritizing accuracy with some compression, while binary quantization excels in large-scale, speed-critical scenarios where maximal memory efficiency outweighs some loss of precision.\\n\\nðŸš€ AIxFunda Newsletter (free) Join ðŸš€ AIxFunda free newsletter to get the latest updates and interesting tutorials related to Generative AI, LLMs, Agents and RAG.\\n\\nâœ¨ Weekly GenAI updates. ðŸ“„ Weekly LLM, Agents and RAG paper updates. ðŸ“ 1 fresh blog post on an interesting topic every week.\\n\\n23 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ61. How does re-ranking diï¬€er from the initial retrieval process in RAG?\\n\\nThe initial retrieval process typically uses a bi-encoder that encodes queries and documents independently and then fetches a broad set of candidates quickly.\\n\\nThe re-ranking process reorders the retrieval results by taking the query and each retrieved document chunk as a single combined input, scoring their relevance through deep interaction. This improves the final ranking quality at the cost of higher computational overhead\\n\\nThis two-stage approach balances efficiency and accuracy by separating fast, broad retrieval from slower, more exact reranking.\\n\\nQ62. Explain the pros and cons of using re-rankers in RAG.\\n\\nRe-rankers reorder search results by taking the query and each retrieved document chunk as a single combined input, scoring their relevance through deep interaction within one model pass. This helps to prioritize the most relevant information in the limited context windows in LLMs.\\n\\nHowever, re-rankers introduce increased latency and higher computational costs since they perform deep, query-chunk interaction at query time, making them less suitable for real-time or high-traffic applications.\\n\\nThe trade-off between enhanced precision and increased costs makes re-rankers ideal for specialized use cases but less suitable for applications prioritizing speed and cost-efficiency.\\n\\nQ63. What are the diï¬€erent types of re-ranker models that can be used in RAG?\\n\\nThe different types of re-ranker models used in Retrieval-Augmented Generation (RAG) are\\n\\nCross-Encoder Rerankers: These models jointly encode the query and document chunk pair to produce a highly accurate relevance score, offering nuanced understanding of relationships but with medium computational cost.\\n\\n24 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nMulti-Vector or Late Interaction Models: Such as ColBERT, they encode queries and document chunks separately but perform fine-grained interaction later, balancing efficiency and performance with lower cost.\\n\\nLarge Language Model (LLM) Rerankers: Utilize powerful LLMs to reason about query-document chunk relevance, achieving great accuracy but incurring high computational overhead.\\n\\nThese models vary in their performance and computational cost, and choice depends on the application\\'s accuracy and latency requirements.\\n\\nQ64. Compare general re-rankers and instruction-following re-rankers in RAG.\\n\\nGeneral re-rankers in RAG systems primarily focus on re-ranking retrieved document chunks just based on their semantic relevance to the user query.\\n\\nIn contrast, instruction-following re-rankers go a step further by dynamically adjusting rankings based on additional user-provided instructions such as document recency, source reliability, or metadata criteria.\\n\\nQ65. Why is the cross-encoder typically used as the re-ranker rather than\\n\\nthe bi-encoder?\\n\\nThe cross-encoder is typically used as the re-ranker rather than the bi-encoder because it processes the query and candidate document chunks together, allowing it to capture intricate contextual interactions and provide more accurate relevance scores.\\n\\nWhile bi-encoders encode queries and document chunks separately, enabling fast and scalable retrieval of broad candidate sets, they miss detailed relationships between query-document chunk pairs.\\n\\nCross-encoders, though slower and more resource-intensive, excel in precision, making them well-suited for re-ranking a small set of top candidates identified by the bi-encoder. This combined approach balances scalability with accuracy, leveraging bi-encoders for efficient candidate retrieval and cross-encoders for refined final ranking.\\n\\n25 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ66. A RAG system retrieves 20 candidate document chunks but can only\\n\\nï¬t 5 in the LLM\\'s context window. Without re-ranking, how might this\\n\\nlimitation aï¬€ect response quality, and what speciï¬c problems would a re-ranker solve?\\n\\nWhen a RAG system retrieves 20 candidate document chunks but can only fit 5 in the LLM\\'s context window, the limitation can cause the model to miss critical information from the discarded document chunks. Without re-ranking, the top 5 document chunks may not be the most relevant, leading to incomplete or less accurate answers.\\n\\nA re-ranker solves this by analyzing and scoring all retrieved document chunks based on relevance and contextual alignment with the query, ensuring the most relevant chunks are included in the limited window.\\n\\nThis filtering reduces retrieval noise, enhances coherence, and maximizes the usefulness of the input for the generative model, thereby improving the overall quality of the response.\\n\\nQ67. Describe a scenario where a BM25 retrieval might return relevant\\n\\nchunks but in poor ranking order. How would a neural re-ranker\\n\\nspeciï¬cally address this limitation?\\n\\nA typical scenario where BM25 retrieval yields relevant document chunks but in poor ranking order arises when the query uses synonyms or phrases that vary from those in the documents. This is because BM25â€™s exact keyword matching may surface all relevant items, but fail to prioritize those most contextually aligned due to its lack of semantic understanding.\\n\\nFor instance, searching for \"car maintenance\" might retrieve document chunks about \"vehicle upkeep\" and \"automobile servicing,\" but BM25 may rank less relevant document chunks higher if they have keyword overlaps rather than semantic closeness. Neural re-rankers explicitly address this by leveraging deep contextual and semantic signals, reordering the candidate set to prioritize document chunks that best match the queryâ€™s intent and meaning.\\n\\n26 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ68. If your RAG system serves both simple factual queries and complex\\n\\nanalytical questions, how would you decide when to bypass the re-ranker for eï¬ƒciency while maintaining quality?\\n\\nTo decide when to bypass the re-ranker in a RAG system, queries should be classified based on complexity. Simple factual queries like \"What is the capital of France?\" require straightforward and well-known answers. Re-ranker can be skipped for simple factual queries, as the initial retrieval is likely to yield highly relevant results.\\n\\nFor complex analytical questions, such as those requiring synthesis or reasoning across multiple chunks, the re-ranker should be used to ensure the most relevant chunks are prioritized.\\n\\nQ69. Describe the vector pre-computation and storage strategy in a\\n\\nbi-encoder + cross-encoder pipeline. Why can\\'t cross-encoders pre-compute text representations like bi-encoders can?\\n\\nThe RAG pipeline leverages bi-encoders for fast retrieval and cross-encoders for the precise reranking of top candidates.\\n\\nBi-encoders pre-compute chunk representations by encoding them into fixed-size dense vectors offline and then storing them in a vector database for efficient retrieval.\\n\\nCross-encoders, however, cannot pre-compute chunk representations because they jointly encode query-chunk pairs, capturing intricate interactions through attention mechanisms, requiring both inputs at inference time to produce a relevance score.\\n\\nQ70. Compare the noise reduction capabilities of re-rankers versus simply\\n\\nincreasing the similarity threshold in initial retrieval. When would each approach be more appropriate?\\n\\nIncreasing the similarity threshold in initial retrieval reduces noise by filtering out less similar chunks but risks missing relevant ones due to embedding limitations. Re-rankers reduce noise by prioritizing relevant chunks by deeply understanding query-chunk relevance.\\n\\nThe choice depends on the trade-off between computational cost and precision requirements. Re-rankers are preferred for high-stakes applications like legal or medical\\n\\n27 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nsearches. resource-constrained environments.\\n\\nIncreasing the similarity threshold is simpler and faster, suitable for\\n\\nQ71. What challenges do re-rankers face regarding computational\\n\\noverhead and latency?\\n\\nRe-rankers increased computational overhead and latency, as each query-chunk pair must be processed. This latency increase can hinder high-throughput environments, making re-rankers computationally expensive compared to initial vector searches and limiting scalability.\\n\\nin RAG systems\\n\\nface significant challenges related to\\n\\nQ72. In real-time applications with strict latency requirements, describe\\n\\ntwo speciï¬c optimization strategies you could implement to reduce re-ranking overhead while preserving most of the quality gains.\\n\\nTwo effective strategies to reduce re-ranking overhead while preserving quality gains in real-time RAG applications are\\n\\n1) Query classifier: Deploy a query classifier to identify complex or analytical queries, invoking the re-ranker only for these while bypassing it for simple factual queries.\\n\\n2) Model distillation: Train a smaller, faster re-ranking model to mimic the behavior of a larger, more accurate model, enabling quicker inference with minimal quality loss.\\n\\nThese approaches balance latency and quality by minimizing computational load without significantly compromising the relevance of retrieved results.\\n\\nQ73. How would you evaluate the eï¬€ectiveness of a reranker in a RAG\\n\\nsystem? Which metrics (e.g., MRR, MAP, NDCG) would you prioritize and why?\\n\\nThe effectiveness of a re-ranker in a RAG system is best evaluated using ranking metrics that capture how well it prioritizes relevant chunks. Mean Reciprocal Rank (MRR) is key when the focus is on how quickly the first relevant chunk appears, ideal for question-answering scenarios.\\n\\n28 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nMean Average Precision (MAP) is useful when multiple relevant chunks matter, measuring both precision and ranking quality across results. Normalized Discounted Cumulative Gain (NDCG) excels when relevance is graded rather than binary, rewarding the correct order of highly relevant chunks.\\n\\nLLM Survey Papers Collection\\n\\nThis repo is highly useful to stay updated with LLM research.\\n\\n29 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ74. Explain the diï¬€erence between Precision@k and Recall@k in the\\n\\ncontext of RAG. When might you prefer one over the other?\\n\\nPrecision@k focuses on accuracy of the retrieval by measuring the proportion of the top-k retrieved chunks that are relevant to the query. Recall@k focuses on completeness of the retrieval by measuring the proportion of all relevant chunks that are retrieved within the top-k results.\\n\\nYou might choose Precision@k when you want to ensure high-quality, relevant chunks to reduce noise. On the other hand, you might choose Recall@k when it is crucial to capture as many relevant chunks as possible.\\n\\nQ75. Why is MRR unsuitable when there are multiple relevant chunks per\\n\\nquery, and how does MAP address this limitation?\\n\\nMRR (Mean Reciprocal Rank) considers the rank of the first relevant chunk and disregards the ranks and presence of other relevant chunks. This limitation makes MRR more appropriate for scenarios where a single chunk sufficiently answers the query.\\n\\nIn contrast, MAP (Mean Average Precision) addresses this by averaging the precision across all relevant ranks, accounting for the presence and order of all relevant chunks. Hence, MAP is preferred over MRR for cases where multiple relevant chunks contribute to answering a query comprehensively.\\n\\nQ76. Given a retrieval result, show how to manually calculate the MAP@5\\n\\n(Mean Average Precision at 5). What does MAP reveal about the retrieval system that raw Precision does not?\\n\\nTo manually calculate MAP@5, list the top 5 retrieved items for each query and note the positions where relevant items appear; then, compute precision at each relevant position (e.g., if the first relevant item appears at rank 2, precision = 1/2) and average these values to get the Average Precision (AP) for that query. Repeat this for all queries and take the mean of their APs for MAP@5.\\n\\nMAP@5 reveals a retrieval system\\'s ability to rank relevant items higher. In contrast, raw Precision only measures the proportion of relevant items retrieved, ignoring their order. This makes MAP@5 a better indicator of how well the system prioritizes relevance at the top of the result list.\\n\\n30 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ77. If all the relevant chunks are at the very bottom, how would this aï¬€ect MRR, MAP, and NDCG metrics? Explain each.\\n\\nIf all relevant chunks are at the bottom of a ranked list for a search query, MRR (Mean Reciprocal Rank) would be low, as it measures the reciprocal of the rank of the first relevant chunk. MAP (Mean Average Precision) would also be low, as it averages precision across all relevant chunks, penalizing late appearances heavily due to increasing denominators in precision calculations.\\n\\nNDCG (Normalized Discounted Cumulative Gain) would similarly be low, as it discounts the relevance scores of chunks appearing later in the ranking, reducing the cumulative gain.\\n\\nQ78. Suppose your RAG retriever gets perfect Recall@10 but low\\n\\nPrecision@10. What problems could this cause for the downstream generator?\\n\\nPerfect Recall@10 means all relevant chunks are retrieved within the top 10 results. Low Precision@10 indicates many of those retrieved chunks are irrelevant. If a RAG retriever achieves perfect Recall@10 but low Precision@10, the downstream generator receives all the relevant information mixed with much irrelevant content.\\n\\nThis will confuse the generator model and increase the chance of generating off-topic or inaccurate responses.\\n\\nQ79. Compare and contrast â€œorder-awareâ€ and â€œorder-unawareâ€ retrieval\\n\\nmetrics in RAG, giving examples for each from the set (Precision, Recall, MRR, MAP, NDCG).\\n\\nOrder-aware retrieval metrics consider the ranking of retrieved items, emphasizing the importance of higher-ranked relevant results. For example, Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) are order-aware, as MRR evaluates the rank of the first relevant item and NDCG accounts for relevance scores and ranking positions.\\n\\nOrder-unaware metrics focus solely on whether relevant items are retrieved and ignore the order. Precision and Recall are order-unaware, measuring the proportion of\\n\\n31 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nrelevant items retrieved (Precision) and the proportion of relevant items found out of all relevant items (Recall), without considering their order.\\n\\nQ80. How would the value of NDCG@k change if all relevant chunks are retrieved but in the reverse order (least to most relevant)?\\n\\nNDCG@k rewards placing highly relevant chunks at earlier ranks and applies a logarithmic discount to relevance scores at lower positions. So reversing the order pushes the most relevant chunks further down the listâ€”making them less valuable in the NDCG calculation.\\n\\nWhile all relevant items are present, their suboptimal positions reduce the overall score since NDCG is sensitive to both the presence and order of relevant items in the top k. The value of NDCG@k will decrease compared to the ideal ranking, but will remain higher than a ranking with irrelevant chunks at the top.\\n\\nQ81. What is the signiï¬cance of Context Precision@K in evaluating a RAG\\n\\nretriever, and how does it diï¬€er from standard Precision@k in traditional information retrieval?\\n\\nThe standard Precision@k in traditional information retrieval just measures the proportion of relevant items among the top-k results and ignores the order. Unlike standard Precision@k, Context Precision@K not only checks whether relevant chunks are retrieved, but also whether they appear at higher ranks in the context.\\n\\nContext Precision@K ensures useful information is prioritized in the retrieved context which greatly impacts the quality of the generated answer.\\n\\nQ82. Why does Context Precision@K use a weighted sum approach with\\n\\nrelevance indicators, and how does this better reï¬‚ect RAG retriever performance?\\n\\nContext Precision is computed as the weighted sum of Precision@k, normalized by the number of relevant chunks. Here the weighted sum accounts for both the presence and the rank of relevant chunks in the retrieved context. By multiplying Precision@k with the relevance indicator at each position, the metric rewards cases where relevant information appears earlier, reflecting the importance of ranking quality.\\n\\n32 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nThis approach better evaluates RAG retrievers, since in generative settings, not just retrieving relevant chunks but placing them at higher ranks significantly impacts the modelâ€™s ability to produce accurate answers.\\n\\nQ83. Given a retrieval result where relevant chunks appear at positions 2,\\n\\n4, 6, and 8 out of 10 total chunks, manually calculate the Context\\n\\nPrecision@10. What does this score tell us about the retriever\\'s ranking ability?\\n\\nTo calculate Context Precision@10 with relevant chunks at positions 2, 4, 6, and 8, first assign relevance indicators v_k=1 at these ranks and 0 elsewhere. Calculate Precision@k at each relevant rank: at 2, Precision@2 = 1/2 = 0.5; at 4, Precision@4 = 2/4 = 0.5; at 6, Precision@6 = 3/6 = 0.5; at 8, Precision@8 = 4/8 = 0.5. The weighted sum is 0.5+0.5+0.5+0.5=2.\\n\\nDividing the weighted sum by the total number of relevant chunks (4) gives Context Precision@10 = 0.5. This score indicates the retriever has moderate ranking ability, retrieving relevant chunks but not consistently ranking them at the very top, thus limiting optimal prioritization of useful context.\\n\\nQ84. A RAG system achieves Context Precision@5 = 0.8. What are the possible scenarios that could lead to this score?\\n\\nA Context Precision@5 score of 0.8 in a RAG system indicates that not all of the top five retrieved chunks are relevant to the ground truth. This could occur if, for instance, four out of five chunks are relevant (v_k = 1) and one is irrelevant (v_k = 0), leading to a lower weighted sum of Precision@k when normalized by the total number of relevant chunks.\\n\\nAnother scenario might involve three relevant chunks and two irrelevant ones, with the relevant chunks ranked higher but still resulting in a score less than 1 due to the presence of irrelevant chunks.\\n\\nQ85. Explain the possible reasons for a RAG retrieval system with\\n\\nconsistently low context precision.\\n\\nContext Precision is computed as the weighted sum of Precision@k, normalized by the number of relevant chunks. So low Context Precision@k scores reflect the presence of\\n\\n33 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nhigh proportion of irrelevant chunks or poor ranking of relevant chunks within the top K results.\\n\\nThis could stem from ineffective query understanding, where the system misinterprets the userâ€™s intent, or a poorly designed retrieval algorithm that fails to prioritize chunks matching the ground truth. Additionally, a noisy or low-quality document corpus might contain few relevant chunks, causing irrelevant ones to dominate the retrieved set.\\n\\nQ86. Compare Context Recall with traditional information retrieval recall.\\n\\nWhy is Context Recall computed using \"ground truth claims\" rather than simply counting relevant documents?\\n\\nContext Recall in RAG retrieval differs from traditional information retrieval recall by focusing on the completeness of information through ground truth claims rather than merely counting relevant documents.\\n\\nWhile traditional recall counts how many relevant documents are retrieved, Context Recall decomposes the reference answer into individual claims and checks if these specific claims are found in the retrieved context.\\n\\nThis approach ensures a more fine-grained evaluation of whether all necessary pieces of information required to answer the query are present in the context or not.\\n\\nQ87. What does context precision measure in a RAG retriever, and how does it diï¬€er from context recall?\\n\\nContext Precision in a RAG retriever measures how well the system ranks relevant chunks of information higher than irrelevant ones within the retrieved context, emphasizing the prioritization of useful data. In contrast, Context Recall assesses the completeness of the retrieved context, evaluating whether all the relevant pieces of information necessary to answer the query are present.\\n\\nTogether, they provide complementary insights: context precision ensures useful information is prioritized, whereas context recall ensures that no important information is missed.\\n\\nQ88. In a RAG pipeline, how might context recall impact the completeness of generated answers? Describe a scenario illustrating this relationship.\\n\\n34 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nContext Recall in a RAG pipeline directly impacts the completeness of generated answers by measuring how well the retriever gathers all relevant pieces of information required to answer the user query.\\n\\nFor instance, if a question about a historical event requires multiple claims or facts, a low Context Recall score indicates that some key facts were missed in the retrieved context, leading to incomplete answers.\\n\\nA high Context Recall ensures the generator (LLM) has access to all necessary information to produce a complete and well-informed response.\\n\\nQ89. If your retriever achieves high context precision but low context recall, what types of user queries would likely suï¬€er most?\\n\\nIf a retriever in a RAG pipeline achieves high context precision but low context recall, user queries that require multiple distinct pieces of information or comprehensive coverage are likely to suffer most.\\n\\nQueries, like complex multi-fact questions or those needing extensive context to answer fully, will suffer because despite the retrieved chunks being relevant (high precision), many essential relevant chunks are missing overall (low recall).\\n\\nThis results in incomplete answers, as important claims or facts are absent, limiting the modelâ€™s ability to generate a thorough response.\\n\\nQ90. In what situations would you prioritize Context Precision over\\n\\nContext Recall in a RAG retriever, and how would this impact the generatorâ€™s performance?\\n\\nIn situations where precision is critical, such as in high-risk domains like healthcare, finance, or legal applications, prioritizing Context Precision over Context Recall in a RAG retriever is essential. This ensures that only the most relevant and trustworthy information is retrieved, minimizing the risk of including irrelevant or misleading content that could negatively impact the generator\\'s response.\\n\\nWhile this may limit the breadth of information (lower recall), it improves the quality and reliability of the generated answers by reducing noise.\\n\\n35 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ91. Describe a scenario where a RAG system might achieve high Context\\n\\nRecall but still produce poor answers. What complementary metrics would\\n\\nyou use alongside Context Recall to get a complete picture of retriever performance?\\n\\nA RAG system might achieve high Context Recall by retrieving most or all relevant information pieces but still produce poor answers if the retrieved context contains noisy or irrelevant data that confuses the generator.\\n\\nTo get a complete picture of retriever performance, complementary metrics like Context Precision should be used alongside Context Recall. Context Recall along with Context Precision ensures retrieved content is not only comprehensive but is also relevant and well-ranked.\\n\\nQ92. If your RAG retriever consistently shows Context Recall scores below 0.6, what are the three potential root causes?\\n\\nContext Recall scores below 0.6 mean that the retriever is missing a significant portion of the relevant information required to answer user queries.\\n\\nThe three potential root causes are (1) an incomplete or outdated knowledge base lacking necessary information, (2) ineffective embedding model or ranking algorithms causing semantically relevant chunks to be missed, and (3) poor chunking strategy leading to loss of key information.\\n\\nQ93. Why is it important for RAG systems to optimize both context precision and context recall simultaneously? What trade-oï¬€s might occur?\\n\\nIt is important for RAG systems to optimize both context precision and context recall simultaneously. This is because context precision ensures that the retrieved information is highly relevant and ranked appropriately. The Context Recall metric ensures that all necessary information is included in the retrieved context so that the generator can output a complete answer.\\n\\nThe trade-off often arises because increasing recall by retrieving more chunks may introduce irrelevant chunks, lowering precision. At the same time, focusing solely on precision might omit important information, leading to incomplete responses.\\n\\n36 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nBalancing these metrics helps create a RAG system that retrieves relevant content efficiently while covering the query comprehensively, resulting in accurate and thorough generated answers.\\n\\nQ94. Explain why Context Relevancy is considered a \"reference-free\" metric\\n\\nwhile Context Precision and Context Recall are \"reference-dependent.\"\\n\\nWhen would you prefer using Context Relevancy over the other two metrics?\\n\\nContext Relevancy is considered a \"reference-free\" metric because it evaluates how relevant the retrieved context is to the userâ€™s query without needing a reference answer. It measures the proportion of statements in the retrieved context that are relevant to the query.\\n\\nIn contrast, Context Precision and Context Recall are \"reference-dependent\" as they require a reference answer to determine relevance and completeness of retrieval.\\n\\nContext Relevancy is preferred when reference answers are unavailable. The Context Relevancy metric offers a way to assess retrieval quality based solely on the query and retrieved context itself. This is useful for real-time scenarios where ground truth may not exist.\\n\\nQ95. Describe a scenario where a RAG retriever achieves high Context\\n\\nRelevancy but low Context Precision. What does this imply about the retrieverâ€™s performance?\\n\\nA RAG retriever achieves high Context Relevancy but low Context Precision when it retrieves a context where most statements are relevant to the userâ€™s query, but the relevant chunks are ranked lower in the retrieved list, overshadowed by irrelevant ones.\\n\\nFor example, if a query about \"machine learning algorithms\" retrieves a context with many relevant statements but places them after less relevant or noisy chunks, Context Relevancy is high (most statements are query-related), but Context Precision@K is low due to poor ranking of relevant chunks.\\n\\nThis implies the retriever is effective at fetching relevant content but struggles to prioritize relevant chunks over irrelevant ones.\\n\\n37 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ96. Suppose a RAG retriever retrieves all relevant chunks but includes\\n\\nmany irrelevant ones, leading to low Context Relevancy. How would you improve the retriever to address this issue?\\n\\nA RAG retriever retrieving all relevant chunks along with many irrelevant ones results in low context relevancy scores. This can be addressed by improving the retriever by refining its filtering and ranking mechanisms. Techniques such as enhancing embedding model quality, applying stricter similarity thresholds, or integrating a re-ranking model can help prioritize highly relevant chunks and suppress noise.\\n\\nAdditionally, improving the chunking strategy to create more precise and semantically coherent chunks can reduce irrelevant retrievals. These optimizations ensure retrieved context is both comprehensive and focused on the most relevant information.\\n\\nQ97. How does the Faithfulness metric assess the quality of a RAG\\n\\ngenerator?\\n\\nThe Faithfulness metric assesses the quality of a RAG generator by measuring how factually consistent the generated response is with the retrieved context. It is computed as the ratio of claims in the response that are supported by the retrieved context to the total number of claims.\\n\\nA score of 1 indicates all claims are fully supported, reflecting high factual accuracy. A score of 0 shows no claims are supported, indicating complete factual inconsistency. This metric ensures the RAG system generates reliable and contextually grounded responses.\\n\\nQ98. Distinguish between Faithfulness and Context Precision metrics in RAG\\n\\nevaluation. Why might a system have high Context Precision but low Faithfulness, and what would this indicate about your pipeline?\\n\\nFaithfulness measures how factually consistent a RAG generatorâ€™s response is with the retrieved context. This metric is computed as the ratio of supported claims to total claims in the response. The Context Precision metric focuses on the prioritization of relevant information by evaluating how well a retriever ranks relevant chunks within the top K.\\n\\n38 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nA system might have high Context Precision but low Faithfulness if the retriever effectively ranks relevant chunks highly, but the generator introduces unsupported or contradictory claims not grounded in the context. This indicates a strong retrieval stage but a flawed generation stage, where the model fails to accurately interpret or utilize the retrieved information.\\n\\nQ99. A RAG system has high context precision but low faithfulness. How would you address this?\\n\\nA RAG system with high context precision and low faithfulness happens when the retriever is selecting relevant chunks accurately, but the generator is producing responses with unsupported claims. To address this, one should focus on improving the generatorâ€™s grounding and claim verification processes.\\n\\nUse stronger cross-checking mechanisms like natural language inference models or fact-checking modules against the retrieved context. Additionally, tuning the generation prompts to encourage reliance on the context can help increase faithfulness.\\n\\nQ100. Why might a RAG system with perfect Context Recall still fail to\\n\\nproduce accurate responses? How does the Faithfulness metric help diagnose this issue?\\n\\nContext recall evaluates the completeness of the retrieved context in a RAG pipeline. Perfect Context Recall means the retrieved context includes all the ground truth claims. A RAG system with perfect Context Recall may still fail to produce accurate responses. This happens when the generator hallucinates and includes claims unsupported by the retrieved context.\\n\\nThe Faithfulness metric helps diagnose this issue by measuring how many claims in the generated response are factually supported by the retrieved context. A low or moderate faithfulness metric score indicates a less accurate response, i.e., the response includes unsupported claims.\\n\\nQ101. Explain how hallucinations\\n\\nin LLMs speciï¬cally\\n\\nimpact the\\n\\nFaithfulness metric. What techniques could you implement to improve the Faithfulness metric score?\\n\\nThe faithfulness metric measures the proportion of claims in the response that are backed up by context. Hallucinations reduce the faithfulness metric score by\\n\\n39 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nintroducing unsupported claims in the generated response. These unsupported claims either contradict or have no basis in the provided context.\\n\\nTo improve Faithfulness scores, techniques such as incorporating natural language inference (NLI) or fact-checking models to verify claims, using prompt engineering to discourage unsupported generation, etc., can be used.\\n\\nQ102. How does Response Relevancy diï¬€er from Context Relevancy, and why do you need both metrics to properly evaluate a RAG system?\\n\\nResponse Relevancy measures how well a RAG system\\'s generated response aligns with the userâ€™s query by calculating the ratio of relevant statements in the response to the total statements. Context Relevancy evaluates the relevance of retrieved context to the query by measuring the proportion of relevant statements in the context.\\n\\nBoth metrics are essential because Context Relevancy ensures the retriever fetches relevant context, while Response Relevancy verifies that the generator produces an answer directly addressing the query.\\n\\nA RAG system could retrieve relevant context but generate an off-topic response, or vice versa. Hence, evaluating both ensures the entire RAG pipelineâ€”retrieval and generationâ€”performs effectively.\\n\\nQ103. The generatorâ€™s response mentions facts not present in the retrieved\\n\\ncontext. Describe how faithfulness and response relevancy metrics would be impacted.\\n\\nThe faithfulness metric measures the proportion of claims in the response that are backed up by context. Therefore, the score will decrease if the LLM-generated answer contains unsupported claims.\\n\\nIn the case of the Response Relevancy metric, the score will decrease only if the unsupported facts are irrelevant to the user query. Otherwise, the score will remain high.\\n\\nThis underscores a key difference between these two metrics: Faithfulness metric looks for answerâ€™s factual consistency with the context, while Response Relevancy assesses answerâ€™s relevancy with the query.\\n\\n40 Kalyan KS (Follow on Twitter and LinkedIn)\\n\\nðŸš€AIxFunda Newsletter aixfunda.substack.com\\n\\nQ104. How does the Response Relevancy metric help evaluate whether a RAG generator is addressing the userâ€™s query eï¬€ectively?\\n\\nThe Response Relevancy metric is computed as the ratio of relevant statements in the response to the total number of statements. So, this metric checks the effectiveness of the RAG generator by measuring how well the response aligns with the user query.\\n\\nA score close to 1 means the answer directly addresses the query with little to no irrelevant content. A score close to 0 means that the answer contains information that is not related to the question.\\n\\nQ105. When evaluating RAG generator output, what are the risks of relying\\n\\nsolely on response relevancy? How can including the faithfulness metric improve reliability?\\n\\nThe Response Relevancy metric tells you how relevant the answer is to the user query. But this metric doesn\\'t check if the answer is based on the retrieved context, so it misses factual errors.\\n\\nThe faithfulness metric is the number of supported claims divided by the total number of claims. Adding the faithfulness metric makes the system more reliable by making sure that the claims in the LLM-generated response are supported by the context.\\n\\nThis dual evaluation ensures the RAG system delivers both relevant and factual responses, reducing the risk of misleading outputs.\\n\\n41 Kalyan KS (Follow on Twitter and LinkedIn)'),\n",
       " Document(metadata={'source': '..\\\\data\\\\pdf\\\\attachment.pdf'}, page_content='ABHISHEK PANT\\n\\nPhone: 8126961106 | Email:- abhishekpant.as@gmail.com LinkedIn:-www.linkedin.com/in/abhishek-pant-2b2129236 GitHub:-https://github.com/Rusty-user365\\n\\nTechnical Skills Programming:Python (Pandas, NumPy, Seaborn, Matplotlib, Scikit-Learn Regression, Classification, Model Evaluation), SQL Machine Learning & Deep Learning:TensorFlow, Transformers (Hugging Face), OpenCV Natural Language Processing: LLM Fineâ€‘Tuning, RAG Systems, Embeddings, LangChain, ChromaDB Data & Visualization: Data Cleaning, Feature Engineering, Exploratory Data Analysis (EDA), Matplotlib, Seaborn MLOps & Deployment: MLflow, DVC, Docker, Kubernetes, CI/CD (GitHub Actions), Model Registry, Experiment Tracking,Monitoring Tools & Platforms: Git, Jupyter Notebook, VS Code, Aws\\n\\nProjects\\n\\nStreamlit Automation for Machine Learning Tasks (Project Link) Nov 2025 â€“ Dec 2025\\n\\nDeveloped a noâ€‘code Streamlit application to automate basic machine learning workflows. â€¢ Implemented dataset selection, preprocessing (scaling, train/test split), and model training pipelines. â€¢ Integrated multiple ML algorithms (Logistic Regression, SVM, Random Forest, XGBoost, KNN) for experimentation. â€¢ Enabled interactive visualizations, classification reports, and accuracy metrics for model evaluation. â€¢ Designed modular utility functions (ml_utility) to streamline data handling, training, and evaluation.\\n\\nTech Used: Python, Streamlit\\n\\nReddit Persona Generation Tool (LLM-based) (PROJECT LINK) June 2025 â€“ July 2025\\n\\nDesigned a system to analyze a userâ€™s Reddit posts and comments to generate a detailed persona profile. â€¢ Implemented persona extraction using a local LLM via Ollama. â€¢ Extracted attributes such as interests, profession, personality traits, and age group with source citations. â€¢ Structured outputs for explainability and traceability.\\n\\nTech Used: Python, LLM (Ollama), NLP Financial RAG Chatbot â€“ Intelligent Document Processing & Retrieval (PROJECT LINK) Feb 2025 â€“ Aug 2025\\n\\nDesigned a financial document processing pipeline using a fine-tuned Qwen family LLM to extract key financial entities from PDFs into structured JSON.\\n\\nIntegrated the financial LLM into a Retrieval-Augmented Generation (RAG) chatbot for domain-specific Q&A. â€¢ Embedded curated financial content into ChromaDB and implemented semantic search with LangChain for context retrieval.\\n\\nEnabled grounded, explainable responses by combining extracted structured data with retrieved knowledge.\\n\\nTech Used: Python, LLM (Qwen, Ollama), LangChain, ChromaDB, NLP, Jupyter Notebook MLOps Pipeline â€“ Experiment Tracking & Deployment (PROJECT LINK) Jan 2026- Feb 2026\\n\\nImplemented an end-to-end machine learning workflow for a classification task using MLflow for experiment tracking and model registry.\\n\\nAutomated hyperparameter tuning with GridSearchCV, logging each runâ€™s parameters, metrics, and artifacts. â€¢ Containerized the application with Docker and deployed it locally for reproducible environments. â€¢ Integrated GitHub Actions CI/CD to automate testing and deployment on every commit. â€¢ Demonstrated model lifecycle management by promoting models from Staging to Production in MLflow Registry.\\n\\nTech Used: Python, Scikit-Learn, MLflow, Docker, GitHub Actions, Git\\n\\nEducation Uttaranchal University Aug 2023 â€“ June 2025 Master of Computer Applications(AI/ML) Dehradun, Uttarakhand\\n\\nCertifications\\n\\nOracle Cloud Infrastructure 2025 Generative AI Professional (1Z0-1127-25) â€“ Oracle â€¢ Oracle Cloud Infrastructure 2025 Data Science Professional (1Z0-1110-25 ) â€“ Oracle â€¢ Advanced AI and Machine Learning Techniques and Capstone â€“ Microsoft â€¢ Python â€“ HackerRank â€¢ Software Engineer Intern (Skill Certification) â€“ HackerRank â€¢ AWS â€“ Scaler'),\n",
       " Document(metadata={'source': '..\\\\data\\\\pdf\\\\Python Interview Questions.pdf'}, page_content='CONTENTS\\n\\n1. How will you improve the performance of a program in Python?\\n\\n2. What are the benefits of using Python?\\n\\n3. How will you specify source code encoding in a Python source file?\\n\\n4. What is the use of PEP 8 in Python?\\n\\n5. What is Pickling in Python?\\n\\n6. How does memory management work in Python?\\n\\n7. How will you perform Static Analysis on a Python Script?\\n\\n8. What is the difference between a Tuple and List in Python?\\n\\n9. What is a Python Decorator?\\n\\n10. How are arguments passed in a Python method? By value or by reference?\\n\\n11. What is the difference between List and Dictionary data types in Python?\\n\\n12. What are the different built-in data types available in Python?\\n\\n13. What is a Namespace in Python?\\n\\n14. How will you concatenate multiple strings together in Python?\\n\\n15. What is the use of Pass statement in Python?\\n\\n16. What is the use of Slicing in Python?\\n\\n17. What is the difference between Docstring in Python and Javadoc in Java?\\n\\n18. How do you perform unit testing for Python code?\\n\\n19. What is the difference between an Iterator and Iterable in Python?\\n\\n20. What is the use of Generator in Python?\\n\\n21. What is the significance of functions that start and end with _ symbol in Python?\\n\\n22. What is the difference between xrange and range in Python?\\n\\n23. What is lambda expression in Python?\\n\\n24. How will you copy an object in Python?\\n\\n25. What are the main benefits of using Python?\\n\\n26. What is a metaclass in Python?\\n\\n27. What is the use of frozenset in Python?\\n\\n28. What is Python Flask?\\n\\n29. What is None in Python?\\n\\n30. What is the use of zip() function in Python?\\n\\n31. What is the use of // operator in Python?\\n\\n32. What is a Module in Python?\\n\\n33. How can we create a dictionary with ordered set of keys in Python?\\n\\n34. Python is an Object Oriented programming language or a functional programming language?\\n\\n35. How can we retrieve data from a MySQL database in a Python script?\\n\\n36. What is the difference between append() and extend() functions of a list in Python?\\n\\n37. How will you handle an error condition in Python code?\\n\\n38. What is the difference between split() and slicing in Python?\\n\\n39. How will you check in Python, if a class is subclass of another class?\\n\\n40. How will you debug a piece of code in Python?\\n\\n41. How do you profile a Python script?\\n\\n42. What is the difference between â€˜isâ€™ and â€˜==â€™ in Python?\\n\\n43. How will you share variables across modules in Python?\\n\\n44. How can we do Functional programming in Python?\\n\\n45. What is the improvement in enumerate() function of Python?\\n\\n46. How will you execute a Python script in Unix?\\n\\n47. What are the popular Python libraries used in Data analysis?\\n\\n48. What is the output of following code in Python?\\n\\n49. What is the output of following code in Python?\\n\\n50. If you have data with name of customers and their location, which data type will you use to store it in Python?\\n\\nACKNOWLEDGMENTS\\n\\nWe thank our readers who constantly send feedback and reviews to motivate us in creating these useful books with the latest information!\\n\\nINTRODUCTION\\n\\nThis book contains basic to expert level Python interview questions that an interviewer asks. Each question is accompanied with an answer so that you can prepare for job interview in short time.\\n\\nWe have compiled this list after attending dozens of technical interviews in top-notch companies like- Google, Facebook, Netflix, Amazon etc.\\n\\nOften, these questions and concepts are used in our daily programming work. But these are most helpful when an Interviewer is trying to test your deep knowledge of Python.\\n\\nThe difficulty rating on these Questions varies from a Fresher level software programmer to a Senior software programmer.\\n\\nOnce you go through them in the first pass, mark the questions that you could not answer by yourself. Then, in second pass go through only the difficult questions.\\n\\nAfter going through this book 2-3 times, you will be well prepared to face a technical interview on Python for an experienced programmer.\\n\\nPython Interview Questions\\n\\n1. How will you improve the performance of a program in Python? There are many ways to improve the performance of a Python program. Some of these are as follows: i.\\n\\n1. How will you improve the performance of a program in Python? There are many ways to improve the performance of a Python program. Some of these are as follows: Data Structure: We have to select the right data structure for our purpose in a Python program. Standard Library: Wherever possible, we should use methods from standard library. Methods implemented in standard library have much better performance than user implementation. iii. Abstraction: At\\n\\nlot of abstraction and indirection can cause slow performance of a program. We should remove the redundant abstraction in code.\\n\\ntimes, a\\n\\niv. Algorithm: Use of right algorithm can make a big difference in a program. We have to find and select the suitable algorithm to solve our problem with high performance.\\n\\n2. What are the benefits of using Python? Python is strong that even Google uses it. Some of the benefits of using Python are as follows: i.\\n\\n2. What are the benefits of using Python? Python is strong that even Google uses it. Some of the benefits of using Python are as follows: in memory Efficient: Python management. For a large data set like Big Data, it is much easier to program in Python. Faster: Though Python code is interpreted, still Python has very fast performance.\\n\\niii. Wide usage: Python is widely used among different organizations for different projects. Due to this wide usage, there are thousands of add-ons available for use with Python.\\n\\niv. Easy to learn: Python is quite easy to learn. This is the biggest benefit of using Python. Complex tasks can be very easily implemented in Python.\\n\\n3. How will you specify source code encoding in a Python source file?\\n\\nBy default, every source code file in Python is in UTF-8 encoding. But we can also specify our own encoding for source files. This can be done by adding following line after #! line in the source file.\\n\\n# -*- coding: encoding -*-\\n\\nIn the above line we can replace encoding with the encoding that we want to use.\\n\\n4. What is the use of PEP 8 in Python?\\n\\nPEP 8 is a style guide for Python code. This document provides the coding conventions for writing code in Python. Coding conventions are about indentation, formatting, tabs, maximum line length, imports organization, line spacing etc. We use PEP 8 to bring consistency in our code. We consistency it is easier for other developers to read the code.\\n\\n5. What is Pickling in Python?\\n\\nPickling is a process by which a Python object hierarchy can be converted into a byte stream. The reverse operation of Pickling is Unpickling.\\n\\nPython has a module named pickle. This module has the implementation of a powerful algorithm for serialization and de-serialization of Python object structure.\\n\\nSome people also call Pickling as Serialization or Marshalling.\\n\\nWith Serialization we can transfer Python objects over the network. It is also used in persisting the state of a Python object. We can write it to a file or a database.\\n\\n6. How does memory management work in Python?\\n\\nThere is a private heap space in Python that contains all the Python objects and data structures. In CPython there is a memory manager responsible for managing the heap space.\\n\\nThere are different components in Python memory manager that handle segmentation, sharing, caching, memory pre-allocation etc.\\n\\nPython memory manager also takes care of garbage collection by using Reference counting algorithm.\\n\\n7. How will you perform Static Analysis on a Python Script?\\n\\nWe can use Static Analysis tool called PyChecker for this purpose. PyChecker can detect errors in Python code.\\n\\nPyChecker also gives warnings for any style issues.\\n\\nSome other tools to find bugs in Python code are pylint and pyflakes.\\n\\n8. What is the difference between a Tuple and List in Python?\\n\\nIn Python, Tuple and List are built-in data structures.\\n\\nSome of the differences between Tuple and List are as follows:\\n\\nI. Syntax: A Tuple is enclosed in parentheses:\\n\\nE.g. myTuple = (10, 20, â€œappleâ€); A List is enclosed in brackets: E.g. myList = [10, 20, 30];\\n\\nII. Mutable: Tuple is an immutable data structure. Whereas, a List is\\n\\na mutable data structure.\\n\\nIII. Size: A Tuple takes much lesser space than a List in Python.\\n\\nIV. Performance: Tuple is faster than a List in Python. So it gives us\\n\\ngood performance.\\n\\nV. Use case: Since Tuple is immutable, we can use it in cases like Dictionary creation. Whereas, a List is preferred in the use case where data can alter.\\n\\n9. What is a Python Decorator?\\n\\nA Python Decorator is a mechanism to wrap a Python function and modify its behavior by adding more functionality to it. We can use @ symbol to call a Python Decorator function.\\n\\n10. How are arguments passed in a Python method? By value or by reference?\\n\\nEvery argument in a Python method is an Object. All the variables in Python have reference to an Object. Therefore arguments in Python method are passed by Reference.\\n\\nSince some of the objects passed as reference are mutable, we can change those objects in a method. But for an Immutable object like String, any change done within a method is not reflected outside.\\n\\n11. What is the difference between List and Dictionary data types in Python?\\n\\nMain differences between List and Dictionary data types in Python are as follows:\\n\\nI. Syntax: In a List we store objects in a sequence. In a Dictionary\\n\\nwe store objects in key-value pairs.\\n\\nII. Reference: In List we access objects by index number. It starts\\n\\nfrom 0 index. In a Dictionary we access objects by key specified at the time of Dictionary creation.\\n\\nIII. Ordering: In a List objects are stored in an ordered sequence. In a\\n\\nDictionary objects are not stored in an ordered sequence.\\n\\nIV. Hashing: In a Dictionary, keys have to be hashable. In a List there\\n\\nis no need for hashing.\\n\\n12. What are the different built-in data types available in Python?\\n\\nSome of the built-in data types available in Python are as follows:\\n\\nNumeric types: These are the data types used to represent numbers in Python.\\n\\nint: It is used for Integers\\n\\nlong: It is used for very large integers of non-limited length.\\n\\nfloat: It is used for decimal numbers.\\n\\ncomplex: This one is for representing complex numbers\\n\\nSequence types: These data types are used to represent sequence of characters or objects.\\n\\nstr: This is similar to String in Java. It can represent a sequence of characters.\\n\\nbytes: This is a sequence of integers in the range of 0-255.\\n\\nbyte array: like bytes, but mutable (see below); only available in Python 3.x\\n\\nlist: This is a sequence of objects.\\n\\ntuple: This is a sequence of immutable objects.\\n\\nSets: These are unordered collections.\\n\\nset: This is a collection of unique objects.\\n\\nfrozen set: This is a collection of unique immutable objects.\\n\\nMappings: This is similar to a Map in Java.\\n\\ndict: This is also called hashmap. It has key value pair to store information by using hashing.\\n\\n13. What is a Namespace in Python?\\n\\nA Namespace in Python is a mapping between a name and an object. It is currently implemented as Python dictionary.\\n\\nE.g. the set of built-in exception names, the set of built-in names, local names in a function\\n\\nAt different moments in Python, different Namespaces are created. Each Namespace in Python can have a different lifetime.\\n\\nFor the list of built-in names, Namespace is created when Python interpreter starts.\\n\\nWhen Python interpreter reads the definition of a module, it creates global namespace for that module.\\n\\nWhen Python interpreter calls a function, it creates local namespace for that function.\\n\\n14. How will you concatenate multiple strings together in Python?\\n\\nWe can use following ways to concatenate multiple string together in Python:\\n\\nI. use + operator:\\n\\nE.g. >>> fname=\"John\" >>> lname=\"Ray\" >>> print fname+lname JohnRay\\n\\nII. use join function:\\n\\nE.g. >>> \\'\\'.join([\\'John\\',\\'Ray\\']) \\'JohnRay\\'\\n\\n15. What is the use of Pass statement in Python?\\n\\nThe use of Pass statement is to do nothing. It is just a placeholder for a statement that is required for syntax purpose. It does not execute any code or command.\\n\\nSome of the use cases for pass statement are as follows:\\n\\nI. Syntax purpose:\\n\\n>>> while True:\\n\\n... pass # Wait till user input is received\\n\\nII. Minimal Class: It can be used for creating minimal classes:\\n\\n>>> class MyMinimalClass:\\n\\n... pass\\n\\nIII. Place-holder for TODO work:\\n\\nWe can also use it as a placeholder for TODO work on a function or code that needs to be implemented at a later point of time.\\n\\n>>> def initialization():\\n\\n... pass # TODO\\n\\n16. What is the use of Slicing in Python?\\n\\nWe can use Slicing in Python to get a substring from a String.\\n\\nThe syntax of Slicing is very convenient to use.\\n\\nE.g. In following example we are getting a substring out of the name John.\\n\\n>>> name=\"John\"\\n\\n>>> name[1:3]\\n\\n\\'oh\\'\\n\\nIn Slicing we can give two indices in the String to create a Substring. If we do not give first index, then it defaults to 0.\\n\\nE.g.\\n\\n>>> name=\"John\"\\n\\n>>> name[:2]\\n\\n\\'Jo\\'\\n\\nIf we do not give second index, then it defaults to the size of the String.\\n\\n>>> name=\"John\"\\n\\n>>> name[3:]\\n\\n\\'n\\'\\n\\n17. What is the difference between Docstring in Python and Javadoc in Java?\\n\\nA Docstring in Python is a string used for adding comments or summarizing a piece of code in Python.\\n\\nThe main difference between Javadoc and Docstring is that docstring is available during runtime as well. Whereas, Javadoc is removed from the Bytecode and it is not present in .class file.\\n\\nWe can even use Docstring comments at run time as an interactive help manual.\\n\\nIn Python, we have to specify docstring as the first statement of a code object, just after the def or class statement.\\n\\nThe docstring for a code object can be accessed from the \\'__doc__\\' attribute of that object.\\n\\n18. How do you perform unit testing for Python code?\\n\\nWe can use the unit testing modules unittest or unittest2 to create and run unit tests for Python code.\\n\\nWe can even do automation of tests with these modules. Some of the main components of unittest are as follows:\\n\\nI. Test fixture: We use test fixture to create preparation methods required to run a test. It can even perform post-test cleanup.\\n\\nII. Test case: This is main unit test that we run on a piece of code. We\\n\\ncan use Testcase base class to create new test cases.\\n\\nIII. Test suite: We can aggregate our unit test cases in a Test suite.\\n\\nIV. Test runner: We use test runner to execute unit tests and produce\\n\\nreports of the test run.\\n\\n19. What is the difference between an Iterator and Iterable in Python?\\n\\nAn Iterable is an object that can be iterated by an Iterator.\\n\\nIn Python, Iterator object provides _iter_() and next() methods.\\n\\nIn Python, an Iterable object has _iter_ function that returns an Iterator object.\\n\\nWhen we work on a map or a for loop in Python, we can use next() method to get an Iterable item from the Iterator.\\n\\n20. What is the use of Generator in Python?\\n\\nWe can use Generator to create Iterators in Python. A Generator is written like a regular function. It can make use yield statement to return data during the function call. In this way we can write complex logic that works as an Iterator.\\n\\nA Generator is more compact than an Iterator due to the fact that _iter_() and next() functions are automatically created in a Generator.\\n\\nAlso within a Generator code, local variables and execution state are saved between multiple calls. Therefore, there is no need to add extra variables like self.index etc to keep track of iteration.\\n\\nGenerator also increases the readability of the code written in Python. It is a very simple implementation of an Iterator.\\n\\n21. What is the significance of functions that start and end with _ symbol in Python?\\n\\nPython provides many built-in functions that are surrounded by _ symbol at the start and end of the function name. As per Python documentation, double _ symbol is used for reserved names of functions.\\n\\nThese are also known as System-defined names.\\n\\nSome of the important functions are:\\n\\nObject._new_\\n\\nObject._init_\\n\\nObject._del_\\n\\n22. What is the difference between xrange and range in Python?\\n\\nIn Python, we use range(0,10) to create a list in memory for 10 numbers.\\n\\nPython provides another function xrange() that is similar to range() but xrange() returns a sequence object instead of list object. In xrange() all the values are not stored simultaneously in memory. It is a lazy loading based function.\\n\\nBut as per Python documentation, the benefit of xrange() over range() is very minimal in regular scenarios.\\n\\nAs of version 3.1, xrange is deprecated.\\n\\n23. What is lambda expression in Python?\\n\\nA lambda expression in Python is used for creating an anonymous function.\\n\\nWherever we need a function, we can also use a lambda expression.\\n\\nWe have to use lambda keyword for creating a lambda expression. Syntax of lambda function is as follows:\\n\\nlambda argumentList: expression\\n\\nE.g. lambda a,b: a+b\\n\\nThe above mentioned lambda expression takes two arguments and returns their sum.\\n\\nWe can use lambda expression to return a function.\\n\\nA lambda expression can be used to pass a function as an argument in another function.\\n\\n24. How will you copy an object in Python?\\n\\nIn Python we have two options to copy an object. It is similar to cloning an object in Java.\\n\\nI. Shallow Copy: To create a shallow copy we call copy.copy(x). In a shallow copy, Python creates a new compound object based on the original object. And it tries to put references from the original object into copy object.\\n\\nII. Deep Copy: To create a deep copy, we call copy.deepcopy(x). In a deep copy, Python creates a new object and recursively creates and inserts copies of the objects from original object into copy object. In a deep copy, we may face the issue of recursive loop due to infinite recursion.\\n\\n25. What are the main benefits of using Python?\\n\\nSome of the main benefits of using Python are as follows:\\n\\nI. Easy to learn: Python is simple language. It is easy to learn for a\\n\\nnew programmer.\\n\\nII. Large library: There is a large library for utilities in Python that\\n\\ncan be used for different kinds of applications.\\n\\nIII. Readability: Python has a variety of statements and expressions that are quite readable and very explicit in their use. It increases the readability of overall code.\\n\\nIV. Memory management: In Python, memory management is built\\n\\ninto the Interpreter. So a developer does not have to spend effort on managing memory among objects.\\n\\nV. Complex built-in Data types: Python has built-in Complex data types like list, set, dict etc. These data types give very good performance as well as save time in coding new features.\\n\\n26. What is a metaclass in Python?\\n\\nA metaclass in Python is also known as class of a class. A class defines the behavior of an instance. A metaclass defines the behavior of a class.\\n\\nOne of the most common metaclass in Python is type. We can subclass type to create our own metaclass.\\n\\nWe can use metaclass as a class-factory to create different types of classes.\\n\\n27. What is the use of frozenset in Python?\\n\\nA frozenset is a collection of unique values in Python. In addition to all the properties of set, a frozenset is immutable and hashable.\\n\\nOnce we have set the values in a frozenset, we cannot change. So we cannot use and update methods from set on frozenset.\\n\\nBeing hashable, we can use the objects in frozenset as keys in a Dictionary.\\n\\n28. What is Python Flask?\\n\\nPython Flask is a micro-framework based on Python to develop a web application.\\n\\nIt is a very simple application framework that has many extensions to build an enterprise level application.\\n\\nFlask does not provide a data abstraction layer or form validation by default. We can use external libraries on top of Flask to perform such tasks.\\n\\n29. What is None in Python?\\n\\nNone is a reserved keyword used in Python for null objects. It is neither a null value nor a null pointer. It is an actual object in Python. But there is only one instance of None in a Python environment.\\n\\nWe can use None as a default argument in a function.\\n\\nDuring comparison we have to use â€œisâ€ operator instead of â€œ==â€ for None.\\n\\n30. What is the use of zip() function in Python?\\n\\nIn Python, we have a built-in function zip() that can be used to aggregate all the Iterable objects of an Iterator.\\n\\nWe can use it to aggregate Iterable objects from two iterators as well.\\n\\nE.g.\\n\\nlist_1 = [\\'a\\', \\'b\\', \\'c\\']\\n\\nlist_2 = [\\'1\\', \\'2\\', \\'3\\']\\n\\nfor a, b in zip(list_1, list_2):\\n\\nprint a, b\\n\\nOutput:\\n\\na1\\n\\nb2\\n\\nc3\\n\\nBy using zip() function we can divide our input data from different sources into fixed number of sets.\\n\\n31. What is the use of // operator in Python?\\n\\nPython provides // operator to perform floor division of a number by another. The result of // operator is a whole number (without decimal part) quotient that we get by dividing left number with right number.\\n\\nIt can also be used floordiv(a,b).\\n\\nE.g.\\n\\n10// 4 = 2\\n\\n10//4 = -3\\n\\n32. What is a Module in Python?\\n\\nA Module is a script written in Python with import statements, classes, functions etc. We can use a module in another Python script by importing it or by giving the complete namespace.\\n\\nWith Modules, we can divide the functionality of our application in smaller chunks that can be easily managed.\\n\\n33. How can we create a dictionary with ordered set of keys in Python?\\n\\nIn a normal dictionary in Python, there is no order maintained between keys. To solve this problem, we can use OrderDict class in Python. This class is available for use since version 2.7.\\n\\nIt is similar to a dictionary in Python, but it maintains the insertion order of keys in the dictionary collection.\\n\\n34. Python is an Object Oriented programming language or a functional programming language?\\n\\nPython uses most of the Object Oriented programming concepts. But we can also do functional programming in Python. As per the opinion of experts, Python is a multi-paradigm programming language.\\n\\nWe can do functional, procedural, object-oriented and imperative programming with the help of Python.\\n\\n35. How can we retrieve data from a MySQL database in a Python script?\\n\\nTo retrieve data from a database we have to make use of the module available for that database. For MySQL database, we import MySQLdb module in our Python script.\\n\\nWe have to first connect to a specific database by passing URL, username, password and the name of database.\\n\\nOnce we establish the connection, we can open a cursor with cursor() function. On an open cursor, we can run fetch() function to execute queries and retrieve data from the database tables.\\n\\n36. What is the difference between append() and extend() functions of a list in Python?\\n\\nIn Python, we get a built-in sequence called list. We can call standard functions like append() and extend() on a list.\\n\\nWe call append() method to add an item to the end of a list.\\n\\nWe call extend() method to add another list to the end of a list.\\n\\nIn append() we have to add items one by one. But in extend() multiple items from another list can be added at the same time.\\n\\n37. How will you handle an error condition in Python code?\\n\\nWe can implement exception handling to handle error conditions in Python code. If we are expecting an error condition that we cannot handle, we can raise an error with appropriate message.\\n\\nE.g.\\n\\n>>> if student_score < 0: raise ValueError(â€œScore can not be negativeâ€)\\n\\nIf we do not want to stop the program, we can just catch the error condition, print a message and continue with our program.\\n\\nE.g. In following code snippet we are catching the error and continuing with the default value of age.\\n\\n#!/usr/bin/python try:\\n\\nage=18+\\'duration\\'\\n\\nexcept:\\n\\nprint(\"duration has to be a number\")\\n\\nage=18 print(age)\\n\\n38. What is the difference between split() and slicing in Python?\\n\\nBoth split() function and slicing work on a String object. By using split() function, we can get the list of words from a String.\\n\\nE.g. \\'a b c \\'.split() returns [â€˜aâ€™, â€˜bâ€™, â€˜câ€™]\\n\\nSlicing is a way of getting substring from a String. It returns another String.\\n\\nE.g. >>> \\'a b c\\'[2:3] returns b\\n\\n39. How will you check in Python, if a class is subclass of another class?\\n\\nPython provides a useful method issubclass(a,b) to check whether class a is a subclass of b.\\n\\nE.g. int is not a subclass of long >>> issubclass(int,long) False\\n\\nbool is a subclass of int\\n\\n>>> issubclass(bool,int) True\\n\\n40. How will you debug a piece of code in Python?\\n\\nIn Python, we can use the debugger pdb for debugging the code. To start debugging we have to enter following lines on the top of a Python script.\\n\\nimport pdb\\n\\npdb.set_trace()\\n\\nAfter adding these lines, our code runs in debug mode. Now we can use commands like breakpoint, step through, step into etc for debugging.\\n\\n41. How do you profile a Python script?\\n\\nPython provides a profiler called cProfile that can be used for profiling Python code.\\n\\nWe can call it from our code as well as from the interpreter.\\n\\nIt gives use the number of function calls as well as the total time taken to run the script.\\n\\nWe can even write the profile results to a file instead of standard out.\\n\\n42. What is the difference between â€˜isâ€™ and â€˜==â€™ in Python?\\n\\nWe use â€˜isâ€™ to check an object against its identity.\\n\\nWe use â€˜==â€™ to check equality of two objects.\\n\\nE.g.\\n\\n>>> lst = [10,20, 20]\\n\\n>>> lst == lst[:]\\n\\nTrue\\n\\n>>> lst is lst[:]\\n\\nFalse\\n\\n43. How will you share variables across modules in Python?\\n\\nWe can create a common module with variables that we want to share.\\n\\nThis common module can be imported in all the modules in which we want to share the variables.\\n\\nIn this way, all the shared variables will be in one module and available for sharing with any new module as well.\\n\\n44. How can we do Functional programming in Python?\\n\\nIn Functional Programming, we decompose a program into functions. These functions take input and after processing give an output. The function does not maintain any state.\\n\\nPython provides built-in functions that can be used for Functional programming. Some of these functions are:\\n\\nI. Map() II. III.\\n\\nreduce() filter()\\n\\nEvent iterators and generators can be used for Functional programming in Python.\\n\\n45. What is the improvement in enumerate() function of Python?\\n\\nIn Python, enumerate() function is an improvement over regular iteration. The enumerate() function returns an iterator that gives (0, item[0]).\\n\\nE.g.\\n\\n>>> thelist=[\\'a\\',\\'b\\'] >>> for i,j in enumerate(thelist): ... print i,j ... 0 a 1 b\\n\\n46. How will you execute a Python script in Unix?\\n\\nTo execute a Python script in Unix, we need to have Python executor in Unix environment.\\n\\nIn addition to that we have to add following line as the first line in a Python script file.\\n\\n#!/usr/local/bin/python\\n\\nThis will tell Unix to use Python interpreter to execute the script.\\n\\n47. What are the popular Python libraries used in Data analysis?\\n\\nSome of the popular libraries of Python used for Data analysis are:\\n\\nI. Pandas: Powerful Python Data Analysis Toolkit II. SciKit: This is a machine learning library in Python. III. Seaborn: This is a statistical data visualization library in Python. IV. SciPy: This is an open source system for science, mathematics and\\n\\nengineering implemented in Python.\\n\\n48. What is the output of following code in Python?\\n\\n>>> thelist=[\\'a\\',\\'b\\']\\n\\n>>> print thelist[3:]\\n\\nAns: The output of this code is following:\\n\\n[]\\n\\nEven though the list has only 2 elements, the call to thelist with index 3 does not give any index error.\\n\\n49. What is the output of following code in Python?\\n\\n>>>name=â€™John Smithâ€™\\n\\n>>>print name[:5] + name[5:]\\n\\nAns: Output of this will be\\n\\nJohn Smith\\n\\nThis is an example of Slicing. Since we are slicing at the same index, the first name[:5] gives the substring name upto 5th location excluding 5th location. The name[5:] gives the rest of the substring of name from the 5th location. So we get the full name as output.\\n\\n50. If you have data with name of customers and their location, which data type will you use to store it in Python?\\n\\nIn Python, we can use dict data type to store key value pairs. In this example, customer name can be the key and their location can be the value in a dict data type.\\n\\nDictionary is an efficient way to store data that can be looked up based on a key.'),\n",
       " Document(metadata={'source': '..\\\\data\\\\pdf\\\\Python Questions -3.pdf'}, page_content='100 Python Interview Questions & Answers for Data Analyst and Data Science\\n\\n1. Question: Reverse a string\\n\\nAnswer:\\n\\ns = \\'hello\\'\\n\\nprint(s[::-1])\\n\\n2. Question: Check if a number is prime\\n\\nAnswer:\\n\\nn = 7\\n\\nprint(all(n % i != 0 for i in range(2, int(n**0.5) + 1)))\\n\\n3. Question: Find factorial using recursion\\n\\nAnswer:\\n\\ndef fact(n):\\n\\nreturn 1 if n == 0 else n * fact(n-1)\\n\\nprint(fact(5))\\n\\n4. Question: Check palindrome string\\n\\nAnswer:\\n\\ns = \\'madam\\'\\n\\nprint(s == s[::-1])\\n\\n5. Question: Find largest number in a list\\n\\nAnswer:\\n\\nnums = [1, 5, 2]\\n\\nprint(max(nums))\\n\\n6. Question: Swap two variables\\n\\nAnswer:\\n\\na, b = 5, 10\\n\\na, b = b, a\\n\\nprint(a, b)\\n\\n7. Question: Count vowels in a string\\n\\nAnswer:\\n\\ns = \\'hello\\'\\n\\nprint(sum(1 for ch in s if ch in \\'aeiou\\'))\\n\\n8. Question: Check Armstrong number\\n\\nAnswer:\\n\\nn = 153\\n\\nprint(sum(int(d) ** 3 for d in str(n)) == n)\\n\\n9. Question: Generate Fibonacci series\\n\\nAnswer:\\n\\na, b = 0, 1\\n\\nfor _ in range(5):\\n\\nprint(a)\\n\\na, b = b, a + b\\n\\n10. Question: Find GCD of two numbers\\n\\nAnswer:\\n\\nimport math\\n\\nprint(math.gcd(12, 15))\\n\\n11. Question: Check anagram\\n\\nAnswer:\\n\\ns1, s2 = \\'listen\\', \\'silent\\'\\n\\nprint(sorted(s1) == sorted(s2))\\n\\n12. Question: Reverse words in a sentence\\n\\nAnswer:\\n\\ns = \\'hello world\\'\\n\\nprint(\\' \\'.join(s.split()[::-1]))\\n\\n13. Question: Find duplicates in a list\\n\\nAnswer:\\n\\nnums = [1, 2, 2, 3]\\n\\nprint([x for x in set(nums) if nums.count(x) > 1])\\n\\n14. Question: Find second largest number in a list\\n\\nAnswer:\\n\\nnums = [1, 2, 3, 4]\\n\\nprint(sorted(set(nums))[-2])\\n\\n15. Question: Check if string contains only digits\\n\\nAnswer:\\n\\nprint(\\'123\\'.isdigit())\\n\\n16. Question: Flatten nested list\\n\\nAnswer:\\n\\nnested = [[1, 2], [3, 4]]\\n\\nï¬‚at = [x for sub in nested for x in sub]\\n\\nprint(ï¬‚at)\\n\\n17. Question: Sort dictionary by values\\n\\nAnswer:\\n\\nd = {\\'a\\': 2, \\'b\\': 1}\\n\\nprint(dict(sorted(d.items(), key=lambda x: x[1])))\\n\\n18. Question: Count words in a string\\n\\nAnswer:\\n\\ns = \\'this is test\\'\\n\\nprint(len(s.split()))\\n\\n19. Question: Check leap year\\n\\nAnswer:\\n\\ny = 2024\\n\\nprint(y % 4 == 0 and (y % 100 != 0 or y % 400 == 0))\\n\\n20. Question: Merge two dictionaries\\n\\nAnswer:\\n\\nd1 = {\\'a\\': 1}\\n\\nd2 = {\\'b\\': 2}\\n\\nd1.update(d2)\\n\\nprint(d1)\\n\\n21. Question: Group words by ï¬rst letter\\n\\nAnswer:\\n\\nfrom collections import defaultdict\\n\\nwords = [\"apple\",\"ant\",\"banana\",\"ball\",\"cat\",\"car\"]\\n\\ngrouped = defaultdict(list)\\n\\nfor word in words:\\n\\ngrouped[word[0]].append(word)\\n\\nprint(dict(grouped))\\n\\n22. Question: Find all pairs in list whose sum equals target\\n\\nAnswer:\\n\\nnums = [1,2,3,4,5,6,7]\\n\\ntarget = 8\\n\\npairs = [(a,b) for i,a in enumerate(nums) for b in nums[i+1:] if a+b==target]\\n\\nprint(pairs)\\n\\n23. Question: Convert list of tuples into dictionary\\n\\nAnswer:\\n\\npairs = [(\"a\",1),(\"b\",2),(\"c\",3)]\\n\\nprint(dict(pairs))\\n\\n24. Question: Find top 3 most frequent elements in a list\\n\\nAnswer:\\n\\nfrom collections import Counter\\n\\nnums = [1,2,2,3,3,3,4,4,4,4,5]\\n\\nprint(Counter(nums).most_common(3))\\n\\n25. Question: Transpose a matrix\\n\\nAnswer:\\n\\nmatrix = [[1,2,3],[4,5,6],[7,8,9]]\\n\\ntranspose = [[row[i] for row in matrix] for i in range(len(matrix[0]))]\\n\\nprint(transpose)\\n\\n26. Question: Implement stack using Python list\\n\\nAnswer:\\n\\nstack = []\\n\\nstack.append(10)\\n\\nstack.append(20)\\n\\nprint(stack.pop())\\n\\n27. Question: Implement queue using collections.deque\\n\\nAnswer:\\n\\nfrom collections import deque\\n\\nqueue = deque()\\n\\nqueue.append(10)\\n\\nqueue.append(20)\\n\\nprint(queue.popleft())\\n\\n28. Question: Generate random numbers without random module\\n\\nAnswer:\\n\\nimport time\\n\\ndef pseudo_random(seed=1):\\n\\nseed = (seed*9301+49297) % 233280\\n\\nreturn seed/233280.0\\n\\nprint(pseudo_random(int(time.time())))\\n\\n29. Question: Convert string to title case\\n\\nAnswer:\\n\\ntext = \"data science with python\"\\n\\nprint(text.title())\\n\\n30. Question: Remove punctuation from string\\n\\nAnswer:\\n\\nimport string\\n\\ntext = \"Hello, World! Data@Science.\"\\n\\nclean = \"\".join(c for c in text if c not in string.punctuation)\\n\\nprint(clean)\\n\\n31. Question: Load CSV in pandas and display ï¬rst 10 rows\\n\\nAnswer:\\n\\nimport pandas as pd\\n\\ndf = pd.read_csv(\"data.csv\")\\n\\nprint(df.head(10))\\n\\n32. Question: Select rows where column value > 100\\n\\nAnswer:\\n\\nprint(df[df[\"Sales\"] > 100])\\n\\n33. Question: Drop rows with missing values\\n\\nAnswer:\\n\\nprint(df.dropna())\\n\\n34. Question: Fill missing values with column mean\\n\\nAnswer:\\n\\ndf[\"Sales\"].ï¬llna(df[\"Sales\"].mean(), inplace=True)\\n\\n35. Question: Merge two DataFrames on a column\\n\\nAnswer:\\n\\ndf1 = pd.DataFrame({\"ID\":[1,2],\"Name\": [\"A\",\"B\"]})\\n\\ndf2 = pd.DataFrame({\"ID\":[1,2],\"Age\": [25,30]})\\n\\nmerged = pd.merge(df1,df2,on=\"ID\")\\n\\nprint(merged)\\n\\n36. Question: Group data by column and calculate mean\\n\\nAnswer:\\n\\nprint(df.groupby(\"Category\") [\"Sales\"].mean())\\n\\n37. Question: Sort DataFrame by multiple columns\\n\\nAnswer:\\n\\nprint(df.sort_values(by= [\"Category\",\"Sales\"], ascending=[True,False]))\\n\\n38. Question: Apply custom function to column\\n\\nAnswer:\\n\\ndf[\"Discounted\"] = df[\"Sales\"].apply(lambda x: x*0.9)\\n\\n39. Question: Find number of unique values in a column\\n\\nAnswer:\\n\\nprint(df[\"CustomerID\"].nunique())\\n\\n40. Question: Rename multiple columns in DataFrame\\n\\nAnswer:\\n\\ndf.rename(columns= {\"Sales\":\"Total_Sales\",\"Date\":\"Order_Date\"}, inplace=True)\\n\\n41. Question: Create a NumPy array of zeros\\n\\nAnswer:\\n\\nimport numpy as np\\n\\narr = np.zeros((3,3))\\n\\nprint(arr)\\n\\n42. Question: Create a NumPy array from 1 to 10\\n\\nAnswer:\\n\\narr = np.arange(1,11)\\n\\nprint(arr)\\n\\n43. Question: Reshape 1D NumPy array to 2D\\n\\nAnswer:\\n\\narr = np.arange(1,7).reshape(2,3)\\n\\nprint(arr)\\n\\n44. Question: Find max and min in NumPy array\\n\\nAnswer:\\n\\narr = np.array([3,7,1,9,2])\\n\\nprint(arr.max(), arr.min())\\n\\n45. Question: Compute mean, median, std of array\\n\\nAnswer:\\n\\narr = np.array([1,2,3,4,5,6])\\n\\nprint(arr.mean(), np.median(arr), arr.std())\\n\\n46. Question: Slice ï¬rst 3 elements of array\\n\\nAnswer:\\n\\narr = np.array([10,20,30,40,50])\\n\\nprint(arr[:3])\\n\\n47. Question: Find index of max in NumPy array\\n\\nAnswer:\\n\\narr = np.array([4,8,2,9,6])\\n\\nprint(arr.argmax())\\n\\n48. Question: Create diagonal matrix in NumPy\\n\\nAnswer:\\n\\narr = np.diag([1,2,3])\\n\\nprint(arr)\\n\\n49. Question: Multiply two NumPy arrays element-wise\\n\\nAnswer:\\n\\na = np.array([1,2,3])\\n\\nb = np.array([4,5,6])\\n\\nprint(a*b)\\n\\n50. Question: Perform matrix multiplication\\n\\nAnswer:\\n\\na = np.array([[1,2],[3,4]])\\n\\nb = np.array([[5,6],[7,8]])\\n\\nprint(np.dot(a,b))\\n\\n51. Question: Create random NumPy array of shape 2x3\\n\\nAnswer:\\n\\narr = np.random.rand(2,3)\\n\\nprint(arr)\\n\\n52. Question: Normalize a NumPy array\\n\\nAnswer:\\n\\narr = np.array([10,20,30])\\n\\nnorm = (arr-arr.min())/(arr.max()-arr.min())\\n\\nprint(norm)\\n\\n53. Question: Get unique values from NumPy array\\n\\nAnswer:\\n\\narr = np.array([1,2,2,3,3,3,4])\\n\\nprint(np.unique(arr))\\n\\n54. Question: Replace negative values with zero\\n\\nAnswer:\\n\\narr = np.array([-2,5,-7,8])\\n\\narr[arr < 0] = 0\\n\\nprint(arr)\\n\\n55. Question: Check for NaN in NumPy array\\n\\nAnswer:\\n\\narr = np.array([1,np.nan,3])\\n\\nprint(np.isnan(arr))\\n\\n56. Question: Convert NumPy array to Python list\\n\\nAnswer:\\n\\narr = np.array([1,2,3])\\n\\nprint(arr.tolist())\\n\\n57. Question: Filter rows in pandas with multiple conditions\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"A\":[1,2,3,4],\"B\": [10,20,30,40]})\\n\\nprint(df[(df[\"A\"] > 2) & (df[\"B\"] < 40)])\\n\\n58. Question: Create pivot table in pandas\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"Category\": [\"A\",\"A\",\"B\",\"B\"],\"Sales\":[100,200,300,400]})\\n\\npivot = df.pivot_table(values=\"Sales\", index=\"Category\", aggfunc=\"sum\")\\n\\nprint(pivot)\\n\\n59. Question: Get correlation matrix\\n\\nAnswer:\\n\\nprint(df.corr())\\n\\n60. Question: Save DataFrame to CSV\\n\\nAnswer:\\n\\ndf.to_csv(\"output.csv\", index=False)\\n\\n61. Question: Select speciï¬c columns\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"A\":[1,2,3],\"B\": [4,5,6],\"C\":[7,8,9]})\\n\\nprint(df[[\"A\",\"C\"]])\\n\\n62. Question: Sort DataFrame by column descending\\n\\nAnswer:\\n\\nprint(df.sort_values(\"B\", ascending=False))\\n\\n63. Question: Rename columns\\n\\nAnswer:\\n\\ndf = df.rename(columns= {\"A\":\"Col1\",\"B\":\"Col2\"})\\n\\nprint(df)\\n\\n64. Question: Drop rows with missing values\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"A\":[1,None,3],\"B\": [4,5,None]})\\n\\nprint(df.dropna())\\n\\n65. Question: Fill missing values with mean\\n\\nAnswer:\\n\\ndf[\"A\"].ï¬llna(df[\"A\"].mean(), inplace=True)\\n\\nprint(df)\\n\\n66. Question: Convert column to datetime\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"Date\":[\"2023-01- 01\",\"2023-02-01\"]})\\n\\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\\n\\nprint(df.dtypes)\\n\\n67. Question: Extract year and month from datetime\\n\\nAnswer:\\n\\ndf[\"Year\"] = df[\"Date\"].dt.year\\n\\ndf[\"Month\"] = df[\"Date\"].dt.month\\n\\nprint(df)\\n\\n68. Question: Remove duplicate rows\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"A\":[1,2,2,3],\"B\": [4,5,5,6]})\\n\\nprint(df.drop_duplicates())\\n\\n69. Question: Group by column and sum\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"Category\": [\"A\",\"A\",\"B\"],\"Sales\":[100,200,300]})\\n\\nprint(df.groupby(\"Category\") [\"Sales\"].sum())\\n\\n70. Question: Apply custom function to column\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"A\":[1,2,3]})\\n\\ndf[\"Square\"] = df[\"A\"].apply(lambda x:x**2)\\n\\nprint(df)\\n\\n71. Question: Merge two DataFrames on key\\n\\nAnswer:\\n\\ndf1 = pd.DataFrame({\"ID\":[1,2],\"Name\": [\"A\",\"B\"]})\\n\\ndf2 = pd.DataFrame({\"ID\":[1,2],\"Age\": [25,30]})\\n\\nprint(pd.merge(df1,df2,on=\"ID\"))\\n\\n72. Question: Concatenate two DataFrames vertically\\n\\nAnswer:\\n\\ndf1 = pd.DataFrame({\"A\":[1,2]})\\n\\ndf2 = pd.DataFrame({\"A\":[3,4]})\\n\\nprint(pd.concat([df1,df2]))\\n\\n73. Question: Find top 2 rows by column\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"A\":[10,40,30,20]})\\n\\nprint(df.nlargest(2,\"A\"))\\n\\n74. Question: Find bottom 2 rows by column\\n\\nAnswer:\\n\\nprint(df.nsmallest(2,\"A\"))\\n\\n75. Question: Count missing values\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"A\":[1,None,3],\"B\": [None,5,6]})\\n\\nprint(df.isnull().sum())\\n\\n76. Question: Replace speciï¬c value\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"A\":[1,2,2,3]})\\n\\ndf[\"A\"].replace(2, 99, inplace=True)\\n\\nprint(df)\\n\\n77. Question: Convert categorical to dummy variables\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"Fruit\": [\"Apple\",\"Banana\",\"Apple\"]})\\n\\nprint(pd.get_dummies(df, columns= [\"Fruit\"]))\\n\\n78. Question: Calculate cumulative sum\\n\\nAnswer:\\n\\ndf = pd.DataFrame({\"Sales\": [100,200,300]})\\n\\ndf[\"Cumulative\"] = df[\"Sales\"].cumsum()\\n\\nprint(df)\\n\\n79. Question: Calculate percentage of total\\n\\nAnswer:\\n\\ndf[\"Percentage\"] = df[\"Sales\"]/df[\"Sales\"].sum()*100\\n\\nprint(df)\\n\\n80. Question: Detect outliers using IQR\\n\\nAnswer:\\n\\nQ1 = df[\"Sales\"].quantile(0.25)\\n\\nQ3 = df[\"Sales\"].quantile(0.75)\\n\\nIQR = Q3 - Q1\\n\\noutliers = df[(df[\"Sales\"] < Q1 - 1.5*IQR) | (df[\"Sales\"] > Q3 + 1.5*IQR)]\\n\\nprint(outliers)\\n\\n81. Question: Standardize a numeric column using z-score\\n\\nAnswer:\\n\\ndf[\"Zscore\"] = (df[\"Sales\"] - df[\"Sales\"].mean()) / df[\"Sales\"].std()\\n\\nprint(df)\\n\\n82. Question: Normalize values between 0 and 1\\n\\nAnswer:\\n\\ndf[\"Normalized\"] = (df[\"Sales\"]- df[\"Sales\"].min())/(df[\"Sales\"].max()- df[\"Sales\"].min())\\n\\nprint(df)\\n\\n83. Question: Split dataset into train and test\\n\\nAnswer:\\n\\nfrom sklearn.model_selection import train_test_split\\n\\nX = df[[\"Sales\"]]\\n\\ny = [0,1,0,1]\\n\\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_stat\\n\\ne=42)\\n\\nprint(X_train,y_train)\\n\\n84. Question: Train simple linear regression\\n\\nAnswer:\\n\\nfrom sklearn.linear_model import LinearRegression\\n\\nX = np.array([[1],[2],[3],[4]])\\n\\ny = np.array([2,4,6,8])\\n\\nmodel = LinearRegression()\\n\\nmodel.ï¬t(X,y)\\n\\nprint(model.coef_, model.intercept_)\\n\\n85. Question: Make predictions\\n\\nAnswer:\\n\\npred = model.predict([[5]])\\n\\nprint(pred)\\n\\n86. Question: Evaluate regression using R2 score\\n\\nAnswer:\\n\\nfrom sklearn.metrics import r2_score\\n\\ny_true = [2,4,6,8]\\n\\ny_pred = model.predict(X)\\n\\nprint(r2_score(y_true,y_pred))\\n\\n87. Question: Train logistic regression\\n\\nAnswer:\\n\\nfrom sklearn.linear_model import LogisticRegression\\n\\nX = np.array([[1],[2],[3],[4]])\\n\\ny = np.array([0,0,1,1])\\n\\nclf = LogisticRegression()\\n\\nclf.ï¬t(X,y)\\n\\nprint(clf.predict([[2.5]]))\\n\\n88. Question: Create confusion matrix\\n\\nAnswer:\\n\\nfrom sklearn.metrics import confusion_matrix\\n\\ny_true = [0,0,1,1]\\n\\ny_pred = [0,1,1,1]\\n\\nprint(confusion_matrix(y_true,y_pred))\\n\\n89. Question: Calculate accuracy, precision, recall, f1-score\\n\\nAnswer:\\n\\nfrom sklearn.metrics import classiï¬cation_report\\n\\nprint(classiï¬cation_report(y_true,y_pred))\\n\\n90. Question: Perform k-fold cross- validation\\n\\nAnswer:\\n\\nfrom sklearn.model_selection import cross_val_score\\n\\nscores = cross_val_score(clf,X,y,cv=3)\\n\\nprint(scores.mean())\\n\\n91. Question: Train decision tree classiï¬er\\n\\nAnswer:\\n\\nfrom sklearn.tree import DecisionTreeClassiï¬er\\n\\nclf = DecisionTreeClassiï¬er()\\n\\nclf.ï¬t(X,y)\\n\\nprint(clf.predict([[2]]))\\n\\n92. Question: Train random forest classiï¬er\\n\\nAnswer:\\n\\nfrom sklearn.ensemble import RandomForestClassiï¬er\\n\\nclf = RandomForestClassiï¬er()\\n\\nclf.ï¬t(X,y)\\n\\nprint(clf.predict([[3]]))\\n\\n93. Question: Train support vector machine classiï¬er\\n\\nAnswer:\\n\\nfrom sklearn.svm import SVC\\n\\nclf = SVC()\\n\\nclf.ï¬t(X,y)\\n\\nprint(clf.predict([[2.5]]))\\n\\n94. Question: Scale features using StandardScaler\\n\\nAnswer:\\n\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\n\\nX_scaled = scaler.ï¬t_transform(X)\\n\\nprint(X_scaled)\\n\\n95. Question: Encode categorical using LabelEncoder\\n\\nAnswer:\\n\\nfrom sklearn.preprocessing import LabelEncoder\\n\\nfruits = [\"Apple\",\"Banana\",\"Apple\",\"Orange\"]\\n\\nencoder = LabelEncoder()\\n\\nencoded = encoder.ï¬t_transform(fruits)\\n\\nprint(encoded)\\n\\n96. Question: One-hot\\n\\nencode categorical feature\\n\\nAnswer:\\n\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\nencoder = OneHotEncoder(sparse=False)\\n\\ndata = np.array(fruits).reshape(-1,1)\\n\\nencoded = encoder.ï¬t_transform(data)\\n\\nprint(encoded)\\n\\n97. Question: Reduce dimensions using PCA\\n\\nAnswer:\\n\\nfrom sklearn.decomposition import PCA\\n\\npca = PCA(n_components=2)\\n\\nX_reduced = pca.ï¬t_transform(X_scaled)\\n\\nprint(X_reduced)\\n\\n98. Question: Save trained model with joblib\\n\\nAnswer:\\n\\nimport joblib\\n\\njoblib.dump(clf,\"model.pkl\")\\n\\n99. Question: Load trained model with joblib\\n\\nAnswer:\\n\\nmodel = joblib.load(\"model.pkl\")\\n\\nprint(model.predict([[2]]))\\n\\n100. Question: Create pipeline with scaler and classiï¬er\\n\\nAnswer:\\n\\nfrom sklearn.pipeline import Pipeline\\n\\npipeline = Pipeline([(\"scaler\", StandardScaler()),(\"clf\", LogisticRegression())])\\n\\npipeline.ï¬t(X,y)\\n\\nprint(pipeline.predict([[2.5]]))\\n\\n---'),\n",
       " Document(metadata={'source': '..\\\\data\\\\pdf\\\\RAG Cheat Sheet.pdf'}, page_content='Tajamul Khan\\n\\nRAG Cheat Sheet\\n\\n@Tajamulkhann\\n\\nLangChain LangChain is a framework designed for building language model (LLM)- powered applications with modular, production-ready components.\\n\\nRAG\\n\\n@Tajamulkhann\\n\\nRetrieval-Augmented Generation enhances language model outputs by fetching relevant, up-to-date information from external knowledge sources and combining it with generative AI capability.\\n\\nInstallation\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nComponents Document Loaders â†’ Bring in data from different sources.\\n\\nText Splitters â†’ Break documents into smaller, manageable chunks.\\n\\n@Tajamulkhann\\n\\nEmbeddings â†’ Turn text into numerical vectors for understanding.\\n\\nVector Stores â†’ Save and efficiently search these vectors.\\n\\nRetrievers â†’ Find the most relevant chunks for a query.\\n\\nGenerators â†’ Generate answers or insights using the retrieved information.\\n\\nEvaluation â†’ Check how accurate or useful the results are.\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nPreprocessing\\n\\nLangChain doesnâ€™t include built-in preprocessingâ€”you need to handle it yourself. Use: Documents often have noise and formatting like headers, footers, or page numbers, so cleaning and standardizing text is essential.\\n\\n@Tajamulkhann\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nDocument Loaders Import data from multiple sources.\\n\\nPDF\\n\\n@Tajamulkhann\\n\\nText\\n\\nWebsites\\n\\nCSV\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nText Splitters\\n\\nBreak large documents into manageable, context-preserving chunks.\\n\\nBest Practice: Use 300 character chunks with 50 character overlap.\\n\\n@Tajamulkhann\\n\\nRecursive Character Text Splitter\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nEmbeddings Convert text chunks to semantic vectors.\\n\\nCommon Models:\\n\\nHuggingFace (\\'all-mpnet-base-v2\\', \\'all-MiniLM-L6-v2\\') OpenAI (\\'text-embedding-ada-002\\')\\n\\n@Tajamulkhann\\n\\nHugging Face Embedding\\n\\nOpen AI Embedding\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nVector Stores\\n\\nStores embeddings and enables fast similarity search (usually using cosine similarity) to find relevant chunks.\\n\\nPopular:\\n\\nFAISS for fast local search Chroma/Pinecone for scalable options.\\n\\n@Tajamulkhann\\n\\nFAISS\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nRetrievers\\n\\nFinds the most relevant document chunks based on user query similarity. How it works:\\n\\nConvert query to embedding Search vector database for similar chunks Return top-k most relevant results\\n\\n@Tajamulkhann\\n\\nKey Settings:\\n\\nk: Number of chunks to retrieve (3-10) search_type: \" similarity \" or \" mmr \" (for diversity) score_threshold: Min. similarity score\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nGenerators retrieval and generationâ€”process user query, fetch relevant info, and generate an informed response with LLM.\\n\\n@Tajamulkhann\\n\\nChain Types:\\n\\nstuff: fast, for small context map-reduce: for longer docs refine: slowest, highest quality\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nEvaluation\\n\\nEvaluates RAG system performance to maintain quality and highlight areas for improvement. Key Metrics:\\n\\nFaithfulness â†’ Response accurately reflects the source documents. Relevance â†’ Response directly answers the question. Retrieval Quality â†’ Retrieved chunks are useful and on-topic.\\n\\n@Tajamulkhann\\n\\nEvaluation Types: Automated: BLEU, ROUGE, BERTScore Human: User ratings, comparative analysis Continuous: A/B testing, feedback loops\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nRAG Tips\\n\\nSuccess Tips for RAG\\n\\nChunking â†’ ~300 chars + 50â€“100 overlap. Retrieval â†’ Try k=3â€“10 based on query. LLM â†’ Temperature = 0 for factual output.\\n\\n@Tajamulkhann\\n\\nCommon Issues\\n\\nPoor retrieval â†’ adjust chunk size/embeddings. Hallucination â†’ improve context quality. Slow response â†’ reduce chunk size or k.\\n\\nProduction Ready\\n\\nCaching frequent embeddings. Monitor performance & feedback. Add fallbacks for errors. Secure API keys & data.\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nRAG Pipeline\\n\\nDocument Loading\\n\\nPreprocessing\\n\\n@Tajamulkhann\\n\\nChunking\\n\\nEmbeddings\\n\\nVector DB\\n\\nRetrieval\\n\\nGeneration\\n\\nEvaluation\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nFound Helpful ? Repost\\n\\n@Tajamulkhann\\n\\n@Tajamul.datascientist\\n\\nFollow for more!')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e586bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
