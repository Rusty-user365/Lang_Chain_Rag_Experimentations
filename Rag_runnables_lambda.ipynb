{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa0426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI get fired from the job? \n",
      "\n",
      "Because it kept saying, \"I'm not good enough.\" \n",
      "\n",
      " word count - 17\n"
     ]
    }
   ],
   "source": [
    "# Make sure you’ve updated the package\n",
    "# pip install -U langchain-ollama\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import  RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.strip().split())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Write a joke about {topic}\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "# Updated Ollama integration\n",
    "model = ChatOllama(model=\"gemma3:270m\", temperature=0.7)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\": RunnablePassthrough(),\n",
    "    \"word_count\": RunnableLambda(word_count)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "result = final_chain.invoke({\"topic\": \"AI\"})\n",
    "\n",
    "print(f\"{result['joke']}\\n word count - {result['word_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf10b235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Impersonate 'chrome_128' does not exist, using 'random'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 day ago · IND vs PAK · India vs Pakistan Highlights, T20 World Cup 2026: Kishan's sensational 77 makes it 8-1 against Pakistan. 1 day ago · India (IND) vs Pakistan (PAK) T20 World Cup 2026 Match: Suryakumar Yadav's India have defeated Pakistan by 61 runs at the Premadasa Stadium in Colombo to ... 1 day ago · India beat Pakistan by 61 runs to progress to Super 8s from Group A. Match took place after Pakistan government ended its order for boycott. 1 day ago · India vs Pakistan Live Score - Live Cricket Scorecard IND vs PAK, Match 27, ICC Men's T20 World Cup, 2026, R.Premadasa Stadium, Colombo. India. 0/00.0 ... 1 day ago · Pakistan were bowled out for 114 in 18 overs, leaving India winners by a comprehensive 61 runs. India's bowlers delivered with clinical precision. Hardik Pandya ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize the tool\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Run a query\n",
    "result = search.run(\"yesterday india pakistan match score\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d464d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " echo Hello World\n",
      "Hello World\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\langchain_community\\tools\\shell\\tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import ShellTool\n",
    "shell = ShellTool()\n",
    "result = shell.run(\"echo Hello World\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0607aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "result = add_numbers.invoke({'a':5,'b': 7})\n",
    "print(result)\n",
    "print(add_numbers.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccd3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "@tool \n",
    "def multiply_numbers(a:int,b:int)->int:\n",
    "    \"\"\"Mutliply two numbers together\"\"\"\n",
    "    return a*b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c3565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm=ChatOllama(model=\"functiongemma:latest\")\n",
    "llm_with_tools=llm.bind_tools([add_numbers, multiply_numbers])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3677391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query=HumanMessage(content=\"What is the sum and product of 5 and 7?\")\n",
    "message=[query]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e4d923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-16T18:44:11.6175559Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2906516300, 'load_duration': 2149094600, 'prompt_eval_count': 167, 'prompt_eval_duration': 246704100, 'eval_count': 17, 'eval_duration': 399495200, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c67c4-5f83-7602-acbf-cf9d585a67a1-0', tool_calls=[{'name': 'multiply_numbers', 'args': {'a': 5, 'b': 7}, 'id': 'd53b1aab-e618-495c-b908-2cc8c73bfa37', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 167, 'output_tokens': 17, 'total_tokens': 184})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=llm_with_tools.invoke(message)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e51de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the sum and product of 5 and 7?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-16T18:44:11.6175559Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2906516300, 'load_duration': 2149094600, 'prompt_eval_count': 167, 'prompt_eval_duration': 246704100, 'eval_count': 17, 'eval_duration': 399495200, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c67c4-5f83-7602-acbf-cf9d585a67a1-0', tool_calls=[{'name': 'multiply_numbers', 'args': {'a': 5, 'b': 7}, 'id': 'd53b1aab-e618-495c-b908-2cc8c73bfa37', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 167, 'output_tokens': 17, 'total_tokens': 184})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.append(result)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af060f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the sum and product of 5 and 7?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-16T18:44:11.6175559Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2906516300, 'load_duration': 2149094600, 'prompt_eval_count': 167, 'prompt_eval_duration': 246704100, 'eval_count': 17, 'eval_duration': 399495200, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c67c4-5f83-7602-acbf-cf9d585a67a1-0', tool_calls=[{'name': 'multiply_numbers', 'args': {'a': 5, 'b': 7}, 'id': 'd53b1aab-e618-495c-b908-2cc8c73bfa37', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 167, 'output_tokens': 17, 'total_tokens': 184})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_result=multiply_numbers.invoke(result.tool_calls[0])\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd0e916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message.append(tool_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28eec615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the sum and product of 5 and 7?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-16T18:44:11.6175559Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2906516300, 'load_duration': 2149094600, 'prompt_eval_count': 167, 'prompt_eval_duration': 246704100, 'eval_count': 17, 'eval_duration': 399495200, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c67c4-5f83-7602-acbf-cf9d585a67a1-0', tool_calls=[{'name': 'multiply_numbers', 'args': {'a': 5, 'b': 7}, 'id': 'd53b1aab-e618-495c-b908-2cc8c73bfa37', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 167, 'output_tokens': 17, 'total_tokens': 184}),\n",
       " ToolMessage(content='35', name='multiply_numbers', tool_call_id='d53b1aab-e618-495c-b908-2cc8c73bfa37')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841e7424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sum of 5 and 7 is 35.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(message).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876f1188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Add two numbers together.', 'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'add_numbers', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "print(add_numbers.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d77f87b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This report provides a comprehensive overview of Russia and Ukraine, covering their history, political landscape, major challenges, and potential future trajectory. It aims to balance the complexities and multifaceted relationship between the two countries.\n",
      "\n",
      "**Key Aspects of the Report:**\n",
      "\n",
      "* **Historical Context:** The report begins by highlighting the rise of Russia as a major power in the late 18th century, fueled by the Tsarist Empire's ambition and the subsequent Russian Civil War. It then transitions to the early years of Russian history, marked by instability and conflict, and the rise of Ukraine as a region of Russian dominance.\n",
      "* **Political Landscape:** The report details the Russian Federation's federal structure, its diverse ethnic composition, and its role in the international order. It also examines the Ukrainian government's democratic republic system and its strong emphasis on national unity.\n",
      "* **Major Challenges:** The report identifies several key challenges facing Russia and Ukraine, including internal conflicts, security concerns, economic challenges, and geopolitical instability.\n",
      "* **Major Challenges in Ukraine:** The report details the war in Ukraine, highlighting the humanitarian crisis, displacement, food shortages, and political instability that have created a significant humanitarian crisis.\n",
      "* **Potential Future Trajectory:** The report predicts a future trajectory for Russia and Ukraine, including the rise of a new power, increased military spending, increased geopolitical competition, and international cooperation.\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "The report presents a nuanced perspective on the relationship between Russia and Ukraine, acknowledging both their strengths and weaknesses. It offers a balanced assessment of the complex interplay of historical, political, and economic factors that shape their future. The report emphasizes the need for continued diplomatic efforts, military aid, and international support to address the challenges facing both countries.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableBranch, RunnableLambda\n",
    "\n",
    "\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "model = ChatOllama(model='gemma3:270m', temperature=0.7)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "report_gen_chain = prompt1 | model | parser\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split())>10, prompt2 | model | parser),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'Russia vs Ukraine'}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6aa6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import (\n",
    "#     RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "# )\n",
    "\n",
    "# # Load a small Hugging Face model (Gemma 270M)\n",
    "# model_id =\"distilgpt2\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# hf_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# # Wrap into a pipeline\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=hf_model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=100,\n",
    "#     temperature=0.7\n",
    "# )\n",
    "\n",
    "# # LangChain wrapper\n",
    "# model = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# # Prompt template\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"Write a joke about {topic}\",\n",
    "#     input_variables=[\"topic\"]\n",
    "# )\n",
    "\n",
    "# parser = StrOutputParser()\n",
    "\n",
    "# # Joke generation chain\n",
    "# joke_gen_chain = RunnableSequence(prompt, model, parser)\n",
    "\n",
    "# # Word count function\n",
    "# def word_count(text):\n",
    "#     return len(text.strip().split())\n",
    "\n",
    "# parallel_chain = RunnableParallel({\n",
    "#     \"joke\": RunnablePassthrough(),\n",
    "#     \"word_count\": RunnableLambda(word_count)\n",
    "# })\n",
    "\n",
    "# final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "# result = final_chain.invoke({\"topic\": \"AI\"})\n",
    "\n",
    "# print(f\"{result['joke']}\\n word count - {result['word_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dffe97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a71d1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tool create\n",
    "import requests\n",
    "from langchain_core.tools import InjectedToolArg\n",
    "from typing import Annotated\n",
    "@tool\n",
    "\n",
    "def get_conversion_factor(base_currency:str,target_currency:str)->float:\n",
    "    \"\"\"Get the conversion factor between two currencies.\"\"\"\n",
    "    url=f'https://v6.exchangerate-api.com/v6/17dcedd14de7bf8638f5fceb/latest/{base_currency}'\n",
    "    response=requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    return data[\"conversion_rates\"][target_currency]\n",
    "\n",
    "@tool\n",
    "def convert(base_currency_value:int,conversion_rate:Annotated[float,InjectedToolArg])-> float:\n",
    "    \"\"\"Convert a currency value using the conversion factor.\"\"\"\n",
    "    return base_currency_value*conversion_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8f64741",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([get_conversion_factor, convert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a8aac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage('What is the conversion factor between INR and USD, and based on that can you convert 10 inr to usd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c24adbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-16T18:44:55.68756Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1319085100, 'load_duration': 551219800, 'prompt_eval_count': 183, 'prompt_eval_duration': 315575800, 'eval_count': 27, 'eval_duration': 424290300, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c67c5-11df-7212-a98c-27ae3306d326-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'INR', 'target_currency': 'USD'}, 'id': '0b5fa454-4698-4b5b-b155-eb8c4b51c802', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 183, 'output_tokens': 27, 'total_tokens': 210})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message = llm_with_tools.invoke(messages)\n",
    "ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "635f60dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the conversion factor between INR and USD, and based on that can you convert 10 inr to usd', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-16T18:44:55.68756Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1319085100, 'load_duration': 551219800, 'prompt_eval_count': 183, 'prompt_eval_duration': 315575800, 'eval_count': 27, 'eval_duration': 424290300, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c67c5-11df-7212-a98c-27ae3306d326-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'INR', 'target_currency': 'USD'}, 'id': '0b5fa454-4698-4b5b-b155-eb8c4b51c802', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 183, 'output_tokens': 27, 'total_tokens': 210})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append(ai_message)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8a8f0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_conversion_factor',\n",
       "  'args': {'base_currency': 'INR', 'target_currency': 'USD'},\n",
       "  'id': '0b5fa454-4698-4b5b-b155-eb8c4b51c802',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "037d0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tool_call in ai_message.tool_calls:\n",
    "  # execute the 1st tool and get the value of conversion rate\n",
    "  if tool_call['name'] == 'get_conversion_factor':\n",
    "    tool_message1 = get_conversion_factor.invoke(tool_call)\n",
    "    # fetch this conversion rate\n",
    "    conversion_rate = tool_message1.content\n",
    "    # append this tool message to messages list\n",
    "    messages.append(tool_message1)\n",
    "  # execute the 2nd tool using the conversion rate from tool 1\n",
    "  if tool_call['name'] == 'convert':\n",
    "    # fetch the current arg\n",
    "    tool_call['args']['conversion_rate'] = conversion_rate\n",
    "    tool_message2 = convert.invoke(tool_call)\n",
    "    messages.append(tool_message2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf85c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the conversion factor between INR and USD, and based on that can you convert 10 inr to usd', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-16T18:44:55.68756Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1319085100, 'load_duration': 551219800, 'prompt_eval_count': 183, 'prompt_eval_duration': 315575800, 'eval_count': 27, 'eval_duration': 424290300, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c67c5-11df-7212-a98c-27ae3306d326-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'INR', 'target_currency': 'USD'}, 'id': '0b5fa454-4698-4b5b-b155-eb8c4b51c802', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 183, 'output_tokens': 27, 'total_tokens': 210}),\n",
       " ToolMessage(content='0.01103', name='get_conversion_factor', tool_call_id='0b5fa454-4698-4b5b-b155-eb8c4b51c802')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac135379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The conversion factor between INR and USD is 0.01103.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1550809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_currency_value': {'title': 'Base Currency Value', 'type': 'integer'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "590fc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency=get_conversion_factor.invoke({'base_currency':'USD','target_currency':'INR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e370703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.6677"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94833546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 1.2.10\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://docs.langchain.com/\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\Users\\abhis\\anaconda3\\Lib\\site-packages\n",
      "Requires: langchain-core, langgraph, pydantic\n",
      "Required-by: ragas\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0635539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in c:\\users\\abhis\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-ollama) (1.2.13)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.6.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.6.4)\n",
      "Requirement already satisfied: packaging>=23.2.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.10.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.27.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.6.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\abhis\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47788375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_12156\\3436112696.py:14: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm, tools)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m tools \u001b[38;5;241m=\u001b[39m [DuckDuckGoSearchRun()]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Build agent graph (function-calling style)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_react_agent(llm, tools)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Wrap in executor\u001b[39;00m\n\u001b[0;32m     17\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\warnings.py:637\u001b[0m, in \u001b[0;36mdeprecated.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(arg)\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    636\u001b[0m     warn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:580\u001b[0m, in \u001b[0;36mcreate_react_agent\u001b[1;34m(model, tools, prompt, response_format, pre_model_hook, post_model_hook, state_schema, context_schema, checkpointer, store, interrupt_before, interrupt_after, debug, version, name, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    574\u001b[0m         model \u001b[38;5;241m=\u001b[39m cast(BaseChatModel, init_chat_model(model))\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    577\u001b[0m         _should_bind_tools(model, tool_classes, num_builtin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(llm_builtin_tools))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tool_classes \u001b[38;5;241m+\u001b[39m llm_builtin_tools) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    579\u001b[0m     ):\n\u001b[1;32m--> 580\u001b[0m         model \u001b[38;5;241m=\u001b[39m cast(BaseChatModel, model)\u001b[38;5;241m.\u001b[39mbind_tools(\n\u001b[0;32m    581\u001b[0m             tool_classes \u001b[38;5;241m+\u001b[39m llm_builtin_tools  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    582\u001b[0m         )\n\u001b[0;32m    584\u001b[0m     static_model: Runnable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m _get_prompt_runnable(prompt) \u001b[38;5;241m|\u001b[39m model  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;66;03m# For dynamic models, we'll create the runnable at runtime\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1538\u001b[0m, in \u001b[0;36mBaseChatModel.bind_tools\u001b[1;34m(self, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind_tools\u001b[39m(\n\u001b[0;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1523\u001b[0m     tools: Sequence[builtins\u001b[38;5;241m.\u001b[39mdict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m|\u001b[39m Callable \u001b[38;5;241m|\u001b[39m BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[LanguageModelInput, AIMessage]:\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Bind tools to the model.\u001b[39;00m\n\u001b[0;32m   1529\u001b[0m \n\u001b[0;32m   1530\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LangGraph agent with Ollama (LangChain 1.2.10+)\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langgraph.prebuilt import  create_react_agent\n",
    "from langgraph.prebuilt import chat_agent_executor\n",
    "\n",
    "# Initialize local Ollama model\n",
    "llm = ChatOllama(model=\"gemma3:250m\", temperature=0)\n",
    "\n",
    "# Define tools (example: DuckDuckGo search)\n",
    "tools = [DuckDuckGoSearchRun()]\n",
    "\n",
    "# Build agent graph (function-calling style)\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Wrap in executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run agent\n",
    "response = agent_executor.invoke({\"input\": \"Search the latest AI trends\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7b63319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "list1=[1,3]\n",
    "list2=list1[:]\n",
    "list1[0]=4\n",
    "print(list2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
